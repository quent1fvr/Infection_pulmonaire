{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de Copy of model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Z9CXuqkVz0mc",
        "OZ0mzZT_HxvC",
        "W3Z-_Zwxe-mU",
        "nKA6GL1PGk0S",
        "5lqlF5RLBIzR"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quent1fvr/Infection_pulmonaire/blob/main/Cross_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **- Introduction**#\n",
        "\n",
        "Le but de ce notebook est de réaliser de la K fold cross validation sur les différents datasets que l'on a crée dans le notebook \"pre_processing.ipynb\" que l'on conseille de regarder pour comprendre les comprendre. Ils seront utilisés sur les deux modèles suivants : \n",
        "\n",
        "1.   Un modèle RNN de convolution2D avec cellule GRU \n",
        "2.   un modèle de convolution3D\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vJP78o6WvxcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0 - Importation des données et librairies**\n",
        "\n"
      ],
      "metadata": {
        "id": "R0SKEyPHdLaR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F10e-oz4Zha_",
        "outputId": "145e9a32-0f5e-45da-fc09-a4f029496149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn70CEYAbrPv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import tensorflow\n",
        "import keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from keras.layers import Conv3D, Conv2D,BatchNormalization,MaxPooling2D , MaxPooling3D, GlobalMaxPool2D,GlobalMaxPool3D\n",
        "from keras.layers import TimeDistributed, GRU, Dense, Dropout\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour l'import des datasets on crée une fonction dont les paramètres sont les chemins des fichiers hdf5 que l'on souhaite utiliser pour l'apprentissage.\n",
        "\n",
        "Les fichiers hdf5 sont générés par le notebook \"pre_processing.ipynb\". \n",
        "Pour faire notre choix de pré-processing, il suffit de choisir les fichiers hdf5 dont les méthodes du notebook sont indiquées dans le nom du fichier : \n",
        "\"dataset_..._methode_num1_num2\""
      ],
      "metadata": {
        "id": "dm-fUdAmYTI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def importation(path_normal, path_malade):\n",
        "  hf_normal = h5py.File(path_normal, \"r\")\n",
        "  hf_malade = h5py.File(path_malade, \"r\")\n",
        "\n",
        "  # conversion des données en tableaux numpy\n",
        "  Data_normal = np.array(hf_normal[\"dataset_1\"][:])\n",
        "  Data_malade = np.array(hf_malade[\"dataset_2\"][:])\n",
        "\n",
        "  return [Data_normal, Data_malade]"
      ],
      "metadata": {
        "id": "50dxvFRJZy-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1 - Etude des données et création des labels**\n",
        "\n"
      ],
      "metadata": {
        "id": "v3imvT8OdXzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = importation(\"/content/drive/MyDrive/UV PROJET P6/dataset_normal_methode_2.1_3.1.hdf5\", \n",
        "                      \"/content/drive/MyDrive/UV PROJET P6/dataset_malade_methode_2.1_3.1.hdf5\")"
      ],
      "metadata": {
        "id": "7f4LhS0F2eE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iURefCa8cbi",
        "outputId": "ac4edbd6-c63d-4726-bf97-7d0a2c2e61ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((55720, 48, 48), (54600, 48, 48))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "Data_normal = Dataset[0]\n",
        "Data_malade = Dataset[1]\n",
        "Data_normal.shape, Data_malade.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afin de pouvoir utiliser des séquences dans les modèles suivant, nous modifions la shape des tableaux en créant des séquences de taille 70. "
      ],
      "metadata": {
        "id": "hgGGpodXcHeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data_normal = Data_normal.reshape(Data_normal.shape[0]//70, 70, 48, 48)\n",
        "Data_malade = Data_malade.reshape(Data_malade.shape[0]//70, 70, 48, 48)\n",
        "Data = np.concatenate((Data_normal, Data_malade))\n",
        "Data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k79efgCdlAeU",
        "outputId": "7f85d6c9-e3d9-47cd-fa4c-34b5f1024083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1576, 70, 48, 48)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le jeu de données est composé de séquences de 70 images par scan, c'est pourquoi on attribue un label pour chaque séquence :\n",
        "- 0 : La personne est négative au COVID\n",
        "- 1 : La personne est positive au COVID"
      ],
      "metadata": {
        "id": "B9aqSe55doPl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wqEZaJ98vEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7674314e-2736-44f6-edce-89cbd2aad0c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1576,)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "Y_normal = np.zeros(Data_normal.shape[0], dtype=np.int8)\n",
        "Y_malade = np.ones(Data_malade.shape[0], dtype=np.int8)\n",
        "Y = np.concatenate((Y_normal, Y_malade))\n",
        "Y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afin de faciliter le traitement pour la suite on encapsule la création des labels dans une fonction :"
      ],
      "metadata": {
        "id": "domC3skI1qC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def labelling(Dataset):\n",
        "  Data_normal = Dataset[0]\n",
        "  Data_malade = Dataset[1]\n",
        "  Data_normal = Data_normal.reshape(Data_normal.shape[0]//70, 70, 48, 48)\n",
        "  Data_malade = Data_malade.reshape(Data_malade.shape[0]//70, 70, 48, 48)\n",
        "  Data = np.concatenate((Data_normal, Data_malade))\n",
        "  Y_normal = np.zeros(Data_normal.shape[0], dtype=np.int8)\n",
        "  Y_malade = np.ones(Data_malade.shape[0], dtype=np.int8)\n",
        "  Y = np.concatenate((Y_normal, Y_malade))\n",
        "  return Data, Y"
      ],
      "metadata": {
        "id": "FNW7UhvO15nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 - Modèles**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vcO2BEhHeztz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Définition des paramètres globaux"
      ],
      "metadata": {
        "id": "Zie1LkbXiDoE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhXYRexwcyC9"
      },
      "outputs": [],
      "source": [
        "SIZE = (48, 48)\n",
        "NBFRAME = 70\n",
        "EPOCH = 40\n",
        "BS = 8\n",
        "CHANNEL = 1\n",
        "INSHAPE = (70, 48, 48, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On fixe la graine aléatoire afin que l'on puisse avoir des résultats similaires en relançant l'apprentissage : "
      ],
      "metadata": {
        "id": "lQqlZXljfFJg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxRDqnX5Iw67"
      },
      "outputs": [],
      "source": [
        "def fix_seed(seed):\n",
        "    tensorflow.random.set_seed(seed)\n",
        "\n",
        "SEED = 42\n",
        "fix_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On utilise le callback EarlyStop pour optimiser l'apprentissage de notre modèle."
      ],
      "metadata": {
        "id": "F9wag0n22EiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_callbacks = [\n",
        "    tensorflow.keras.callbacks.EarlyStopping(\n",
        "        monitor = 'val_acc',\n",
        "        min_delta = 0,\n",
        "        patience = 15,\n",
        "        verbose = 1,\n",
        "        restore_best_weights = True),\n",
        "]"
      ],
      "metadata": {
        "id": "IGbvZBfdIIcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Définition des modèles "
      ],
      "metadata": {
        "id": "hs79V2SVig94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Modèle 1 : 2DCNN + GRU"
      ],
      "metadata": {
        "id": "OZ0mzZT_HxvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chaque image d'entrée de la séquence doit passer dans un réseau convolutif. Le but est d'entraîner ce modèle pour chaque image et de décider ensuite de la classe à déduire.\n",
        "\n",
        "\n",
        "Un ConvNet est créé et distribué en temps pour détecter les \"caractéristiques\".\n",
        "La sortie distribuée dans le temps est injectée dans un GRU ou un LSTM pour traiter les \"séries temporelles\".\n",
        "Un DenseNet est ensuite appliqué pour prendre la décision de \"classer\" si la personne a le covid ou non.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vQGePepR31XB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSYZFlKERU3g"
      },
      "outputs": [],
      "source": [
        "# création du modèle d'apprentissage\n",
        "\n",
        "def build_convnet2D(shape=(48, 48)):\n",
        "    momentum = .9\n",
        "    model = keras.Sequential()\n",
        "    model.add(Conv2D(64, (3, 3), input_shape=shape,\n",
        "        padding='same', activation='relu'))\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(MaxPooling2D())\n",
        "    \n",
        "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    # flatten...\n",
        "    model.add(GlobalMaxPool2D())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HHIhoAbRiFZ"
      },
      "outputs": [],
      "source": [
        "# création du réseau de décision\n",
        "\n",
        "def action_model2D(shape=(70, 48, 48), nbout=1):\n",
        "    # Create our convnet with (112, 112, 3) input shape\n",
        "    convnet = build_convnet2D(shape[1:])\n",
        "    \n",
        "    # then create our final model\n",
        "    model = keras.Sequential()\n",
        "    model.add(TimeDistributed(convnet, input_shape=(70, 48, 48,1)))\n",
        "    # add the GRU cell\n",
        "    model.add(GRU(64))\n",
        "    # and finally, we make a decision network\n",
        "    model.add(Dense(1024, activation='relu'))\n",
        "    model.add(Dropout(.2))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(.2))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(nbout, activation='sigmoid'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modèle 2 : 3DCNN\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W3Z-_Zwxe-mU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chaque image d'entrée de la séquence doit passer dans un réseau convolutif. Le but est d'entraîner ce modèle pour **chaque séquence** ici  et de décider ensuite de la classe à déduire.\n",
        "\n",
        "Un DenseNet est ensuite appliqué pour prendre la décision de \"classer\" si la personne a le covid ou non."
      ],
      "metadata": {
        "id": "42flW7X-4OQA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76Mz9v0RbzAE"
      },
      "outputs": [],
      "source": [
        "# création du modèle d'apprentissage\n",
        "\n",
        "def build_convnet3D(shape=(70, 48, 48)):\n",
        "    momentum = .9\n",
        "    model = keras.Sequential()\n",
        "    model.add(Conv3D(64, (3, 3, 3), input_shape=shape, padding='same', activation='relu'))\n",
        "    model.add(Conv3D(64, (3, 3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(MaxPooling3D())\n",
        "    \n",
        "    model.add(Conv3D(128, (3, 3, 3), padding='same', activation='relu'))\n",
        "    model.add(Conv3D(128, (3, 3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(GlobalMaxPool3D())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sLlW2a2b_pO"
      },
      "outputs": [],
      "source": [
        "# création du réseau de décision\n",
        "\n",
        "def action_model3D(shape=(70, 48, 48, 1), nbout=1):\n",
        "    convnet = build_convnet3D(shape)\n",
        "    \n",
        "    model = keras.Sequential()\n",
        "    model.add(convnet)\n",
        "    # and finally, we make a decision network\n",
        "    model.add(Dense(1024, activation='relu'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(nbout, activation='sigmoid'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3 - CROSS VALIDATION**\n"
      ],
      "metadata": {
        "id": "ShGymUpmS4aI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fonctions de cross validation pour chaque modèle"
      ],
      "metadata": {
        "id": "8EUPXHeKD0_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def KfoldValidation2D(n_splits):\n",
        "  kfold = KFold(n_splits=4, shuffle=True)\n",
        "  acc_per_fold = []\n",
        "  loss_per_fold = []\n",
        "# K-fold Cross Validation model evaluation\n",
        "  fold_no = 1\n",
        "  for train, validation in kfold.split(Data_train, y_train):\n",
        "\n",
        "    model2D = action_model2D(INSHAPE, nbout=1)\n",
        "    optimizer = tensorflow.keras.optimizers.Adam(0.001)\n",
        "    model2D.compile(\n",
        "    optimizer,\n",
        "    'binary_crossentropy',\n",
        "    metrics=['acc']\n",
        "  )\n",
        "\n",
        "    # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model2D.fit(Data_train[train], y_train[train],\n",
        "              batch_size=BS,\n",
        "              epochs=EPOCH,callbacks = my_callbacks, validation_data=(Data_train[validation], y_train[validation])\n",
        "              )\n",
        "\n",
        "  # Evaluation\n",
        "    scores = model2D.evaluate(Data_test,y_test, verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model2D.metrics_names[0]} of {scores[0]}; {model2D.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# == Provide average scores ==\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Score per fold')\n",
        "  for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Average scores for all folds:')\n",
        "  print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "  print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "  print('------------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "keZRtTDBA5F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def KfoldValidation3D(n_splits):\n",
        "  kfold = KFold(n_splits=4, shuffle=True)\n",
        "  acc_per_fold = []\n",
        "  loss_per_fold = []\n",
        "# K-fold Cross Validation model evaluation\n",
        "  fold_no = 1\n",
        "  for train, validation in kfold.split(Data_train, y_train):\n",
        "\n",
        "    model3D = action_model3D(INSHAPE, nbout=1)\n",
        "    optimizer = tensorflow.keras.optimizers.Adam(0.001)\n",
        "    model3D.compile(\n",
        "    optimizer,\n",
        "    'binary_crossentropy',\n",
        "    metrics=['acc']\n",
        ")\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model3D.fit(Data_train[train], y_train[train],\n",
        "              batch_size=BS,\n",
        "              epochs=EPOCH,callbacks = my_callbacks, validation_data=(Data_train[validation], y_train[validation])\n",
        "              )\n",
        "\n",
        "  # Evaluation\n",
        "    scores = model3D.evaluate(Data_test,y_test, verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model3D.metrics_names[0]} of {scores[0]}; {model3D.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# == Provide average scores ==\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Score per fold')\n",
        "  for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Average scores for all folds:')\n",
        "  print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "  print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "  print('------------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "jPqWSGTVDJeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modèle 1 : 2DCNN + GRU"
      ],
      "metadata": {
        "id": "nKA6GL1PGk0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On utilise la cross validation sur ce modèle avec toutes les différentes combinaisons possibles de pre-processing"
      ],
      "metadata": {
        "id": "wZaNqJ6o4cSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1er pre-processing : elimination random + ajout images noires à la fin \n",
        "\n"
      ],
      "metadata": {
        "id": "k8tiaafx4ucM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = importation(\"/content/drive/MyDrive/UV PROJET P6/dataset_normal_methode_2.1_3.1.hdf5\", \n",
        "                      \"/content/drive/MyDrive/UV PROJET P6/dataset_malade_methode_2.1_3.1.hdf5\")\n",
        "Data, Y = labelling(Dataset)\n"
      ],
      "metadata": {
        "id": "wgUF_yYuBTVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_train, Data_test, y_train, y_test = train_test_split(Data, Y, test_size=0.1, random_state=42)\n"
      ],
      "metadata": {
        "id": "ZmS9Qfv9B5ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KfoldValidation2D(4)"
      ],
      "metadata": {
        "id": "z2MveOudG0fw",
        "outputId": "8dfab56e-31b9-42f0-ef9f-d722fe017c0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 38s 170ms/step - loss: 0.5905 - acc: 0.7375 - val_loss: 0.4899 - val_acc: 0.7859\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.4625 - acc: 0.8006 - val_loss: 0.9968 - val_acc: 0.6225\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.4370 - acc: 0.7968 - val_loss: 0.2473 - val_acc: 0.8930\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.3266 - acc: 0.8664 - val_loss: 0.2542 - val_acc: 0.8817\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.3077 - acc: 0.8833 - val_loss: 0.5497 - val_acc: 0.6676\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.3132 - acc: 0.8815 - val_loss: 0.2510 - val_acc: 0.9070\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2585 - acc: 0.8975 - val_loss: 0.3452 - val_acc: 0.8479\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2770 - acc: 0.8862 - val_loss: 0.2064 - val_acc: 0.9014\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2372 - acc: 0.9031 - val_loss: 0.7053 - val_acc: 0.7831\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2388 - acc: 0.9003 - val_loss: 0.1838 - val_acc: 0.9211\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.3154 - acc: 0.8777 - val_loss: 0.3229 - val_acc: 0.8225\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2438 - acc: 0.9040 - val_loss: 0.3559 - val_acc: 0.8845\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2351 - acc: 0.9135 - val_loss: 0.1953 - val_acc: 0.9155\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2004 - acc: 0.9238 - val_loss: 0.1960 - val_acc: 0.9296\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2555 - acc: 0.8852 - val_loss: 1.3116 - val_acc: 0.5211\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2762 - acc: 0.8909 - val_loss: 0.1570 - val_acc: 0.9296\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2389 - acc: 0.8993 - val_loss: 0.1715 - val_acc: 0.9268\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.2355 - acc: 0.9050 - val_loss: 0.1660 - val_acc: 0.9324\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.2508 - acc: 0.9040 - val_loss: 0.3124 - val_acc: 0.8620\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.2294 - acc: 0.9106 - val_loss: 0.1987 - val_acc: 0.9268\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.2718 - acc: 0.8928 - val_loss: 0.6251 - val_acc: 0.6901\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.2496 - acc: 0.8984 - val_loss: 0.1514 - val_acc: 0.9380\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.2095 - acc: 0.9257 - val_loss: 0.1523 - val_acc: 0.9437\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1998 - acc: 0.9191 - val_loss: 0.1420 - val_acc: 0.9437\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.1743 - acc: 0.9341 - val_loss: 0.1561 - val_acc: 0.9324\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 20s 148ms/step - loss: 0.1807 - acc: 0.9285 - val_loss: 0.1302 - val_acc: 0.9549\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1922 - acc: 0.9257 - val_loss: 0.1428 - val_acc: 0.9549\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1619 - acc: 0.9426 - val_loss: 0.1450 - val_acc: 0.9549\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.1588 - acc: 0.9483 - val_loss: 0.1924 - val_acc: 0.9408\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.1655 - acc: 0.9370 - val_loss: 0.1505 - val_acc: 0.9465\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.1576 - acc: 0.9426 - val_loss: 0.1477 - val_acc: 0.9437\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.1795 - acc: 0.9360 - val_loss: 0.1697 - val_acc: 0.9465\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.1587 - acc: 0.9294 - val_loss: 0.2194 - val_acc: 0.9324\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1672 - acc: 0.9266 - val_loss: 0.1347 - val_acc: 0.9521\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1715 - acc: 0.9398 - val_loss: 0.1543 - val_acc: 0.9408\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.2314 - acc: 0.9144 - val_loss: 0.1876 - val_acc: 0.9352\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.1901 - acc: 0.9229 - val_loss: 0.1779 - val_acc: 0.9211\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1839 - acc: 0.9266 - val_loss: 0.2005 - val_acc: 0.9437\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1891 - acc: 0.9247 - val_loss: 0.3467 - val_acc: 0.8338\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1800 - acc: 0.9304 - val_loss: 0.1443 - val_acc: 0.9465\n",
            "Score for fold 1: loss of 0.11600209772586823; acc of 96.20253443717957%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 23s 152ms/step - loss: 0.5593 - acc: 0.7601 - val_loss: 0.8438 - val_acc: 0.4901\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.4379 - acc: 0.8156 - val_loss: 0.4561 - val_acc: 0.7493\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.3387 - acc: 0.8627 - val_loss: 0.4469 - val_acc: 0.8113\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.3038 - acc: 0.8692 - val_loss: 0.3491 - val_acc: 0.8310\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.2563 - acc: 0.8862 - val_loss: 0.2702 - val_acc: 0.8704\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.2512 - acc: 0.9031 - val_loss: 0.4369 - val_acc: 0.8282\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.2488 - acc: 0.9059 - val_loss: 0.2312 - val_acc: 0.9127\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.2204 - acc: 0.9078 - val_loss: 0.3366 - val_acc: 0.8648\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.2063 - acc: 0.9191 - val_loss: 0.2504 - val_acc: 0.8901\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1943 - acc: 0.9163 - val_loss: 0.3041 - val_acc: 0.8901\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.2051 - acc: 0.9135 - val_loss: 0.2871 - val_acc: 0.9070\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1906 - acc: 0.9229 - val_loss: 0.2717 - val_acc: 0.8732\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.2277 - acc: 0.9059 - val_loss: 0.3519 - val_acc: 0.8620\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1622 - acc: 0.9313 - val_loss: 0.2519 - val_acc: 0.8958\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1594 - acc: 0.9360 - val_loss: 0.2236 - val_acc: 0.8986\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1544 - acc: 0.9398 - val_loss: 0.2215 - val_acc: 0.9014\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1937 - acc: 0.9247 - val_loss: 0.3548 - val_acc: 0.8394\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1750 - acc: 0.9294 - val_loss: 0.2490 - val_acc: 0.8845\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1704 - acc: 0.9407 - val_loss: 0.2243 - val_acc: 0.9014\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1835 - acc: 0.9304 - val_loss: 0.2231 - val_acc: 0.8986\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1608 - acc: 0.9285 - val_loss: 0.3023 - val_acc: 0.8761\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.1628 - acc: 0.9407Restoring model weights from the end of the best epoch: 7.\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1628 - acc: 0.9407 - val_loss: 0.2441 - val_acc: 0.9042\n",
            "Epoch 22: early stopping\n",
            "Score for fold 2: loss of 0.15915542840957642; acc of 90.5063271522522%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 23s 157ms/step - loss: 0.5661 - acc: 0.7425 - val_loss: 0.4585 - val_acc: 0.7938\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.4450 - acc: 0.8130 - val_loss: 0.3901 - val_acc: 0.8559\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.3388 - acc: 0.8590 - val_loss: 0.2615 - val_acc: 0.8842\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2913 - acc: 0.8910 - val_loss: 0.2117 - val_acc: 0.9011\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2469 - acc: 0.9023 - val_loss: 0.2246 - val_acc: 0.9068\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2608 - acc: 0.8929 - val_loss: 0.2468 - val_acc: 0.9096\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2626 - acc: 0.8872 - val_loss: 0.1914 - val_acc: 0.9181\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2165 - acc: 0.9060 - val_loss: 0.1764 - val_acc: 0.9294\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2264 - acc: 0.9164 - val_loss: 0.1864 - val_acc: 0.9237\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2083 - acc: 0.9211 - val_loss: 0.2055 - val_acc: 0.9040\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2080 - acc: 0.9164 - val_loss: 0.1998 - val_acc: 0.9181\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2173 - acc: 0.9154 - val_loss: 0.2423 - val_acc: 0.9040\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1937 - acc: 0.9248 - val_loss: 0.1524 - val_acc: 0.9266\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1921 - acc: 0.9164 - val_loss: 0.2668 - val_acc: 0.9040\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2206 - acc: 0.9098 - val_loss: 0.2031 - val_acc: 0.9237\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1578 - acc: 0.9342 - val_loss: 0.2374 - val_acc: 0.9209\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2315 - acc: 0.9088 - val_loss: 0.1749 - val_acc: 0.9322\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1764 - acc: 0.9408 - val_loss: 0.1936 - val_acc: 0.9379\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2071 - acc: 0.9229 - val_loss: 0.1759 - val_acc: 0.9322\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1833 - acc: 0.9239 - val_loss: 0.1728 - val_acc: 0.9294\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1499 - acc: 0.9408 - val_loss: 0.1844 - val_acc: 0.9322\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1388 - acc: 0.9474 - val_loss: 0.1734 - val_acc: 0.9237\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1612 - acc: 0.9370 - val_loss: 0.1328 - val_acc: 0.9379\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1771 - acc: 0.9258 - val_loss: 0.2726 - val_acc: 0.8842\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1908 - acc: 0.9258 - val_loss: 0.1983 - val_acc: 0.9124\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1546 - acc: 0.9521 - val_loss: 0.1480 - val_acc: 0.9237\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1596 - acc: 0.9389 - val_loss: 0.1423 - val_acc: 0.9294\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1413 - acc: 0.9445 - val_loss: 0.1618 - val_acc: 0.9266\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1585 - acc: 0.9370 - val_loss: 0.2469 - val_acc: 0.9096\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1632 - acc: 0.9352 - val_loss: 0.1903 - val_acc: 0.9209\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1305 - acc: 0.9445 - val_loss: 0.1657 - val_acc: 0.9294\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1526 - acc: 0.9492 - val_loss: 0.1291 - val_acc: 0.9435\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1129 - acc: 0.9521 - val_loss: 0.1205 - val_acc: 0.9576\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1133 - acc: 0.9539 - val_loss: 0.1206 - val_acc: 0.9435\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1081 - acc: 0.9633 - val_loss: 0.1454 - val_acc: 0.9322\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.0953 - acc: 0.9652 - val_loss: 0.1078 - val_acc: 0.9520\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1020 - acc: 0.9643 - val_loss: 0.1523 - val_acc: 0.9492\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1086 - acc: 0.9633 - val_loss: 0.3786 - val_acc: 0.8616\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1326 - acc: 0.9502 - val_loss: 0.1197 - val_acc: 0.9407\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1138 - acc: 0.9615 - val_loss: 0.3000 - val_acc: 0.9322\n",
            "Score for fold 3: loss of 0.2647395431995392; acc of 94.9367105960846%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 22s 152ms/step - loss: 0.5814 - acc: 0.7303 - val_loss: 0.5162 - val_acc: 0.7825\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.4772 - acc: 0.7867 - val_loss: 0.3459 - val_acc: 0.8729\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.3739 - acc: 0.8477 - val_loss: 0.1694 - val_acc: 0.9322\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.3367 - acc: 0.8656 - val_loss: 0.2263 - val_acc: 0.8983\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.3078 - acc: 0.8797 - val_loss: 0.1799 - val_acc: 0.9294\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2948 - acc: 0.9032 - val_loss: 0.2145 - val_acc: 0.9096\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2511 - acc: 0.8976 - val_loss: 0.1748 - val_acc: 0.9379\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2512 - acc: 0.8976 - val_loss: 0.1675 - val_acc: 0.9407\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2396 - acc: 0.9220 - val_loss: 0.1568 - val_acc: 0.9379\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2291 - acc: 0.9126 - val_loss: 0.1408 - val_acc: 0.9435\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2476 - acc: 0.9004 - val_loss: 0.3662 - val_acc: 0.8503\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2746 - acc: 0.8872 - val_loss: 0.3687 - val_acc: 0.9181\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.3011 - acc: 0.8797 - val_loss: 0.1283 - val_acc: 0.9463\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2480 - acc: 0.8919 - val_loss: 0.1529 - val_acc: 0.9463\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1934 - acc: 0.9201 - val_loss: 0.1298 - val_acc: 0.9520\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1786 - acc: 0.9164 - val_loss: 0.1383 - val_acc: 0.9520\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1972 - acc: 0.9182 - val_loss: 0.1185 - val_acc: 0.9605\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1949 - acc: 0.9333 - val_loss: 0.1399 - val_acc: 0.9520\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1937 - acc: 0.9154 - val_loss: 0.1091 - val_acc: 0.9548\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2080 - acc: 0.9079 - val_loss: 0.1359 - val_acc: 0.9576\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1946 - acc: 0.9182 - val_loss: 0.1384 - val_acc: 0.9435\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1842 - acc: 0.9201 - val_loss: 0.1175 - val_acc: 0.9605\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1717 - acc: 0.9286 - val_loss: 0.1165 - val_acc: 0.9605\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1799 - acc: 0.9305 - val_loss: 0.2276 - val_acc: 0.8927\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1718 - acc: 0.9305 - val_loss: 0.1657 - val_acc: 0.9237\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1248 - acc: 0.9445 - val_loss: 0.1185 - val_acc: 0.9576\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1456 - acc: 0.9417 - val_loss: 0.1141 - val_acc: 0.9689\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1334 - acc: 0.9427 - val_loss: 0.1018 - val_acc: 0.9661\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1350 - acc: 0.9474 - val_loss: 0.1192 - val_acc: 0.9633\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2060 - acc: 0.9248 - val_loss: 0.1055 - val_acc: 0.9746\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1675 - acc: 0.9342 - val_loss: 0.2361 - val_acc: 0.9181\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.2059 - acc: 0.9192 - val_loss: 0.1096 - val_acc: 0.9548\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1397 - acc: 0.9464 - val_loss: 0.1200 - val_acc: 0.9661\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1483 - acc: 0.9417 - val_loss: 0.1178 - val_acc: 0.9746\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1459 - acc: 0.9445 - val_loss: 0.0946 - val_acc: 0.9774\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1391 - acc: 0.9408 - val_loss: 0.1237 - val_acc: 0.9492\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1564 - acc: 0.9333 - val_loss: 0.1582 - val_acc: 0.9576\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1212 - acc: 0.9492 - val_loss: 0.1387 - val_acc: 0.9350\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1349 - acc: 0.9521 - val_loss: 0.0908 - val_acc: 0.9576\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1309 - acc: 0.9521 - val_loss: 0.2078 - val_acc: 0.9266\n",
            "Score for fold 4: loss of 0.17665019631385803; acc of 93.03797483444214%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.11600209772586823 - Accuracy: 96.20253443717957%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.15915542840957642 - Accuracy: 90.5063271522522%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.2647395431995392 - Accuracy: 94.9367105960846%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.17665019631385803 - Accuracy: 93.03797483444214%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 93.67088675498962 (+- 2.1463087477033036)\n",
            "> Loss: 0.17913681641221046\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2eme pre-processing : elimination SSIM + ajout images noires à partir du milieu du scan\n"
      ],
      "metadata": {
        "id": "F1edFkHx5Dkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = importation(\"/content/drive/MyDrive/UV PROJET P6/dataset_normal_methode_2.2_3.2.hdf5\", \n",
        "                      \"/content/drive/MyDrive/UV PROJET P6/dataset_malade_methode_2.2_3.2.hdf5\")\n",
        "Data, Y = labelling(Dataset)\n",
        "Data_train, Data_test, y_train, y_test = train_test_split(Data, Y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "1OllfZsmlDu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KfoldValidation2D(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LcP_-f5IFq8",
        "outputId": "74401323-95f9-41db-d359-a9cd1587f57c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 40s 282ms/step - loss: 0.6009 - acc: 0.6679 - val_loss: 0.5507 - val_acc: 0.7408\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.4689 - acc: 0.8100 - val_loss: 0.3756 - val_acc: 0.8479\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.3465 - acc: 0.8674 - val_loss: 0.3745 - val_acc: 0.8394\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3865 - acc: 0.8570 - val_loss: 0.3139 - val_acc: 0.9239\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3398 - acc: 0.8627 - val_loss: 0.2221 - val_acc: 0.9352\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3139 - acc: 0.8786 - val_loss: 0.2392 - val_acc: 0.9127\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2595 - acc: 0.9031 - val_loss: 0.2227 - val_acc: 0.9239\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2460 - acc: 0.9163 - val_loss: 0.2513 - val_acc: 0.8986\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2386 - acc: 0.9106 - val_loss: 0.4100 - val_acc: 0.8141\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3228 - acc: 0.8617 - val_loss: 0.6223 - val_acc: 0.6366\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.4338 - acc: 0.8100 - val_loss: 0.4210 - val_acc: 0.8789\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.4154 - acc: 0.8485 - val_loss: 0.3663 - val_acc: 0.8451\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3713 - acc: 0.8570 - val_loss: 0.2489 - val_acc: 0.9127\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.3234 - acc: 0.8862 - val_loss: 0.2821 - val_acc: 0.8873\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3127 - acc: 0.8881 - val_loss: 0.1772 - val_acc: 0.9324\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3331 - acc: 0.8796 - val_loss: 0.2303 - val_acc: 0.9211\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2204 - acc: 0.9229 - val_loss: 0.2484 - val_acc: 0.8986\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2785 - acc: 0.8862 - val_loss: 0.2023 - val_acc: 0.9239\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2657 - acc: 0.8975 - val_loss: 0.1800 - val_acc: 0.9324\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.2491 - acc: 0.9059Restoring model weights from the end of the best epoch: 5.\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2491 - acc: 0.9059 - val_loss: 0.1811 - val_acc: 0.9268\n",
            "Epoch 20: early stopping\n",
            "Score for fold 1: loss of 0.23133613169193268; acc of 91.77215099334717%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 40s 282ms/step - loss: 0.5686 - acc: 0.7093 - val_loss: 0.5043 - val_acc: 0.7577\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.4113 - acc: 0.8316 - val_loss: 0.4403 - val_acc: 0.9042\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.3145 - acc: 0.8805 - val_loss: 0.3127 - val_acc: 0.8958\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.3555 - acc: 0.8645 - val_loss: 0.2836 - val_acc: 0.9099\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.3087 - acc: 0.8796 - val_loss: 0.2458 - val_acc: 0.9099\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.2814 - acc: 0.8946 - val_loss: 0.3860 - val_acc: 0.8310\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2486 - acc: 0.9059 - val_loss: 0.2139 - val_acc: 0.9211\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2066 - acc: 0.9182 - val_loss: 0.2530 - val_acc: 0.9155\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2547 - acc: 0.9106 - val_loss: 0.2101 - val_acc: 0.9127\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2291 - acc: 0.9116 - val_loss: 0.2397 - val_acc: 0.9070\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2024 - acc: 0.9247 - val_loss: 0.2129 - val_acc: 0.9352\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3165 - acc: 0.8843 - val_loss: 0.2294 - val_acc: 0.9239\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2386 - acc: 0.9172 - val_loss: 0.2216 - val_acc: 0.9211\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2043 - acc: 0.9304 - val_loss: 0.2143 - val_acc: 0.9268\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1775 - acc: 0.9407 - val_loss: 0.2043 - val_acc: 0.9127\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1599 - acc: 0.9407 - val_loss: 0.3223 - val_acc: 0.9070\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2022 - acc: 0.9229 - val_loss: 0.2067 - val_acc: 0.9268\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2156 - acc: 0.9125 - val_loss: 0.2525 - val_acc: 0.9239\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2443 - acc: 0.9116 - val_loss: 0.2325 - val_acc: 0.9155\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1709 - acc: 0.9351 - val_loss: 0.2065 - val_acc: 0.9352\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2557 - acc: 0.9040 - val_loss: 0.2054 - val_acc: 0.9324\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1785 - acc: 0.9351 - val_loss: 0.1919 - val_acc: 0.9296\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1601 - acc: 0.9417 - val_loss: 0.1910 - val_acc: 0.9324\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1697 - acc: 0.9464 - val_loss: 0.1928 - val_acc: 0.9324\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1449 - acc: 0.9530 - val_loss: 0.1927 - val_acc: 0.9324\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.1484 - acc: 0.9464Restoring model weights from the end of the best epoch: 11.\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1484 - acc: 0.9464 - val_loss: 0.1864 - val_acc: 0.9324\n",
            "Epoch 26: early stopping\n",
            "Score for fold 2: loss of 0.18590445816516876; acc of 95.56962251663208%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 39s 282ms/step - loss: 0.6017 - acc: 0.6504 - val_loss: 0.6868 - val_acc: 0.6073\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.4142 - acc: 0.8346 - val_loss: 0.3628 - val_acc: 0.8220\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3137 - acc: 0.8947 - val_loss: 0.4240 - val_acc: 0.8672\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3285 - acc: 0.8882 - val_loss: 0.2642 - val_acc: 0.8898\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2834 - acc: 0.9023 - val_loss: 0.2482 - val_acc: 0.8955\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2721 - acc: 0.9032 - val_loss: 0.2412 - val_acc: 0.8983\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3963 - acc: 0.8600 - val_loss: 0.3347 - val_acc: 0.8531\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2766 - acc: 0.9126 - val_loss: 0.2118 - val_acc: 0.9153\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2445 - acc: 0.9070 - val_loss: 0.3795 - val_acc: 0.8531\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2206 - acc: 0.9295 - val_loss: 0.1986 - val_acc: 0.9181\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2217 - acc: 0.9229 - val_loss: 0.2944 - val_acc: 0.8503\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2136 - acc: 0.9229 - val_loss: 0.2170 - val_acc: 0.9181\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2532 - acc: 0.8985 - val_loss: 0.1831 - val_acc: 0.9153\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2085 - acc: 0.9333 - val_loss: 0.1726 - val_acc: 0.9350\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1703 - acc: 0.9417 - val_loss: 0.1977 - val_acc: 0.9068\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1774 - acc: 0.9370 - val_loss: 0.1271 - val_acc: 0.9492\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1874 - acc: 0.9380 - val_loss: 0.1800 - val_acc: 0.9237\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1629 - acc: 0.9342 - val_loss: 0.3816 - val_acc: 0.8446\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1467 - acc: 0.9455 - val_loss: 0.1319 - val_acc: 0.9548\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1274 - acc: 0.9521 - val_loss: 0.1401 - val_acc: 0.9407\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1568 - acc: 0.9464 - val_loss: 0.1551 - val_acc: 0.9407\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1337 - acc: 0.9558 - val_loss: 0.1358 - val_acc: 0.9350\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1270 - acc: 0.9539 - val_loss: 0.1493 - val_acc: 0.9576\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1548 - acc: 0.9427 - val_loss: 0.2166 - val_acc: 0.9237\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1459 - acc: 0.9445 - val_loss: 0.2984 - val_acc: 0.8757\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1735 - acc: 0.9455 - val_loss: 0.1294 - val_acc: 0.9520\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1396 - acc: 0.9492 - val_loss: 0.1414 - val_acc: 0.9520\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1506 - acc: 0.9483 - val_loss: 0.1452 - val_acc: 0.9520\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1006 - acc: 0.9605 - val_loss: 0.2036 - val_acc: 0.9379\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1311 - acc: 0.9445 - val_loss: 0.2600 - val_acc: 0.8785\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1407 - acc: 0.9464 - val_loss: 0.1948 - val_acc: 0.9407\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1091 - acc: 0.9549 - val_loss: 0.1323 - val_acc: 0.9576\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1100 - acc: 0.9586 - val_loss: 0.1395 - val_acc: 0.9435\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1186 - acc: 0.9558 - val_loss: 0.1280 - val_acc: 0.9548\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1262 - acc: 0.9558 - val_loss: 0.1648 - val_acc: 0.9350\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.0793 - acc: 0.9727 - val_loss: 0.2568 - val_acc: 0.9407\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1073 - acc: 0.9615 - val_loss: 0.2968 - val_acc: 0.9181\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.0977 - acc: 0.9680Restoring model weights from the end of the best epoch: 23.\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.0977 - acc: 0.9680 - val_loss: 0.2147 - val_acc: 0.9068\n",
            "Epoch 38: early stopping\n",
            "Score for fold 3: loss of 0.18741407990455627; acc of 91.77215099334717%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 40s 283ms/step - loss: 0.5745 - acc: 0.6898 - val_loss: 0.5616 - val_acc: 0.7006\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.4271 - acc: 0.8308 - val_loss: 0.2663 - val_acc: 0.9266\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3482 - acc: 0.8609 - val_loss: 0.2099 - val_acc: 0.9181\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2829 - acc: 0.8957 - val_loss: 0.3709 - val_acc: 0.8277\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2969 - acc: 0.8985 - val_loss: 0.1977 - val_acc: 0.9322\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3047 - acc: 0.8844 - val_loss: 0.2708 - val_acc: 0.8814\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2452 - acc: 0.9051 - val_loss: 0.2211 - val_acc: 0.9237\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2755 - acc: 0.9023 - val_loss: 0.2330 - val_acc: 0.9237\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2598 - acc: 0.9060 - val_loss: 0.4033 - val_acc: 0.8588\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2779 - acc: 0.9013 - val_loss: 0.3239 - val_acc: 0.8785\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2269 - acc: 0.9107 - val_loss: 0.2079 - val_acc: 0.9237\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2032 - acc: 0.9192 - val_loss: 0.2410 - val_acc: 0.9209\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2256 - acc: 0.9248 - val_loss: 0.1746 - val_acc: 0.9237\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1925 - acc: 0.9276 - val_loss: 0.1604 - val_acc: 0.9379\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2059 - acc: 0.9192 - val_loss: 0.2528 - val_acc: 0.9266\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2045 - acc: 0.9314 - val_loss: 0.1754 - val_acc: 0.9266\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1306 - acc: 0.9530 - val_loss: 0.1695 - val_acc: 0.9463\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2116 - acc: 0.9276 - val_loss: 0.3513 - val_acc: 0.8305\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1892 - acc: 0.9333 - val_loss: 0.2080 - val_acc: 0.9124\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1751 - acc: 0.9323 - val_loss: 0.2653 - val_acc: 0.9181\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1848 - acc: 0.9258 - val_loss: 0.1303 - val_acc: 0.9463\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1590 - acc: 0.9417 - val_loss: 0.1950 - val_acc: 0.9379\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1398 - acc: 0.9492 - val_loss: 0.2006 - val_acc: 0.9096\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1734 - acc: 0.9352 - val_loss: 0.2584 - val_acc: 0.8927\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1575 - acc: 0.9492 - val_loss: 0.1685 - val_acc: 0.9294\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1877 - acc: 0.9286 - val_loss: 0.2592 - val_acc: 0.8898\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1753 - acc: 0.9352 - val_loss: 0.1606 - val_acc: 0.9435\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1534 - acc: 0.9417 - val_loss: 0.1670 - val_acc: 0.9294\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1495 - acc: 0.9474 - val_loss: 0.1716 - val_acc: 0.9520\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1750 - acc: 0.9474 - val_loss: 0.2294 - val_acc: 0.9435\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1375 - acc: 0.9483 - val_loss: 0.1789 - val_acc: 0.9435\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1565 - acc: 0.9539 - val_loss: 0.2023 - val_acc: 0.9209\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1183 - acc: 0.9558 - val_loss: 0.1902 - val_acc: 0.9548\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1219 - acc: 0.9577 - val_loss: 0.1736 - val_acc: 0.9463\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1182 - acc: 0.9596 - val_loss: 0.2124 - val_acc: 0.9209\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1148 - acc: 0.9577 - val_loss: 0.2855 - val_acc: 0.9124\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1518 - acc: 0.9445 - val_loss: 0.1385 - val_acc: 0.9492\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.0942 - acc: 0.9652 - val_loss: 0.1895 - val_acc: 0.9463\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1069 - acc: 0.9596 - val_loss: 0.1446 - val_acc: 0.9492\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.0914 - acc: 0.9680 - val_loss: 0.1823 - val_acc: 0.9350\n",
            "Score for fold 4: loss of 0.15826432406902313; acc of 94.30379867553711%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.23133613169193268 - Accuracy: 91.77215099334717%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.18590445816516876 - Accuracy: 95.56962251663208%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.18741407990455627 - Accuracy: 91.77215099334717%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.15826432406902313 - Accuracy: 94.30379867553711%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 93.35443079471588 (+- 1.6443534046563617)\n",
            "> Loss: 0.1907297484576702\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 eme preprocessing : random + milieu de scan"
      ],
      "metadata": {
        "id": "SeRBm79580oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = importation(\"/content/drive/MyDrive/UV PROJET P6/dataset_normal_methode_2.2_3.1.hdf5\", \n",
        "                      \"/content/drive/MyDrive/UV PROJET P6/dataset_malade_methode_2.2_3.1.hdf5\")\n",
        "Data, Y = labelling(Dataset)\n",
        "Data_train, Data_test, y_train, y_test = train_test_split(Data, Y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "FW6IsPDqvMov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KfoldValidation2D(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoYbWndz0hTO",
        "outputId": "63e79417-c02d-4174-bb2a-59c95874b9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 40s 285ms/step - loss: 0.5533 - acc: 0.7112 - val_loss: 0.4760 - val_acc: 0.8310\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.4222 - acc: 0.8307 - val_loss: 0.3560 - val_acc: 0.8648\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.3329 - acc: 0.8683 - val_loss: 0.2739 - val_acc: 0.8789\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2930 - acc: 0.8890 - val_loss: 0.2789 - val_acc: 0.9183\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.3236 - acc: 0.8768 - val_loss: 0.1856 - val_acc: 0.9239\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2535 - acc: 0.8937 - val_loss: 0.2385 - val_acc: 0.9324\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2882 - acc: 0.8824 - val_loss: 0.2198 - val_acc: 0.9014\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2591 - acc: 0.9097 - val_loss: 0.2037 - val_acc: 0.9155\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2343 - acc: 0.9106 - val_loss: 0.2174 - val_acc: 0.9211\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2605 - acc: 0.8956 - val_loss: 0.5269 - val_acc: 0.7211\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2533 - acc: 0.9069 - val_loss: 0.1516 - val_acc: 0.9408\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2631 - acc: 0.9022 - val_loss: 0.2306 - val_acc: 0.8817\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2404 - acc: 0.9069 - val_loss: 0.1713 - val_acc: 0.9268\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.2101 - acc: 0.9247 - val_loss: 0.1495 - val_acc: 0.9408\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1996 - acc: 0.9153 - val_loss: 0.1891 - val_acc: 0.9408\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.3111 - acc: 0.9003 - val_loss: 0.2114 - val_acc: 0.9239\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.2398 - acc: 0.8918 - val_loss: 0.5508 - val_acc: 0.6789\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.2133 - acc: 0.9050 - val_loss: 0.1659 - val_acc: 0.9324\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.2182 - acc: 0.9266 - val_loss: 0.1703 - val_acc: 0.9268\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.2428 - acc: 0.9069 - val_loss: 0.1550 - val_acc: 0.9296\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2096 - acc: 0.9163 - val_loss: 0.1794 - val_acc: 0.9437\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.2654 - acc: 0.9050 - val_loss: 0.1616 - val_acc: 0.9380\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1796 - acc: 0.9323 - val_loss: 0.1558 - val_acc: 0.9324\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1651 - acc: 0.9370 - val_loss: 0.1292 - val_acc: 0.9493\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1711 - acc: 0.9332 - val_loss: 0.1441 - val_acc: 0.9352\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1590 - acc: 0.9389 - val_loss: 0.1806 - val_acc: 0.9239\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1849 - acc: 0.9238 - val_loss: 0.2302 - val_acc: 0.9268\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1811 - acc: 0.9238 - val_loss: 0.1981 - val_acc: 0.9155\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1397 - acc: 0.9473 - val_loss: 0.1311 - val_acc: 0.9465\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1156 - acc: 0.9511 - val_loss: 0.1725 - val_acc: 0.9211\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.2692 - acc: 0.8993 - val_loss: 0.1816 - val_acc: 0.9296\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1698 - acc: 0.9294 - val_loss: 0.1741 - val_acc: 0.9211\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1414 - acc: 0.9426 - val_loss: 0.1562 - val_acc: 0.9324\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1213 - acc: 0.9520 - val_loss: 0.1466 - val_acc: 0.9380\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.0985 - acc: 0.9605 - val_loss: 0.1416 - val_acc: 0.9437\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1498 - acc: 0.9436 - val_loss: 0.1811 - val_acc: 0.9437\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1261 - acc: 0.9417 - val_loss: 0.1288 - val_acc: 0.9549\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1194 - acc: 0.9530 - val_loss: 0.1410 - val_acc: 0.9577\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1017 - acc: 0.9586 - val_loss: 0.1311 - val_acc: 0.9437\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1562 - acc: 0.9445 - val_loss: 0.1367 - val_acc: 0.9408\n",
            "Score for fold 1: loss of 0.10736994445323944; acc of 94.30379867553711%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 39s 280ms/step - loss: 0.5232 - acc: 0.7479 - val_loss: 0.3381 - val_acc: 0.8394\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3198 - acc: 0.8786 - val_loss: 0.4810 - val_acc: 0.8338\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2741 - acc: 0.8881 - val_loss: 0.2893 - val_acc: 0.8676\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.2477 - acc: 0.9144 - val_loss: 0.2632 - val_acc: 0.8620\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2385 - acc: 0.9003 - val_loss: 0.2503 - val_acc: 0.8958\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2380 - acc: 0.9097 - val_loss: 0.3272 - val_acc: 0.8761\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.2557 - acc: 0.9022 - val_loss: 0.4132 - val_acc: 0.8085\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.3015 - acc: 0.8645 - val_loss: 0.2777 - val_acc: 0.8648\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1936 - acc: 0.9210 - val_loss: 0.3129 - val_acc: 0.8901\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.2189 - acc: 0.9172 - val_loss: 0.2732 - val_acc: 0.8732\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.2089 - acc: 0.9135 - val_loss: 0.2136 - val_acc: 0.8986\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1992 - acc: 0.9172 - val_loss: 0.2470 - val_acc: 0.9099\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1619 - acc: 0.9360 - val_loss: 0.4343 - val_acc: 0.8901\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1838 - acc: 0.9323 - val_loss: 0.1942 - val_acc: 0.9183\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1216 - acc: 0.9501 - val_loss: 0.2276 - val_acc: 0.9127\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1618 - acc: 0.9360 - val_loss: 0.2091 - val_acc: 0.9211\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1137 - acc: 0.9501 - val_loss: 0.3048 - val_acc: 0.9127\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1527 - acc: 0.9398 - val_loss: 0.2291 - val_acc: 0.9099\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1759 - acc: 0.9379 - val_loss: 0.1970 - val_acc: 0.9296\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 36s 272ms/step - loss: 0.1728 - acc: 0.9407 - val_loss: 0.2528 - val_acc: 0.9239\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1503 - acc: 0.9483 - val_loss: 0.1571 - val_acc: 0.9437\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1231 - acc: 0.9454 - val_loss: 0.1433 - val_acc: 0.9296\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.0860 - acc: 0.9661 - val_loss: 0.1467 - val_acc: 0.9521\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1090 - acc: 0.9567 - val_loss: 0.2592 - val_acc: 0.9352\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1316 - acc: 0.9464 - val_loss: 0.1531 - val_acc: 0.9493\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 36s 272ms/step - loss: 0.0993 - acc: 0.9614 - val_loss: 0.4024 - val_acc: 0.9042\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 36s 272ms/step - loss: 0.0994 - acc: 0.9586 - val_loss: 0.1748 - val_acc: 0.9408\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 36s 272ms/step - loss: 0.0820 - acc: 0.9605 - val_loss: 0.2211 - val_acc: 0.9183\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 36s 272ms/step - loss: 0.1114 - acc: 0.9614 - val_loss: 0.2576 - val_acc: 0.8817\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 36s 272ms/step - loss: 0.0820 - acc: 0.9671 - val_loss: 0.4631 - val_acc: 0.8986\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 36s 272ms/step - loss: 0.1077 - acc: 0.9567 - val_loss: 0.1724 - val_acc: 0.9324\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 36s 272ms/step - loss: 0.1211 - acc: 0.9501 - val_loss: 0.1902 - val_acc: 0.9324\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 36s 272ms/step - loss: 0.1385 - acc: 0.9426 - val_loss: 0.2694 - val_acc: 0.8986\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 36s 272ms/step - loss: 0.1119 - acc: 0.9652 - val_loss: 0.1780 - val_acc: 0.9465\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 36s 272ms/step - loss: 0.0799 - acc: 0.9727 - val_loss: 0.2439 - val_acc: 0.9380\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 36s 272ms/step - loss: 0.0778 - acc: 0.9671 - val_loss: 0.1942 - val_acc: 0.9352\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 36s 272ms/step - loss: 0.1000 - acc: 0.9661 - val_loss: 0.2351 - val_acc: 0.9296\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.0674 - acc: 0.9727Restoring model weights from the end of the best epoch: 23.\n",
            "133/133 [==============================] - 36s 272ms/step - loss: 0.0674 - acc: 0.9727 - val_loss: 0.2117 - val_acc: 0.9352\n",
            "Epoch 38: early stopping\n",
            "Score for fold 2: loss of 0.10602650791406631; acc of 94.9367105960846%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 39s 282ms/step - loss: 0.5246 - acc: 0.7575 - val_loss: 0.6960 - val_acc: 0.6808\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.3785 - acc: 0.8543 - val_loss: 0.3979 - val_acc: 0.8333\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.3088 - acc: 0.8835 - val_loss: 0.2644 - val_acc: 0.8785\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3588 - acc: 0.8487 - val_loss: 0.3455 - val_acc: 0.8588\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3521 - acc: 0.8600 - val_loss: 0.3491 - val_acc: 0.8418\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2875 - acc: 0.8985 - val_loss: 0.2052 - val_acc: 0.9096\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2104 - acc: 0.9126 - val_loss: 0.2050 - val_acc: 0.9181\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1813 - acc: 0.9211 - val_loss: 0.2422 - val_acc: 0.9181\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2279 - acc: 0.9088 - val_loss: 0.1948 - val_acc: 0.9209\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2026 - acc: 0.9248 - val_loss: 0.1957 - val_acc: 0.9379\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1855 - acc: 0.9201 - val_loss: 0.3006 - val_acc: 0.8644\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2326 - acc: 0.9164 - val_loss: 0.2241 - val_acc: 0.9181\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2376 - acc: 0.9098 - val_loss: 0.1858 - val_acc: 0.9294\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2244 - acc: 0.9117 - val_loss: 0.1605 - val_acc: 0.9294\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2059 - acc: 0.9220 - val_loss: 0.1931 - val_acc: 0.9294\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1713 - acc: 0.9342 - val_loss: 0.1442 - val_acc: 0.9322\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1923 - acc: 0.9173 - val_loss: 0.2230 - val_acc: 0.9096\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2436 - acc: 0.9051 - val_loss: 0.4630 - val_acc: 0.9011\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2228 - acc: 0.9098 - val_loss: 0.1841 - val_acc: 0.9294\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2016 - acc: 0.9192 - val_loss: 0.1815 - val_acc: 0.9266\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2746 - acc: 0.8900 - val_loss: 0.2030 - val_acc: 0.9124\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2248 - acc: 0.9135 - val_loss: 0.1980 - val_acc: 0.9322\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1682 - acc: 0.9389 - val_loss: 0.1586 - val_acc: 0.9379\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1366 - acc: 0.9502 - val_loss: 0.1665 - val_acc: 0.9407\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1339 - acc: 0.9474 - val_loss: 0.1573 - val_acc: 0.9407\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1434 - acc: 0.9455 - val_loss: 0.2292 - val_acc: 0.8870\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2334 - acc: 0.9032 - val_loss: 0.2405 - val_acc: 0.9181\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1695 - acc: 0.9380 - val_loss: 0.1466 - val_acc: 0.9407\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1619 - acc: 0.9295 - val_loss: 0.1834 - val_acc: 0.9322\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1216 - acc: 0.9455 - val_loss: 0.1320 - val_acc: 0.9520\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1524 - acc: 0.9436 - val_loss: 0.1810 - val_acc: 0.9237\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1412 - acc: 0.9408 - val_loss: 0.1538 - val_acc: 0.9322\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1436 - acc: 0.9389 - val_loss: 0.1471 - val_acc: 0.9576\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1165 - acc: 0.9511 - val_loss: 0.2253 - val_acc: 0.9266\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1558 - acc: 0.9380 - val_loss: 0.1642 - val_acc: 0.9350\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0995 - acc: 0.9624 - val_loss: 0.1479 - val_acc: 0.9520\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1083 - acc: 0.9624 - val_loss: 0.1651 - val_acc: 0.9350\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1559 - acc: 0.9464 - val_loss: 0.1825 - val_acc: 0.9237\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1502 - acc: 0.9342 - val_loss: 0.2253 - val_acc: 0.9237\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1259 - acc: 0.9511 - val_loss: 0.3044 - val_acc: 0.8898\n",
            "Score for fold 3: loss of 0.2824360430240631; acc of 86.07594966888428%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 39s 281ms/step - loss: 0.6104 - acc: 0.6692 - val_loss: 0.4395 - val_acc: 0.7853\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.4084 - acc: 0.8177 - val_loss: 0.2675 - val_acc: 0.8870\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3317 - acc: 0.8694 - val_loss: 0.1904 - val_acc: 0.9322\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3012 - acc: 0.8797 - val_loss: 0.2377 - val_acc: 0.9294\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2769 - acc: 0.8872 - val_loss: 0.3134 - val_acc: 0.8701\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3133 - acc: 0.9013 - val_loss: 0.1869 - val_acc: 0.9350\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.3245 - acc: 0.8731 - val_loss: 0.5685 - val_acc: 0.7034\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3139 - acc: 0.8853 - val_loss: 0.1561 - val_acc: 0.9463\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.2493 - acc: 0.9013 - val_loss: 0.1945 - val_acc: 0.9068\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.2357 - acc: 0.8994 - val_loss: 0.1563 - val_acc: 0.9379\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1946 - acc: 0.9220 - val_loss: 0.1423 - val_acc: 0.9435\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1948 - acc: 0.9248 - val_loss: 0.1156 - val_acc: 0.9605\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1833 - acc: 0.9258 - val_loss: 0.4655 - val_acc: 0.8220\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2116 - acc: 0.9145 - val_loss: 0.3140 - val_acc: 0.8927\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.2625 - acc: 0.8976 - val_loss: 0.2247 - val_acc: 0.9350\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1925 - acc: 0.9258 - val_loss: 0.1573 - val_acc: 0.9209\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1511 - acc: 0.9464 - val_loss: 0.0921 - val_acc: 0.9605\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1468 - acc: 0.9352 - val_loss: 0.2323 - val_acc: 0.9153\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1557 - acc: 0.9455 - val_loss: 0.0908 - val_acc: 0.9689\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1388 - acc: 0.9483 - val_loss: 0.1840 - val_acc: 0.9463\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1442 - acc: 0.9492 - val_loss: 0.0822 - val_acc: 0.9661\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1764 - acc: 0.9352 - val_loss: 0.0946 - val_acc: 0.9661\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1255 - acc: 0.9502 - val_loss: 0.1463 - val_acc: 0.9322\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1294 - acc: 0.9511 - val_loss: 0.2021 - val_acc: 0.9209\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1622 - acc: 0.9427 - val_loss: 0.0616 - val_acc: 0.9718\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1525 - acc: 0.9380 - val_loss: 0.1220 - val_acc: 0.9520\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1103 - acc: 0.9530 - val_loss: 0.0670 - val_acc: 0.9689\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1207 - acc: 0.9549 - val_loss: 0.1168 - val_acc: 0.9576\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.0941 - acc: 0.9662 - val_loss: 0.0552 - val_acc: 0.9802\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1047 - acc: 0.9643 - val_loss: 0.0847 - val_acc: 0.9661\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.0960 - acc: 0.9662 - val_loss: 0.1004 - val_acc: 0.9633\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1117 - acc: 0.9539 - val_loss: 0.1174 - val_acc: 0.9435\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1085 - acc: 0.9596 - val_loss: 0.1123 - val_acc: 0.9520\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1828 - acc: 0.9295 - val_loss: 0.0917 - val_acc: 0.9689\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1532 - acc: 0.9352 - val_loss: 0.0704 - val_acc: 0.9746\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1280 - acc: 0.9474 - val_loss: 0.0808 - val_acc: 0.9689\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1239 - acc: 0.9417 - val_loss: 0.0700 - val_acc: 0.9661\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.0962 - acc: 0.9568 - val_loss: 0.1381 - val_acc: 0.9435\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1229 - acc: 0.9483 - val_loss: 0.0926 - val_acc: 0.9746\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1197 - acc: 0.9643 - val_loss: 0.1069 - val_acc: 0.9661\n",
            "Score for fold 4: loss of 0.09761904180049896; acc of 97.46835231781006%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.10736994445323944 - Accuracy: 94.30379867553711%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.10602650791406631 - Accuracy: 94.9367105960846%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.2824360430240631 - Accuracy: 86.07594966888428%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.09761904180049896 - Accuracy: 97.46835231781006%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 93.19620281457901 (+- 4.278007777150073)\n",
            "> Loss: 0.14836288429796696\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 eme pre processing : random + debut et fin de scan"
      ],
      "metadata": {
        "id": "Ck114Rep80yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = importation(\"/content/drive/MyDrive/UV PROJET P6/dataset_normal_methode_2.3_3.1.hdf5\", \n",
        "                      \"/content/drive/MyDrive/UV PROJET P6/dataset_malade_methode_2.3_3.1.hdf5\")\n",
        "Data, Y = labelling(Dataset)\n",
        "Data_train, Data_test, y_train, y_test = train_test_split(Data, Y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "W745Cf5BGy-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KfoldValidation2D(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSRid3bRG671",
        "outputId": "7559d456-e2e2-4b6a-aa6a-828c64f1b976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 38s 272ms/step - loss: 0.5738 - acc: 0.6961 - val_loss: 0.6337 - val_acc: 0.5803\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.4584 - acc: 0.7987 - val_loss: 0.3473 - val_acc: 0.8620\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.3924 - acc: 0.8420 - val_loss: 0.3625 - val_acc: 0.8592\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3279 - acc: 0.8721 - val_loss: 0.2252 - val_acc: 0.9014\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3216 - acc: 0.8608 - val_loss: 0.2478 - val_acc: 0.9042\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3254 - acc: 0.8683 - val_loss: 0.2910 - val_acc: 0.8789\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2819 - acc: 0.8909 - val_loss: 0.4528 - val_acc: 0.7718\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2420 - acc: 0.8993 - val_loss: 0.2937 - val_acc: 0.9211\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.2493 - acc: 0.9135 - val_loss: 0.3788 - val_acc: 0.8394\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2267 - acc: 0.9050 - val_loss: 0.1869 - val_acc: 0.9380\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2572 - acc: 0.9012 - val_loss: 0.2725 - val_acc: 0.9070\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.2261 - acc: 0.9135 - val_loss: 0.1599 - val_acc: 0.9324\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2020 - acc: 0.9182 - val_loss: 0.1680 - val_acc: 0.9296\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2413 - acc: 0.9200 - val_loss: 0.4501 - val_acc: 0.8451\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.2071 - acc: 0.9200 - val_loss: 0.2094 - val_acc: 0.9239\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1667 - acc: 0.9332 - val_loss: 0.1389 - val_acc: 0.9465\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1606 - acc: 0.9313 - val_loss: 0.1296 - val_acc: 0.9437\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1951 - acc: 0.9238 - val_loss: 0.1998 - val_acc: 0.9155\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1796 - acc: 0.9276 - val_loss: 0.1791 - val_acc: 0.9324\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1578 - acc: 0.9351 - val_loss: 0.1496 - val_acc: 0.9352\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1705 - acc: 0.9389 - val_loss: 0.1477 - val_acc: 0.9437\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2188 - acc: 0.9238 - val_loss: 0.1408 - val_acc: 0.9380\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1362 - acc: 0.9454 - val_loss: 0.1513 - val_acc: 0.9408\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1283 - acc: 0.9539 - val_loss: 0.1603 - val_acc: 0.9352\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1096 - acc: 0.9567 - val_loss: 0.1434 - val_acc: 0.9408\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1080 - acc: 0.9643 - val_loss: 0.5592 - val_acc: 0.7014\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1245 - acc: 0.9501 - val_loss: 0.1291 - val_acc: 0.9437\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0921 - acc: 0.9633 - val_loss: 0.1155 - val_acc: 0.9662\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1620 - acc: 0.9436 - val_loss: 0.1304 - val_acc: 0.9408\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0874 - acc: 0.9690 - val_loss: 0.1213 - val_acc: 0.9493\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1144 - acc: 0.9633 - val_loss: 0.1098 - val_acc: 0.9577\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0940 - acc: 0.9643 - val_loss: 0.1031 - val_acc: 0.9662\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0948 - acc: 0.9595 - val_loss: 0.1126 - val_acc: 0.9577\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0904 - acc: 0.9718 - val_loss: 0.2263 - val_acc: 0.9268\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0830 - acc: 0.9746 - val_loss: 0.1463 - val_acc: 0.9408\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1203 - acc: 0.9530 - val_loss: 0.1306 - val_acc: 0.9437\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0708 - acc: 0.9746 - val_loss: 0.0974 - val_acc: 0.9577\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0625 - acc: 0.9737 - val_loss: 0.1375 - val_acc: 0.9606\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1076 - acc: 0.9643 - val_loss: 0.1361 - val_acc: 0.9408\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1039 - acc: 0.9661 - val_loss: 0.2231 - val_acc: 0.9352\n",
            "Score for fold 1: loss of 0.14204929769039154; acc of 94.9367105960846%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 39s 280ms/step - loss: 0.5650 - acc: 0.7197 - val_loss: 0.4913 - val_acc: 0.7831\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3991 - acc: 0.8344 - val_loss: 0.3655 - val_acc: 0.8507\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.3123 - acc: 0.8824 - val_loss: 0.3681 - val_acc: 0.8451\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.3600 - acc: 0.8589 - val_loss: 0.2630 - val_acc: 0.9070\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3570 - acc: 0.8739 - val_loss: 0.2527 - val_acc: 0.9324\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.3284 - acc: 0.8805 - val_loss: 0.2885 - val_acc: 0.8986\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2915 - acc: 0.8862 - val_loss: 0.2102 - val_acc: 0.9155\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2611 - acc: 0.8956 - val_loss: 0.3310 - val_acc: 0.9155\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2520 - acc: 0.9022 - val_loss: 0.1947 - val_acc: 0.9211\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2112 - acc: 0.9247 - val_loss: 0.1618 - val_acc: 0.9521\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2377 - acc: 0.9059 - val_loss: 0.2658 - val_acc: 0.8958\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2083 - acc: 0.9153 - val_loss: 0.2449 - val_acc: 0.9155\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1987 - acc: 0.9219 - val_loss: 0.2761 - val_acc: 0.8817\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2262 - acc: 0.9087 - val_loss: 0.2287 - val_acc: 0.9099\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2060 - acc: 0.9266 - val_loss: 0.2065 - val_acc: 0.9352\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1677 - acc: 0.9351 - val_loss: 0.2210 - val_acc: 0.9099\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1864 - acc: 0.9257 - val_loss: 0.1499 - val_acc: 0.9380\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1545 - acc: 0.9426 - val_loss: 0.3201 - val_acc: 0.8986\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2142 - acc: 0.9219 - val_loss: 0.3253 - val_acc: 0.8479\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2795 - acc: 0.9050 - val_loss: 0.1727 - val_acc: 0.9437\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1776 - acc: 0.9285 - val_loss: 0.1497 - val_acc: 0.9549\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1799 - acc: 0.9341 - val_loss: 0.2344 - val_acc: 0.8930\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1596 - acc: 0.9351 - val_loss: 0.4421 - val_acc: 0.8310\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1642 - acc: 0.9379 - val_loss: 0.2847 - val_acc: 0.9070\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1573 - acc: 0.9454 - val_loss: 0.1473 - val_acc: 0.9437\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1785 - acc: 0.9417 - val_loss: 0.1295 - val_acc: 0.9606\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1376 - acc: 0.9501 - val_loss: 0.2457 - val_acc: 0.9239\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1308 - acc: 0.9558 - val_loss: 0.2435 - val_acc: 0.9296\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1395 - acc: 0.9539 - val_loss: 0.1663 - val_acc: 0.9380\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.0933 - acc: 0.9652 - val_loss: 0.2146 - val_acc: 0.9380\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1212 - acc: 0.9586 - val_loss: 0.1551 - val_acc: 0.9352\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1249 - acc: 0.9558 - val_loss: 0.1343 - val_acc: 0.9521\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.0800 - acc: 0.9746 - val_loss: 0.1636 - val_acc: 0.9577\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1404 - acc: 0.9407 - val_loss: 0.2111 - val_acc: 0.9099\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1140 - acc: 0.9633 - val_loss: 0.1991 - val_acc: 0.9408\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1121 - acc: 0.9605 - val_loss: 0.1775 - val_acc: 0.9437\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1790 - acc: 0.9379 - val_loss: 0.3822 - val_acc: 0.8648\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1936 - acc: 0.9323 - val_loss: 0.1958 - val_acc: 0.9211\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1340 - acc: 0.9539 - val_loss: 0.1974 - val_acc: 0.9296\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0993 - acc: 0.9614 - val_loss: 0.1169 - val_acc: 0.9634\n",
            "Score for fold 2: loss of 0.09403183311223984; acc of 96.20253443717957%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 39s 280ms/step - loss: 0.5753 - acc: 0.7021 - val_loss: 0.4812 - val_acc: 0.7599\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.4671 - acc: 0.7763 - val_loss: 0.7644 - val_acc: 0.7062\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.4106 - acc: 0.8412 - val_loss: 0.2759 - val_acc: 0.9011\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.3434 - acc: 0.8684 - val_loss: 0.3472 - val_acc: 0.9011\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.3749 - acc: 0.8562 - val_loss: 0.3416 - val_acc: 0.8898\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.3647 - acc: 0.8374 - val_loss: 0.2354 - val_acc: 0.9096\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2686 - acc: 0.9004 - val_loss: 0.3021 - val_acc: 0.8531\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.2665 - acc: 0.9079 - val_loss: 0.2103 - val_acc: 0.9209\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2512 - acc: 0.9032 - val_loss: 0.2418 - val_acc: 0.9237\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2333 - acc: 0.9126 - val_loss: 0.4332 - val_acc: 0.8051\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.2246 - acc: 0.9211 - val_loss: 0.2135 - val_acc: 0.9266\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2508 - acc: 0.9032 - val_loss: 0.2265 - val_acc: 0.9124\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2160 - acc: 0.9201 - val_loss: 0.1704 - val_acc: 0.9322\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2107 - acc: 0.9220 - val_loss: 0.4509 - val_acc: 0.7486\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2273 - acc: 0.9248 - val_loss: 0.1680 - val_acc: 0.9294\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2170 - acc: 0.9182 - val_loss: 0.2248 - val_acc: 0.9096\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2166 - acc: 0.9145 - val_loss: 0.1906 - val_acc: 0.9266\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2052 - acc: 0.9173 - val_loss: 0.1679 - val_acc: 0.9379\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1975 - acc: 0.9276 - val_loss: 0.3217 - val_acc: 0.8701\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1983 - acc: 0.9239 - val_loss: 0.1485 - val_acc: 0.9435\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1734 - acc: 0.9352 - val_loss: 0.3446 - val_acc: 0.8446\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1615 - acc: 0.9370 - val_loss: 0.1324 - val_acc: 0.9350\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1585 - acc: 0.9427 - val_loss: 0.4261 - val_acc: 0.8164\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.2066 - acc: 0.9173 - val_loss: 0.2160 - val_acc: 0.8955\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1507 - acc: 0.9398 - val_loss: 0.1332 - val_acc: 0.9407\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1450 - acc: 0.9474 - val_loss: 0.1704 - val_acc: 0.9492\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1752 - acc: 0.9342 - val_loss: 0.1844 - val_acc: 0.9294\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1700 - acc: 0.9380 - val_loss: 0.3933 - val_acc: 0.8898\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1869 - acc: 0.9258 - val_loss: 0.1515 - val_acc: 0.9350\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1278 - acc: 0.9539 - val_loss: 0.1162 - val_acc: 0.9548\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1060 - acc: 0.9652 - val_loss: 0.2534 - val_acc: 0.9096\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.0898 - acc: 0.9709 - val_loss: 0.1100 - val_acc: 0.9605\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.0791 - acc: 0.9756 - val_loss: 0.1305 - val_acc: 0.9492\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0995 - acc: 0.9690 - val_loss: 0.1299 - val_acc: 0.9492\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1088 - acc: 0.9558 - val_loss: 0.2086 - val_acc: 0.9294\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2585 - acc: 0.8994 - val_loss: 0.1644 - val_acc: 0.9379\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1181 - acc: 0.9586 - val_loss: 0.1850 - val_acc: 0.9463\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1007 - acc: 0.9699 - val_loss: 0.1109 - val_acc: 0.9548\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.0734 - acc: 0.9746 - val_loss: 0.1233 - val_acc: 0.9520\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1027 - acc: 0.9596 - val_loss: 0.1273 - val_acc: 0.9548\n",
            "Score for fold 3: loss of 0.044971171766519547; acc of 98.73417615890503%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 39s 281ms/step - loss: 0.5959 - acc: 0.6711 - val_loss: 0.4332 - val_acc: 0.8249\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.3776 - acc: 0.8506 - val_loss: 0.3682 - val_acc: 0.8842\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.3364 - acc: 0.8759 - val_loss: 0.3205 - val_acc: 0.8983\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.3331 - acc: 0.8806 - val_loss: 0.3380 - val_acc: 0.8531\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3253 - acc: 0.8806 - val_loss: 0.2630 - val_acc: 0.8898\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2789 - acc: 0.8966 - val_loss: 0.2433 - val_acc: 0.8955\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2883 - acc: 0.8929 - val_loss: 0.2427 - val_acc: 0.8955\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2675 - acc: 0.8872 - val_loss: 0.2592 - val_acc: 0.8814\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2506 - acc: 0.8966 - val_loss: 0.2143 - val_acc: 0.8927\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2335 - acc: 0.9079 - val_loss: 0.2779 - val_acc: 0.8616\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1730 - acc: 0.9380 - val_loss: 0.1795 - val_acc: 0.9237\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2219 - acc: 0.9070 - val_loss: 0.2163 - val_acc: 0.8955\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2668 - acc: 0.9211 - val_loss: 0.5501 - val_acc: 0.7571\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2549 - acc: 0.9023 - val_loss: 0.1964 - val_acc: 0.8870\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1771 - acc: 0.9342 - val_loss: 0.2994 - val_acc: 0.8418\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1911 - acc: 0.9361 - val_loss: 0.1704 - val_acc: 0.9322\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1386 - acc: 0.9492 - val_loss: 0.1447 - val_acc: 0.9492\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1601 - acc: 0.9352 - val_loss: 0.3754 - val_acc: 0.8701\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1658 - acc: 0.9342 - val_loss: 0.2135 - val_acc: 0.9237\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2717 - acc: 0.8947 - val_loss: 0.2041 - val_acc: 0.9379\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1764 - acc: 0.9370 - val_loss: 0.2945 - val_acc: 0.8927\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1903 - acc: 0.9314 - val_loss: 0.1537 - val_acc: 0.9181\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1187 - acc: 0.9624 - val_loss: 0.2751 - val_acc: 0.8870\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1323 - acc: 0.9492 - val_loss: 0.2141 - val_acc: 0.9096\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1795 - acc: 0.9389 - val_loss: 0.1729 - val_acc: 0.9266\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1699 - acc: 0.9398 - val_loss: 0.1387 - val_acc: 0.9463\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1055 - acc: 0.9624 - val_loss: 0.1615 - val_acc: 0.9209\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1332 - acc: 0.9586 - val_loss: 0.3691 - val_acc: 0.8898\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1013 - acc: 0.9690 - val_loss: 0.1306 - val_acc: 0.9548\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1219 - acc: 0.9483 - val_loss: 0.1871 - val_acc: 0.9209\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.0848 - acc: 0.9709 - val_loss: 0.1814 - val_acc: 0.9096\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.0903 - acc: 0.9699 - val_loss: 0.1327 - val_acc: 0.9435\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0852 - acc: 0.9652 - val_loss: 0.1908 - val_acc: 0.9237\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1389 - acc: 0.9539 - val_loss: 0.1599 - val_acc: 0.9576\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1008 - acc: 0.9690 - val_loss: 0.1577 - val_acc: 0.9379\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0677 - acc: 0.9756 - val_loss: 0.1576 - val_acc: 0.9266\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0913 - acc: 0.9718 - val_loss: 0.3373 - val_acc: 0.8305\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1387 - acc: 0.9577 - val_loss: 0.1400 - val_acc: 0.9548\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1112 - acc: 0.9671 - val_loss: 0.8043 - val_acc: 0.8136\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1744 - acc: 0.9474 - val_loss: 0.1737 - val_acc: 0.9435\n",
            "Score for fold 4: loss of 0.10350416600704193; acc of 97.46835231781006%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.14204929769039154 - Accuracy: 94.9367105960846%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.09403183311223984 - Accuracy: 96.20253443717957%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.044971171766519547 - Accuracy: 98.73417615890503%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.10350416600704193 - Accuracy: 97.46835231781006%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 96.83544337749481 (+- 1.4152314125140009)\n",
            "> Loss: 0.09613911714404821\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 eme preprocessing : SSIM + fin de scan"
      ],
      "metadata": {
        "id": "LIf-JHRn807g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = importation(\"/content/drive/MyDrive/UV PROJET P6/dataset_normal_methode_2.1_3.2.hdf5\", \n",
        "                      \"/content/drive/MyDrive/UV PROJET P6/dataset_malade_methode_2.1_3.2.hdf5\")\n",
        "Data, Y = labelling(Dataset)\n",
        "Data_train, Data_test, y_train, y_test = train_test_split(Data, Y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "tutn2uESG0Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KfoldValidation2D(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYWnnAh6G8o0",
        "outputId": "d3713788-e287-47d8-bab6-c82adcbf5ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 43s 309ms/step - loss: 0.5378 - acc: 0.7554 - val_loss: 0.5090 - val_acc: 0.7606\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.4603 - acc: 0.7949 - val_loss: 0.4891 - val_acc: 0.7606\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3844 - acc: 0.8448 - val_loss: 0.4623 - val_acc: 0.8141\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.3737 - acc: 0.8438 - val_loss: 0.4964 - val_acc: 0.6423\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.4764 - acc: 0.7996 - val_loss: 0.4560 - val_acc: 0.8169\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.4050 - acc: 0.8354 - val_loss: 0.5168 - val_acc: 0.8254\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.3607 - acc: 0.8448 - val_loss: 0.4306 - val_acc: 0.8479\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.3239 - acc: 0.8730 - val_loss: 0.2515 - val_acc: 0.8817\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2813 - acc: 0.8815 - val_loss: 0.2081 - val_acc: 0.9183\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2777 - acc: 0.8899 - val_loss: 0.2898 - val_acc: 0.9014\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2928 - acc: 0.8975 - val_loss: 0.2615 - val_acc: 0.8901\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2374 - acc: 0.9144 - val_loss: 0.1846 - val_acc: 0.9352\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2187 - acc: 0.9153 - val_loss: 0.2055 - val_acc: 0.9099\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2120 - acc: 0.9266 - val_loss: 0.1673 - val_acc: 0.9380\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1797 - acc: 0.9332 - val_loss: 0.1442 - val_acc: 0.9493\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1875 - acc: 0.9389 - val_loss: 0.1602 - val_acc: 0.9521\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1776 - acc: 0.9360 - val_loss: 0.3271 - val_acc: 0.8958\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1925 - acc: 0.9238 - val_loss: 0.1633 - val_acc: 0.9437\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1655 - acc: 0.9454 - val_loss: 0.3057 - val_acc: 0.8563\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1702 - acc: 0.9398 - val_loss: 0.1612 - val_acc: 0.9408\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1112 - acc: 0.9605 - val_loss: 0.1208 - val_acc: 0.9577\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1422 - acc: 0.9520 - val_loss: 0.1478 - val_acc: 0.9549\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1706 - acc: 0.9398 - val_loss: 0.1073 - val_acc: 0.9690\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1195 - acc: 0.9520 - val_loss: 0.1279 - val_acc: 0.9324\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1237 - acc: 0.9643 - val_loss: 0.1457 - val_acc: 0.9718\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1005 - acc: 0.9624 - val_loss: 0.1145 - val_acc: 0.9775\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1041 - acc: 0.9671 - val_loss: 0.1699 - val_acc: 0.9549\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.0961 - acc: 0.9558 - val_loss: 0.1143 - val_acc: 0.9746\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1960 - acc: 0.9238 - val_loss: 0.2036 - val_acc: 0.9211\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.0991 - acc: 0.9671 - val_loss: 0.1595 - val_acc: 0.9549\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1019 - acc: 0.9605 - val_loss: 0.1228 - val_acc: 0.9690\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.0730 - acc: 0.9755 - val_loss: 0.1166 - val_acc: 0.9634\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1070 - acc: 0.9690 - val_loss: 0.1270 - val_acc: 0.9634\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1205 - acc: 0.9558 - val_loss: 0.3806 - val_acc: 0.9042\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1335 - acc: 0.9417 - val_loss: 0.1267 - val_acc: 0.9606\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1127 - acc: 0.9671 - val_loss: 0.1438 - val_acc: 0.9577\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1294 - acc: 0.9464 - val_loss: 0.1267 - val_acc: 0.9690\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1810 - acc: 0.9304 - val_loss: 0.1625 - val_acc: 0.9296\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.0974 - acc: 0.9558 - val_loss: 0.1431 - val_acc: 0.9437\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1107 - acc: 0.9558 - val_loss: 0.2021 - val_acc: 0.9437\n",
            "Score for fold 1: loss of 0.2530246376991272; acc of 95.56962251663208%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 40s 285ms/step - loss: 0.5587 - acc: 0.7178 - val_loss: 0.4796 - val_acc: 0.8085\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.4504 - acc: 0.8062 - val_loss: 0.3257 - val_acc: 0.8761\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.3344 - acc: 0.8730 - val_loss: 0.3079 - val_acc: 0.8761\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.3607 - acc: 0.8645 - val_loss: 0.3446 - val_acc: 0.8704\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.3465 - acc: 0.8655 - val_loss: 0.2982 - val_acc: 0.8761\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2861 - acc: 0.8777 - val_loss: 0.2425 - val_acc: 0.8901\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2638 - acc: 0.8833 - val_loss: 0.3283 - val_acc: 0.8845\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2572 - acc: 0.9022 - val_loss: 0.2042 - val_acc: 0.9042\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2427 - acc: 0.9087 - val_loss: 0.2150 - val_acc: 0.9042\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2236 - acc: 0.9200 - val_loss: 0.2030 - val_acc: 0.9239\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2003 - acc: 0.9266 - val_loss: 0.2984 - val_acc: 0.8704\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2004 - acc: 0.9210 - val_loss: 0.1957 - val_acc: 0.9324\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2515 - acc: 0.9059 - val_loss: 0.2676 - val_acc: 0.8901\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1845 - acc: 0.9351 - val_loss: 0.1865 - val_acc: 0.9296\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.1562 - acc: 0.9351 - val_loss: 0.1799 - val_acc: 0.9296\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1555 - acc: 0.9351 - val_loss: 0.4318 - val_acc: 0.8225\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1722 - acc: 0.9351 - val_loss: 0.2132 - val_acc: 0.9239\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1368 - acc: 0.9454 - val_loss: 0.1526 - val_acc: 0.9380\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.1577 - acc: 0.9407 - val_loss: 0.1827 - val_acc: 0.9408\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.1283 - acc: 0.9445 - val_loss: 0.2576 - val_acc: 0.8986\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1137 - acc: 0.9548 - val_loss: 0.1769 - val_acc: 0.9380\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.1129 - acc: 0.9530 - val_loss: 0.2980 - val_acc: 0.8761\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1934 - acc: 0.9332 - val_loss: 0.6219 - val_acc: 0.7155\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.4641 - acc: 0.7930 - val_loss: 0.4591 - val_acc: 0.8254\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2915 - acc: 0.8928 - val_loss: 0.4147 - val_acc: 0.8028\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2863 - acc: 0.8975 - val_loss: 0.3413 - val_acc: 0.8789\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1911 - acc: 0.9257 - val_loss: 0.2061 - val_acc: 0.9239\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1467 - acc: 0.9454 - val_loss: 0.3055 - val_acc: 0.8873\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1696 - acc: 0.9313 - val_loss: 0.2247 - val_acc: 0.9127\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1782 - acc: 0.9360 - val_loss: 0.1966 - val_acc: 0.9268\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1189 - acc: 0.9577 - val_loss: 0.2331 - val_acc: 0.9099\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1220 - acc: 0.9548 - val_loss: 0.1791 - val_acc: 0.9493\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1163 - acc: 0.9539 - val_loss: 0.2160 - val_acc: 0.9183\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 39s 295ms/step - loss: 0.1156 - acc: 0.9539 - val_loss: 0.3332 - val_acc: 0.8732\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1269 - acc: 0.9548 - val_loss: 0.3264 - val_acc: 0.9099\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1553 - acc: 0.9483 - val_loss: 0.1917 - val_acc: 0.9380\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1011 - acc: 0.9577 - val_loss: 0.2040 - val_acc: 0.9408\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1104 - acc: 0.9567 - val_loss: 0.2430 - val_acc: 0.9352\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.0997 - acc: 0.9661 - val_loss: 0.1602 - val_acc: 0.9521\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.0910 - acc: 0.9652 - val_loss: 0.1919 - val_acc: 0.9521\n",
            "Score for fold 2: loss of 0.05789584293961525; acc of 98.10126423835754%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 39s 283ms/step - loss: 0.5811 - acc: 0.7030 - val_loss: 0.4292 - val_acc: 0.8164\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.4378 - acc: 0.8102 - val_loss: 0.2547 - val_acc: 0.9153\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.3489 - acc: 0.8571 - val_loss: 0.2761 - val_acc: 0.9124\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.3377 - acc: 0.8675 - val_loss: 0.2271 - val_acc: 0.9153\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.3161 - acc: 0.8684 - val_loss: 0.2454 - val_acc: 0.9011\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.2650 - acc: 0.9070 - val_loss: 0.2492 - val_acc: 0.9011\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.2802 - acc: 0.8863 - val_loss: 0.3162 - val_acc: 0.8305\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.2731 - acc: 0.8919 - val_loss: 0.2658 - val_acc: 0.8842\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.3906 - acc: 0.8393 - val_loss: 0.2497 - val_acc: 0.8983\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.3942 - acc: 0.8167 - val_loss: 0.3037 - val_acc: 0.8729\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.3714 - acc: 0.8252 - val_loss: 0.2959 - val_acc: 0.8870\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.3251 - acc: 0.8788 - val_loss: 0.2111 - val_acc: 0.9153\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2489 - acc: 0.9041 - val_loss: 0.3161 - val_acc: 0.8531\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2188 - acc: 0.9182 - val_loss: 0.1831 - val_acc: 0.9237\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1997 - acc: 0.9239 - val_loss: 0.5993 - val_acc: 0.6977\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1958 - acc: 0.9323 - val_loss: 0.2141 - val_acc: 0.8983\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.1988 - acc: 0.9201 - val_loss: 0.1957 - val_acc: 0.9209\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1975 - acc: 0.9295 - val_loss: 0.1730 - val_acc: 0.9435\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1861 - acc: 0.9295 - val_loss: 0.6186 - val_acc: 0.7090\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2408 - acc: 0.9060 - val_loss: 0.2397 - val_acc: 0.8870\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2267 - acc: 0.9126 - val_loss: 0.1561 - val_acc: 0.9407\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.4018 - acc: 0.8280 - val_loss: 0.4026 - val_acc: 0.8362\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.4458 - acc: 0.8064 - val_loss: 0.3691 - val_acc: 0.8588\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.4077 - acc: 0.8327 - val_loss: 0.3369 - val_acc: 0.8785\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2988 - acc: 0.8788 - val_loss: 0.2414 - val_acc: 0.8955\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2698 - acc: 0.8891 - val_loss: 0.2176 - val_acc: 0.9040\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2524 - acc: 0.9135 - val_loss: 0.2644 - val_acc: 0.8814\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.2278 - acc: 0.9088 - val_loss: 0.2095 - val_acc: 0.9068\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1770 - acc: 0.9314 - val_loss: 0.1362 - val_acc: 0.9576\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1691 - acc: 0.9380 - val_loss: 0.2984 - val_acc: 0.8785\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1547 - acc: 0.9427 - val_loss: 0.1932 - val_acc: 0.9322\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1718 - acc: 0.9427 - val_loss: 0.1875 - val_acc: 0.9322\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1186 - acc: 0.9596 - val_loss: 0.1399 - val_acc: 0.9322\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1650 - acc: 0.9427 - val_loss: 0.1512 - val_acc: 0.9407\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1093 - acc: 0.9624 - val_loss: 0.2126 - val_acc: 0.9294\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.0945 - acc: 0.9709 - val_loss: 0.1109 - val_acc: 0.9576\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1179 - acc: 0.9633 - val_loss: 0.4728 - val_acc: 0.7994\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1401 - acc: 0.9398 - val_loss: 0.1176 - val_acc: 0.9633\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.0967 - acc: 0.9709 - val_loss: 0.1635 - val_acc: 0.9322\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0939 - acc: 0.9643 - val_loss: 0.1523 - val_acc: 0.9435\n",
            "Score for fold 3: loss of 0.08327232301235199; acc of 96.20253443717957%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 39s 282ms/step - loss: 0.5583 - acc: 0.7199 - val_loss: 0.3964 - val_acc: 0.8475\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.4124 - acc: 0.8280 - val_loss: 0.3299 - val_acc: 0.8785\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.4265 - acc: 0.8092 - val_loss: 0.5398 - val_acc: 0.7655\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.4254 - acc: 0.8299 - val_loss: 0.4708 - val_acc: 0.8616\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.3800 - acc: 0.8355 - val_loss: 0.9146 - val_acc: 0.6554\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.3494 - acc: 0.8365 - val_loss: 0.1856 - val_acc: 0.9068\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.3540 - acc: 0.8468 - val_loss: 0.5420 - val_acc: 0.7119\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.3326 - acc: 0.8628 - val_loss: 0.2297 - val_acc: 0.8927\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.3245 - acc: 0.8618 - val_loss: 0.1974 - val_acc: 0.9040\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.3375 - acc: 0.8797 - val_loss: 0.4301 - val_acc: 0.8249\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.3115 - acc: 0.8797 - val_loss: 0.2173 - val_acc: 0.9068\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2551 - acc: 0.8938 - val_loss: 0.2721 - val_acc: 0.9124\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2783 - acc: 0.8919 - val_loss: 0.2465 - val_acc: 0.9068\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2388 - acc: 0.9173 - val_loss: 0.3170 - val_acc: 0.8503\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2473 - acc: 0.9088 - val_loss: 0.1734 - val_acc: 0.9181\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2252 - acc: 0.9051 - val_loss: 0.1759 - val_acc: 0.9209\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2141 - acc: 0.9182 - val_loss: 0.1546 - val_acc: 0.9266\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2020 - acc: 0.9220 - val_loss: 0.2080 - val_acc: 0.9153\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1767 - acc: 0.9248 - val_loss: 0.1526 - val_acc: 0.9237\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1897 - acc: 0.9201 - val_loss: 0.2225 - val_acc: 0.9266\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1821 - acc: 0.9352 - val_loss: 0.3837 - val_acc: 0.8814\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1796 - acc: 0.9408 - val_loss: 0.1625 - val_acc: 0.9379\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1708 - acc: 0.9361 - val_loss: 0.1644 - val_acc: 0.9237\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1495 - acc: 0.9464 - val_loss: 0.1341 - val_acc: 0.9520\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1430 - acc: 0.9455 - val_loss: 0.1483 - val_acc: 0.9463\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1488 - acc: 0.9427 - val_loss: 0.1154 - val_acc: 0.9492\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1956 - acc: 0.9267 - val_loss: 0.1514 - val_acc: 0.9379\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1246 - acc: 0.9549 - val_loss: 0.1698 - val_acc: 0.9209\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1308 - acc: 0.9549 - val_loss: 0.1628 - val_acc: 0.9322\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1498 - acc: 0.9380 - val_loss: 0.1423 - val_acc: 0.9407\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1591 - acc: 0.9427 - val_loss: 0.1527 - val_acc: 0.9407\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1720 - acc: 0.9389 - val_loss: 0.1803 - val_acc: 0.9068\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1474 - acc: 0.9511 - val_loss: 0.1501 - val_acc: 0.9407\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2061 - acc: 0.9314 - val_loss: 0.3822 - val_acc: 0.8559\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1500 - acc: 0.9436 - val_loss: 0.1171 - val_acc: 0.9492\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1119 - acc: 0.9539 - val_loss: 0.1501 - val_acc: 0.9548\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1124 - acc: 0.9558 - val_loss: 0.1392 - val_acc: 0.9435\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.1075 - acc: 0.9615 - val_loss: 0.3138 - val_acc: 0.9237\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.0966 - acc: 0.9671 - val_loss: 0.1537 - val_acc: 0.9661\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1239 - acc: 0.9521 - val_loss: 0.1011 - val_acc: 0.9661\n",
            "Score for fold 4: loss of 0.11931820213794708; acc of 95.56962251663208%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.2530246376991272 - Accuracy: 95.56962251663208%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.05789584293961525 - Accuracy: 98.10126423835754%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.08327232301235199 - Accuracy: 96.20253443717957%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.11931820213794708 - Accuracy: 95.56962251663208%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 96.36076092720032 (+- 1.0375677529258547)\n",
            "> Loss: 0.12837775144726038\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6 eme pre processing : SSIM + debut et fin de scan"
      ],
      "metadata": {
        "id": "CS79_6II81Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = importation(\"/content/drive/MyDrive/UV PROJET P6/dataset_normal_methode_2.3_3.2.hdf5\", \n",
        "                      \"/content/drive/MyDrive/UV PROJET P6/dataset_malade_methode_2.3_3.2.hdf5\")\n",
        "Data, Y = labelling(Dataset)\n",
        "\n",
        "Data_train, Data_test, y_train, y_test = train_test_split(Data, Y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "u2pPIWH5G0yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KfoldValidation2D(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCYeB5dfG9uw",
        "outputId": "e2c95ab0-acb0-48bb-e8d3-d515ce209ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 49s 327ms/step - loss: 0.5577 - acc: 0.7197 - val_loss: 0.3946 - val_acc: 0.8451\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.4570 - acc: 0.8175 - val_loss: 0.2540 - val_acc: 0.8873\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.3244 - acc: 0.8617 - val_loss: 0.2344 - val_acc: 0.9183\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2907 - acc: 0.8881 - val_loss: 0.6557 - val_acc: 0.7690\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3058 - acc: 0.8833 - val_loss: 0.2095 - val_acc: 0.9099\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2584 - acc: 0.8852 - val_loss: 0.1934 - val_acc: 0.9211\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2034 - acc: 0.9163 - val_loss: 0.4508 - val_acc: 0.9099\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2469 - acc: 0.9069 - val_loss: 0.1709 - val_acc: 0.9380\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2220 - acc: 0.9087 - val_loss: 0.1991 - val_acc: 0.9324\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.2506 - acc: 0.9012 - val_loss: 0.3435 - val_acc: 0.9042\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1972 - acc: 0.9294 - val_loss: 0.1809 - val_acc: 0.9408\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2225 - acc: 0.9106 - val_loss: 0.5028 - val_acc: 0.8676\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2083 - acc: 0.9219 - val_loss: 0.1768 - val_acc: 0.9296\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2142 - acc: 0.9153 - val_loss: 0.1482 - val_acc: 0.9437\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.2088 - acc: 0.9238 - val_loss: 0.1893 - val_acc: 0.9239\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1861 - acc: 0.9266 - val_loss: 0.1815 - val_acc: 0.9324\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1907 - acc: 0.9285 - val_loss: 0.2587 - val_acc: 0.9070\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2881 - acc: 0.8890 - val_loss: 0.2085 - val_acc: 0.9127\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2108 - acc: 0.9172 - val_loss: 0.1856 - val_acc: 0.9324\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2215 - acc: 0.9200 - val_loss: 0.1458 - val_acc: 0.9380\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2093 - acc: 0.9144 - val_loss: 0.2015 - val_acc: 0.9211\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2127 - acc: 0.9247 - val_loss: 0.2366 - val_acc: 0.9099\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1910 - acc: 0.9294 - val_loss: 0.2075 - val_acc: 0.9211\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1622 - acc: 0.9417 - val_loss: 0.1776 - val_acc: 0.9380\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1331 - acc: 0.9539 - val_loss: 0.1955 - val_acc: 0.9465\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1364 - acc: 0.9436 - val_loss: 0.2041 - val_acc: 0.9296\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1586 - acc: 0.9417 - val_loss: 1.1191 - val_acc: 0.6338\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2629 - acc: 0.9040 - val_loss: 0.1827 - val_acc: 0.9268\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1325 - acc: 0.9586 - val_loss: 0.1509 - val_acc: 0.9408\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1498 - acc: 0.9379 - val_loss: 0.1703 - val_acc: 0.9324\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1394 - acc: 0.9539 - val_loss: 0.2440 - val_acc: 0.9070\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1333 - acc: 0.9501 - val_loss: 0.1810 - val_acc: 0.9352\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1129 - acc: 0.9567 - val_loss: 0.2155 - val_acc: 0.9211\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1442 - acc: 0.9417 - val_loss: 0.2826 - val_acc: 0.9070\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2137 - acc: 0.9266 - val_loss: 0.2806 - val_acc: 0.8789\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.2346 - acc: 0.9153 - val_loss: 0.2658 - val_acc: 0.9099\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1501 - acc: 0.9389 - val_loss: 0.2041 - val_acc: 0.9239\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1729 - acc: 0.9379 - val_loss: 0.1748 - val_acc: 0.9465\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1130 - acc: 0.9680 - val_loss: 0.1943 - val_acc: 0.9268\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.1365 - acc: 0.9370Restoring model weights from the end of the best epoch: 25.\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1365 - acc: 0.9370 - val_loss: 0.1615 - val_acc: 0.9437\n",
            "Epoch 40: early stopping\n",
            "Score for fold 1: loss of 0.12104811519384384; acc of 96.20253443717957%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 40s 281ms/step - loss: 0.5735 - acc: 0.7084 - val_loss: 0.5135 - val_acc: 0.7634\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.4194 - acc: 0.8109 - val_loss: 0.2693 - val_acc: 0.8930\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.4263 - acc: 0.8109 - val_loss: 0.3714 - val_acc: 0.8141\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3492 - acc: 0.8608 - val_loss: 0.2389 - val_acc: 0.9042\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3001 - acc: 0.8749 - val_loss: 0.2707 - val_acc: 0.8648\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2919 - acc: 0.8786 - val_loss: 0.1864 - val_acc: 0.9352\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.2395 - acc: 0.9031 - val_loss: 0.3145 - val_acc: 0.8535\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2280 - acc: 0.8993 - val_loss: 0.1843 - val_acc: 0.9239\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2277 - acc: 0.9040 - val_loss: 0.2091 - val_acc: 0.8958\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2107 - acc: 0.9172 - val_loss: 0.1718 - val_acc: 0.9268\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1883 - acc: 0.9304 - val_loss: 0.2955 - val_acc: 0.8930\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2403 - acc: 0.9003 - val_loss: 0.1800 - val_acc: 0.9380\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1923 - acc: 0.9210 - val_loss: 0.2672 - val_acc: 0.9099\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1830 - acc: 0.9341 - val_loss: 0.3185 - val_acc: 0.8958\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1829 - acc: 0.9323 - val_loss: 0.1961 - val_acc: 0.9521\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1905 - acc: 0.9257 - val_loss: 0.1531 - val_acc: 0.9437\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1937 - acc: 0.9153 - val_loss: 0.3270 - val_acc: 0.8732\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1682 - acc: 0.9266 - val_loss: 0.1227 - val_acc: 0.9549\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1436 - acc: 0.9332 - val_loss: 0.1442 - val_acc: 0.9521\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1924 - acc: 0.9172 - val_loss: 0.1558 - val_acc: 0.9634\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1562 - acc: 0.9285 - val_loss: 0.1115 - val_acc: 0.9662\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1275 - acc: 0.9530 - val_loss: 0.1636 - val_acc: 0.9408\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1239 - acc: 0.9558 - val_loss: 0.1032 - val_acc: 0.9577\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1118 - acc: 0.9586 - val_loss: 0.1705 - val_acc: 0.9493\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1155 - acc: 0.9558 - val_loss: 0.1054 - val_acc: 0.9775\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1115 - acc: 0.9624 - val_loss: 0.1283 - val_acc: 0.9606\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1022 - acc: 0.9577 - val_loss: 0.1123 - val_acc: 0.9577\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1022 - acc: 0.9633 - val_loss: 0.0941 - val_acc: 0.9746\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0945 - acc: 0.9708 - val_loss: 0.2290 - val_acc: 0.9352\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1374 - acc: 0.9426 - val_loss: 0.1728 - val_acc: 0.9324\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0754 - acc: 0.9680 - val_loss: 0.0913 - val_acc: 0.9775\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0726 - acc: 0.9718 - val_loss: 0.1131 - val_acc: 0.9690\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0822 - acc: 0.9680 - val_loss: 0.1898 - val_acc: 0.9634\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1292 - acc: 0.9511 - val_loss: 0.1996 - val_acc: 0.9127\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1789 - acc: 0.9454 - val_loss: 0.2289 - val_acc: 0.9465\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1265 - acc: 0.9586 - val_loss: 0.2814 - val_acc: 0.8817\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1836 - acc: 0.9257 - val_loss: 0.1356 - val_acc: 0.9549\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1302 - acc: 0.9501 - val_loss: 0.0942 - val_acc: 0.9718\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1002 - acc: 0.9643 - val_loss: 0.1313 - val_acc: 0.9606\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.0885 - acc: 0.9652Restoring model weights from the end of the best epoch: 25.\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0885 - acc: 0.9652 - val_loss: 0.1409 - val_acc: 0.9437\n",
            "Epoch 40: early stopping\n",
            "Score for fold 2: loss of 0.13369077444076538; acc of 96.83544039726257%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 39s 285ms/step - loss: 0.5306 - acc: 0.7350 - val_loss: 0.5236 - val_acc: 0.7966\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.4416 - acc: 0.8148 - val_loss: 0.4714 - val_acc: 0.7994\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.3252 - acc: 0.8712 - val_loss: 0.5297 - val_acc: 0.7825\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3028 - acc: 0.8806 - val_loss: 0.3649 - val_acc: 0.8475\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2890 - acc: 0.8882 - val_loss: 0.4020 - val_acc: 0.7994\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.3223 - acc: 0.8430 - val_loss: 0.3809 - val_acc: 0.8588\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2599 - acc: 0.8797 - val_loss: 0.3632 - val_acc: 0.8446\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.2206 - acc: 0.9135 - val_loss: 0.4690 - val_acc: 0.7797\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2385 - acc: 0.9135 - val_loss: 0.2438 - val_acc: 0.9124\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1992 - acc: 0.9145 - val_loss: 0.2350 - val_acc: 0.8955\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1811 - acc: 0.9248 - val_loss: 0.3329 - val_acc: 0.8644\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2223 - acc: 0.9098 - val_loss: 0.2232 - val_acc: 0.9237\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1742 - acc: 0.9361 - val_loss: 0.1817 - val_acc: 0.9379\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1813 - acc: 0.9342 - val_loss: 0.2916 - val_acc: 0.8927\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1475 - acc: 0.9389 - val_loss: 0.1656 - val_acc: 0.9435\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2318 - acc: 0.9023 - val_loss: 0.2139 - val_acc: 0.9209\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1317 - acc: 0.9530 - val_loss: 0.2214 - val_acc: 0.9209\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1427 - acc: 0.9521 - val_loss: 0.1970 - val_acc: 0.9350\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1208 - acc: 0.9633 - val_loss: 0.2308 - val_acc: 0.9124\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1190 - acc: 0.9530 - val_loss: 0.2141 - val_acc: 0.9209\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0996 - acc: 0.9662 - val_loss: 0.3861 - val_acc: 0.8249\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1423 - acc: 0.9492 - val_loss: 0.1829 - val_acc: 0.9576\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1157 - acc: 0.9568 - val_loss: 0.1860 - val_acc: 0.9492\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1076 - acc: 0.9577 - val_loss: 0.2275 - val_acc: 0.9492\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1279 - acc: 0.9445 - val_loss: 0.2905 - val_acc: 0.9209\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1012 - acc: 0.9690 - val_loss: 0.2931 - val_acc: 0.9266\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1521 - acc: 0.9511 - val_loss: 0.1794 - val_acc: 0.9407\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1173 - acc: 0.9558 - val_loss: 0.3076 - val_acc: 0.9040\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1385 - acc: 0.9521 - val_loss: 0.2648 - val_acc: 0.8870\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.0749 - acc: 0.9756 - val_loss: 0.2378 - val_acc: 0.9407\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1252 - acc: 0.9558 - val_loss: 0.2285 - val_acc: 0.9379\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.0785 - acc: 0.9699 - val_loss: 0.2516 - val_acc: 0.9237\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.0804 - acc: 0.9699 - val_loss: 0.3235 - val_acc: 0.9266\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.1130 - acc: 0.9577 - val_loss: 0.1676 - val_acc: 0.9576\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.0644 - acc: 0.9756 - val_loss: 0.3505 - val_acc: 0.9350\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 36s 273ms/step - loss: 0.0825 - acc: 0.9652 - val_loss: 0.2850 - val_acc: 0.8757\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.0885 - acc: 0.9662Restoring model weights from the end of the best epoch: 22.\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0885 - acc: 0.9662 - val_loss: 0.1970 - val_acc: 0.9322\n",
            "Epoch 37: early stopping\n",
            "Score for fold 3: loss of 0.09674295783042908; acc of 96.83544039726257%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 40s 286ms/step - loss: 0.5540 - acc: 0.7190 - val_loss: 0.3641 - val_acc: 0.8362\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.4023 - acc: 0.8393 - val_loss: 0.4866 - val_acc: 0.7627\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 37s 277ms/step - loss: 0.3435 - acc: 0.8637 - val_loss: 0.3492 - val_acc: 0.8644\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2914 - acc: 0.8872 - val_loss: 0.3413 - val_acc: 0.8418\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2931 - acc: 0.8769 - val_loss: 2.7421 - val_acc: 0.4887\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.3193 - acc: 0.8675 - val_loss: 0.2887 - val_acc: 0.8757\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 37s 276ms/step - loss: 0.2564 - acc: 0.9013 - val_loss: 0.2685 - val_acc: 0.8983\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2898 - acc: 0.8938 - val_loss: 0.2407 - val_acc: 0.8870\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2621 - acc: 0.9032 - val_loss: 0.2201 - val_acc: 0.9040\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2237 - acc: 0.9041 - val_loss: 0.2545 - val_acc: 0.8870\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.2082 - acc: 0.9220 - val_loss: 0.1855 - val_acc: 0.9096\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1903 - acc: 0.9220 - val_loss: 0.3520 - val_acc: 0.8842\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1848 - acc: 0.9276 - val_loss: 0.2191 - val_acc: 0.9124\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1736 - acc: 0.9380 - val_loss: 0.1708 - val_acc: 0.9294\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1872 - acc: 0.9370 - val_loss: 0.2412 - val_acc: 0.8898\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1798 - acc: 0.9286 - val_loss: 0.4747 - val_acc: 0.8503\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1749 - acc: 0.9352 - val_loss: 0.1947 - val_acc: 0.9294\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1632 - acc: 0.9342 - val_loss: 0.2135 - val_acc: 0.9322\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1535 - acc: 0.9427 - val_loss: 0.2926 - val_acc: 0.8616\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1505 - acc: 0.9427 - val_loss: 0.1523 - val_acc: 0.9407\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.1362 - acc: 0.9549 - val_loss: 0.1493 - val_acc: 0.9407\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1136 - acc: 0.9568 - val_loss: 0.1262 - val_acc: 0.9492\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1402 - acc: 0.9445 - val_loss: 0.3272 - val_acc: 0.8757\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1791 - acc: 0.9342 - val_loss: 0.2444 - val_acc: 0.9209\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1343 - acc: 0.9521 - val_loss: 0.1684 - val_acc: 0.9463\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.0927 - acc: 0.9615 - val_loss: 0.2029 - val_acc: 0.9492\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 36s 275ms/step - loss: 0.1102 - acc: 0.9652 - val_loss: 0.1401 - val_acc: 0.9407\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1279 - acc: 0.9539 - val_loss: 0.1939 - val_acc: 0.9322\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1241 - acc: 0.9577 - val_loss: 0.2348 - val_acc: 0.9040\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1055 - acc: 0.9643 - val_loss: 0.1702 - val_acc: 0.9520\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0878 - acc: 0.9662 - val_loss: 0.1377 - val_acc: 0.9463\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0922 - acc: 0.9652 - val_loss: 0.1753 - val_acc: 0.9181\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0868 - acc: 0.9652 - val_loss: 0.2209 - val_acc: 0.9096\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 37s 275ms/step - loss: 0.0796 - acc: 0.9756 - val_loss: 0.1510 - val_acc: 0.9520\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0761 - acc: 0.9737 - val_loss: 0.2124 - val_acc: 0.9350\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.0847 - acc: 0.9746 - val_loss: 0.1957 - val_acc: 0.9350\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1013 - acc: 0.9690 - val_loss: 0.1719 - val_acc: 0.9379\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1329 - acc: 0.9568 - val_loss: 0.2705 - val_acc: 0.9350\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.2106 - acc: 0.9276 - val_loss: 0.3302 - val_acc: 0.8588\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 36s 274ms/step - loss: 0.1432 - acc: 0.9483 - val_loss: 0.3233 - val_acc: 0.8644\n",
            "Score for fold 4: loss of 0.2781120836734772; acc of 88.60759735107422%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.12104811519384384 - Accuracy: 96.20253443717957%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.13369077444076538 - Accuracy: 96.83544039726257%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.09674295783042908 - Accuracy: 96.83544039726257%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.2781120836734772 - Accuracy: 88.60759735107422%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 94.62025314569473 (+- 3.481011092663449)\n",
            "> Loss: 0.15739848278462887\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modèle 2 : 3DCNN "
      ],
      "metadata": {
        "id": "OjM3WPF06Y00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1er preprocessing : elimination aleatoire + images noires a la fin \n"
      ],
      "metadata": {
        "id": "rLq0h9Cl6w82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = importation(\"/content/drive/MyDrive/UV PROJET P6/dataset_normal_methode_2.1_3.1.hdf5\", \n",
        "                      \"/content/drive/MyDrive/UV PROJET P6/dataset_malade_methode_2.1_3.1.hdf5\")\n",
        "Data, Y = labelling(Dataset)\n",
        "Data_train, Data_test, y_train, y_test = train_test_split(Data, Y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "VCCrtRBpESPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KfoldValidation3D(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JQQ-It7btim",
        "outputId": "20fbb0fe-1478-4470-be0e-5c28d0242250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 46s 336ms/step - loss: 0.9745 - acc: 0.5268 - val_loss: 0.6701 - val_acc: 0.4676\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.6812 - acc: 0.5682 - val_loss: 0.5355 - val_acc: 0.8704\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.5051 - acc: 0.7705 - val_loss: 0.4812 - val_acc: 0.7380\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.4883 - acc: 0.7893 - val_loss: 0.4231 - val_acc: 0.9070\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.4064 - acc: 0.8269 - val_loss: 0.3385 - val_acc: 0.9155\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.3742 - acc: 0.8438 - val_loss: 0.2402 - val_acc: 0.9155\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.3433 - acc: 0.8674 - val_loss: 0.2850 - val_acc: 0.8817\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.3619 - acc: 0.8645 - val_loss: 0.2823 - val_acc: 0.9296\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.3119 - acc: 0.8815 - val_loss: 0.2603 - val_acc: 0.9155\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2736 - acc: 0.8965 - val_loss: 0.2669 - val_acc: 0.9268\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.3217 - acc: 0.8815 - val_loss: 0.2948 - val_acc: 0.9070\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2961 - acc: 0.8833 - val_loss: 0.2319 - val_acc: 0.9239\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.3616 - acc: 0.8627 - val_loss: 0.2517 - val_acc: 0.9183\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2729 - acc: 0.8946 - val_loss: 0.1948 - val_acc: 0.9296\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.3014 - acc: 0.8749 - val_loss: 0.1809 - val_acc: 0.9324\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2614 - acc: 0.9059 - val_loss: 0.2660 - val_acc: 0.8986\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2383 - acc: 0.9078 - val_loss: 0.2051 - val_acc: 0.9437\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2075 - acc: 0.9210 - val_loss: 0.1786 - val_acc: 0.9324\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2715 - acc: 0.8928 - val_loss: 0.2662 - val_acc: 0.9070\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2527 - acc: 0.9050 - val_loss: 0.1569 - val_acc: 0.9324\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2300 - acc: 0.9172 - val_loss: 0.1832 - val_acc: 0.9211\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2096 - acc: 0.9153 - val_loss: 0.1924 - val_acc: 0.9352\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.1674 - acc: 0.9351 - val_loss: 0.1375 - val_acc: 0.9408\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2127 - acc: 0.9276 - val_loss: 0.1739 - val_acc: 0.9437\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2049 - acc: 0.9276 - val_loss: 0.1687 - val_acc: 0.9352\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.1582 - acc: 0.9436 - val_loss: 0.1852 - val_acc: 0.9296\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.1403 - acc: 0.9501 - val_loss: 0.1906 - val_acc: 0.9437\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2160 - acc: 0.9125 - val_loss: 0.2704 - val_acc: 0.8817\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1503 - acc: 0.9398 - val_loss: 0.1264 - val_acc: 0.9437\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1263 - acc: 0.9558 - val_loss: 0.1498 - val_acc: 0.9408\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.1382 - acc: 0.9567 - val_loss: 0.1688 - val_acc: 0.9352\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.1312 - acc: 0.9614Restoring model weights from the end of the best epoch: 17.\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1312 - acc: 0.9614 - val_loss: 0.2005 - val_acc: 0.9380\n",
            "Epoch 32: early stopping\n",
            "Score for fold 1: loss of 0.21366216242313385; acc of 93.03797483444214%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 45s 336ms/step - loss: 1.1368 - acc: 0.4958 - val_loss: 0.6661 - val_acc: 0.5718\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.7453 - acc: 0.5259 - val_loss: 0.7102 - val_acc: 0.4845\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.7347 - acc: 0.4920 - val_loss: 0.6933 - val_acc: 0.4958\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.7121 - acc: 0.5155 - val_loss: 0.6912 - val_acc: 0.5155\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.7055 - acc: 0.5099 - val_loss: 0.6944 - val_acc: 0.5155\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.7056 - acc: 0.5108 - val_loss: 0.6886 - val_acc: 0.4958\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.6986 - acc: 0.5230 - val_loss: 0.6874 - val_acc: 0.5155\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.7033 - acc: 0.4798 - val_loss: 0.6872 - val_acc: 0.4958\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.6710 - acc: 0.5503 - val_loss: 1.2568 - val_acc: 0.5014\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.7157 - acc: 0.4854 - val_loss: 0.6973 - val_acc: 0.5014\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.7068 - acc: 0.4929 - val_loss: 0.6958 - val_acc: 0.5014\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.6972 - acc: 0.4939 - val_loss: 0.6926 - val_acc: 0.4845\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.6874 - acc: 0.5381 - val_loss: 0.6637 - val_acc: 0.7465\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.6867 - acc: 0.5315 - val_loss: 0.6934 - val_acc: 0.5014\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.6643 - acc: 0.5663 - val_loss: 0.5399 - val_acc: 0.5465\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.5101 - acc: 0.7723 - val_loss: 0.4022 - val_acc: 0.8451\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.4491 - acc: 0.8119 - val_loss: 0.3913 - val_acc: 0.8225\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.4127 - acc: 0.8429 - val_loss: 0.3635 - val_acc: 0.8620\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.3820 - acc: 0.8504 - val_loss: 0.3324 - val_acc: 0.8817\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.3774 - acc: 0.8561 - val_loss: 0.5433 - val_acc: 0.6901\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3603 - acc: 0.8429 - val_loss: 0.3086 - val_acc: 0.8592\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3334 - acc: 0.8636 - val_loss: 0.3173 - val_acc: 0.8761\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3105 - acc: 0.8758 - val_loss: 0.2868 - val_acc: 0.8817\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3717 - acc: 0.8570 - val_loss: 0.3069 - val_acc: 0.8761\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3391 - acc: 0.8636 - val_loss: 0.2931 - val_acc: 0.8817\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2898 - acc: 0.8843 - val_loss: 0.2784 - val_acc: 0.8873\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2753 - acc: 0.8899 - val_loss: 0.2793 - val_acc: 0.8845\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2907 - acc: 0.8768 - val_loss: 0.2978 - val_acc: 0.8873\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.3079 - acc: 0.8918 - val_loss: 0.2803 - val_acc: 0.8761\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2833 - acc: 0.8862 - val_loss: 0.2765 - val_acc: 0.8817\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2719 - acc: 0.8937 - val_loss: 0.2524 - val_acc: 0.9014\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2905 - acc: 0.8805 - val_loss: 0.2585 - val_acc: 0.8930\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2671 - acc: 0.8946 - val_loss: 0.2644 - val_acc: 0.8901\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2572 - acc: 0.8918 - val_loss: 0.2442 - val_acc: 0.9099\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2716 - acc: 0.8984 - val_loss: 0.2792 - val_acc: 0.8986\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2583 - acc: 0.8956 - val_loss: 0.2906 - val_acc: 0.8817\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2355 - acc: 0.9172 - val_loss: 0.2507 - val_acc: 0.8986\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2226 - acc: 0.9182 - val_loss: 0.2560 - val_acc: 0.8901\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2498 - acc: 0.9022 - val_loss: 0.2400 - val_acc: 0.8958\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 44s 333ms/step - loss: 0.2383 - acc: 0.9135 - val_loss: 0.2645 - val_acc: 0.8958\n",
            "Score for fold 2: loss of 0.24669936299324036; acc of 91.77215099334717%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 46s 338ms/step - loss: 1.0101 - acc: 0.4962 - val_loss: 0.6941 - val_acc: 0.4492\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.6846 - acc: 0.5733 - val_loss: 0.5939 - val_acc: 0.7458\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.6550 - acc: 0.6457 - val_loss: 0.3912 - val_acc: 0.8588\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.5358 - acc: 0.7472 - val_loss: 0.3336 - val_acc: 0.8559\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.4125 - acc: 0.8365 - val_loss: 0.3682 - val_acc: 0.8588\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3693 - acc: 0.8628 - val_loss: 0.3279 - val_acc: 0.8588\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3647 - acc: 0.8440 - val_loss: 0.4027 - val_acc: 0.8277\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3549 - acc: 0.8506 - val_loss: 0.3110 - val_acc: 0.8644\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3369 - acc: 0.8647 - val_loss: 0.2853 - val_acc: 0.8757\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.4014 - acc: 0.8477 - val_loss: 0.3552 - val_acc: 0.8362\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.3322 - acc: 0.8656 - val_loss: 0.2874 - val_acc: 0.8870\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2985 - acc: 0.8929 - val_loss: 0.4019 - val_acc: 0.8305\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2929 - acc: 0.9032 - val_loss: 0.3199 - val_acc: 0.8701\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2667 - acc: 0.9051 - val_loss: 0.3228 - val_acc: 0.8955\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2667 - acc: 0.9060 - val_loss: 0.2606 - val_acc: 0.8955\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2574 - acc: 0.9041 - val_loss: 0.2571 - val_acc: 0.8842\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2622 - acc: 0.9079 - val_loss: 0.2411 - val_acc: 0.8842\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3094 - acc: 0.8994 - val_loss: 0.2464 - val_acc: 0.9011\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2540 - acc: 0.9126 - val_loss: 0.2588 - val_acc: 0.8842\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2102 - acc: 0.9220 - val_loss: 0.2335 - val_acc: 0.8983\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2163 - acc: 0.9342 - val_loss: 0.2436 - val_acc: 0.9040\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2072 - acc: 0.9258 - val_loss: 0.2549 - val_acc: 0.8927\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2390 - acc: 0.9201 - val_loss: 0.2359 - val_acc: 0.9011\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2025 - acc: 0.9267 - val_loss: 0.2164 - val_acc: 0.9040\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2238 - acc: 0.9154 - val_loss: 0.2341 - val_acc: 0.8870\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1839 - acc: 0.9333 - val_loss: 0.2254 - val_acc: 0.9237\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2060 - acc: 0.9220 - val_loss: 0.2436 - val_acc: 0.8870\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1715 - acc: 0.9342 - val_loss: 0.2048 - val_acc: 0.9181\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1664 - acc: 0.9464 - val_loss: 0.2467 - val_acc: 0.9237\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1706 - acc: 0.9342 - val_loss: 0.2402 - val_acc: 0.9209\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1794 - acc: 0.9323 - val_loss: 0.2583 - val_acc: 0.8983\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1865 - acc: 0.9342 - val_loss: 0.2358 - val_acc: 0.8983\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1632 - acc: 0.9398 - val_loss: 0.2066 - val_acc: 0.9237\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1515 - acc: 0.9483 - val_loss: 0.2344 - val_acc: 0.9294\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1767 - acc: 0.9445 - val_loss: 0.2085 - val_acc: 0.9294\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1121 - acc: 0.9549 - val_loss: 0.2416 - val_acc: 0.9294\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1371 - acc: 0.9605 - val_loss: 0.2733 - val_acc: 0.9124\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.0992 - acc: 0.9596 - val_loss: 0.2355 - val_acc: 0.9379\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1493 - acc: 0.9455 - val_loss: 0.2361 - val_acc: 0.9237\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1303 - acc: 0.9568 - val_loss: 0.2381 - val_acc: 0.9407\n",
            "Score for fold 3: loss of 0.18603329360485077; acc of 92.40506291389465%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 45s 337ms/step - loss: 0.9296 - acc: 0.5150 - val_loss: 0.6796 - val_acc: 0.5960\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.7011 - acc: 0.5761 - val_loss: 0.6778 - val_acc: 0.8023\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.6331 - acc: 0.6175 - val_loss: 0.4232 - val_acc: 0.8559\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.5131 - acc: 0.7829 - val_loss: 0.3602 - val_acc: 0.8475\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.4464 - acc: 0.8139 - val_loss: 0.3670 - val_acc: 0.8475\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.4034 - acc: 0.8224 - val_loss: 0.6457 - val_acc: 0.5254\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3914 - acc: 0.8402 - val_loss: 0.3244 - val_acc: 0.8616\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3926 - acc: 0.8543 - val_loss: 0.2941 - val_acc: 0.8927\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3264 - acc: 0.8750 - val_loss: 0.2618 - val_acc: 0.8983\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3488 - acc: 0.8759 - val_loss: 0.2611 - val_acc: 0.9124\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3493 - acc: 0.8665 - val_loss: 0.3355 - val_acc: 0.8701\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3389 - acc: 0.8712 - val_loss: 0.7097 - val_acc: 0.5000\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3906 - acc: 0.8430 - val_loss: 0.2702 - val_acc: 0.9011\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3398 - acc: 0.8882 - val_loss: 0.2909 - val_acc: 0.9068\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3059 - acc: 0.8985 - val_loss: 0.3085 - val_acc: 0.8955\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2788 - acc: 0.8976 - val_loss: 0.2366 - val_acc: 0.9209\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2558 - acc: 0.9107 - val_loss: 0.2451 - val_acc: 0.8983\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2153 - acc: 0.9192 - val_loss: 0.2191 - val_acc: 0.9237\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2359 - acc: 0.9135 - val_loss: 0.2927 - val_acc: 0.8757\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2192 - acc: 0.9267 - val_loss: 0.2054 - val_acc: 0.9209\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2155 - acc: 0.9267 - val_loss: 0.2013 - val_acc: 0.9266\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1733 - acc: 0.9398 - val_loss: 0.2152 - val_acc: 0.9294\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1868 - acc: 0.9314 - val_loss: 0.2199 - val_acc: 0.9096\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1975 - acc: 0.9323 - val_loss: 0.2081 - val_acc: 0.9124\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1461 - acc: 0.9474 - val_loss: 0.1899 - val_acc: 0.9379\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1286 - acc: 0.9605 - val_loss: 0.2735 - val_acc: 0.8983\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1356 - acc: 0.9445 - val_loss: 0.3589 - val_acc: 0.9294\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1887 - acc: 0.9380 - val_loss: 0.2625 - val_acc: 0.9040\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1653 - acc: 0.9380 - val_loss: 0.2283 - val_acc: 0.9266\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1553 - acc: 0.9521 - val_loss: 0.2378 - val_acc: 0.9124\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1222 - acc: 0.9558 - val_loss: 0.2608 - val_acc: 0.9209\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1100 - acc: 0.9680 - val_loss: 0.2115 - val_acc: 0.9266\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1116 - acc: 0.9699 - val_loss: 0.2514 - val_acc: 0.9096\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1105 - acc: 0.9558 - val_loss: 0.4025 - val_acc: 0.8644\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1183 - acc: 0.9633 - val_loss: 0.2423 - val_acc: 0.9237\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1031 - acc: 0.9671 - val_loss: 0.2527 - val_acc: 0.9322\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.0570 - acc: 0.9793 - val_loss: 0.3257 - val_acc: 0.9237\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1008 - acc: 0.9671 - val_loss: 0.2816 - val_acc: 0.8955\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.0778 - acc: 0.9680 - val_loss: 0.3339 - val_acc: 0.9124\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.0453 - acc: 0.9821Restoring model weights from the end of the best epoch: 25.\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.0453 - acc: 0.9821 - val_loss: 0.4486 - val_acc: 0.9266\n",
            "Epoch 40: early stopping\n",
            "Score for fold 4: loss of 0.1819366216659546; acc of 93.67088675498962%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.21366216242313385 - Accuracy: 93.03797483444214%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.24669936299324036 - Accuracy: 91.77215099334717%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.18603329360485077 - Accuracy: 92.40506291389465%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.1819366216659546 - Accuracy: 93.67088675498962%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 92.7215188741684 (+- 0.7076170390570615)\n",
            "> Loss: 0.2070828601717949\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2eme preprocessing : SSIM + images noires a partir du milieu du scan\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "roQP38hM6jsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = importation(\"/content/drive/MyDrive/UV PROJET P6/dataset_normal_methode_2.2_3.2.hdf5\", \n",
        "                      \"/content/drive/MyDrive/UV PROJET P6/dataset_malade_methode_2.2_3.2.hdf5\")\n",
        "Data, Y = labelling(Dataset)\n",
        "Data_train, Data_test, y_train, y_test = train_test_split(Data, Y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "UIy8fiMgEydZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KfoldValidation3D(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PL17F_wsPnW",
        "outputId": "0b3b7200-f2b3-4e5f-f200-7ec1eb170270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 48s 346ms/step - loss: 1.0418 - acc: 0.5315 - val_loss: 0.6285 - val_acc: 0.5493\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.7056 - acc: 0.5701 - val_loss: 0.7010 - val_acc: 0.5465\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.6027 - acc: 0.6849 - val_loss: 0.4324 - val_acc: 0.8479\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.4753 - acc: 0.8024 - val_loss: 0.4232 - val_acc: 0.7887\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3722 - acc: 0.8542 - val_loss: 0.4977 - val_acc: 0.7690\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.3886 - acc: 0.8532 - val_loss: 0.4978 - val_acc: 0.8141\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3926 - acc: 0.8325 - val_loss: 0.3918 - val_acc: 0.8197\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3892 - acc: 0.8495 - val_loss: 0.3402 - val_acc: 0.8507\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3943 - acc: 0.8373 - val_loss: 0.3323 - val_acc: 0.8648\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.3721 - acc: 0.8495 - val_loss: 0.4575 - val_acc: 0.7775\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3423 - acc: 0.8692 - val_loss: 0.3291 - val_acc: 0.8817\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3333 - acc: 0.8796 - val_loss: 0.3074 - val_acc: 0.8817\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3452 - acc: 0.8608 - val_loss: 0.3482 - val_acc: 0.8423\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3186 - acc: 0.8796 - val_loss: 0.3687 - val_acc: 0.8338\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3232 - acc: 0.8777 - val_loss: 0.3427 - val_acc: 0.8479\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2963 - acc: 0.8824 - val_loss: 0.3159 - val_acc: 0.8451\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3013 - acc: 0.8871 - val_loss: 0.2947 - val_acc: 0.8732\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3001 - acc: 0.8862 - val_loss: 0.3543 - val_acc: 0.8704\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3156 - acc: 0.8890 - val_loss: 0.3506 - val_acc: 0.8479\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.3054 - acc: 0.8786 - val_loss: 0.4988 - val_acc: 0.7859\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.2993 - acc: 0.8975 - val_loss: 0.3752 - val_acc: 0.8310\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.2496 - acc: 0.9022 - val_loss: 0.2769 - val_acc: 0.9014\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.2909 - acc: 0.8928 - val_loss: 0.2835 - val_acc: 0.8592\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.2443 - acc: 0.9040 - val_loss: 0.2461 - val_acc: 0.8986\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.2839 - acc: 0.8984 - val_loss: 0.3510 - val_acc: 0.8789\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.3297 - acc: 0.8852 - val_loss: 0.2865 - val_acc: 0.8817\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.2375 - acc: 0.9069 - val_loss: 0.2383 - val_acc: 0.9183\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.2357 - acc: 0.9097 - val_loss: 0.2910 - val_acc: 0.8704\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.2251 - acc: 0.9210 - val_loss: 0.3018 - val_acc: 0.8789\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.2802 - acc: 0.9050 - val_loss: 0.2597 - val_acc: 0.8930\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 45s 336ms/step - loss: 0.1890 - acc: 0.9370 - val_loss: 0.2061 - val_acc: 0.9239\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 45s 336ms/step - loss: 0.1572 - acc: 0.9483 - val_loss: 0.2024 - val_acc: 0.9155\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1918 - acc: 0.9285 - val_loss: 0.1941 - val_acc: 0.9239\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.1834 - acc: 0.9464 - val_loss: 0.2341 - val_acc: 0.9042\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.1844 - acc: 0.9417 - val_loss: 0.2359 - val_acc: 0.9127\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.1503 - acc: 0.9464 - val_loss: 0.1920 - val_acc: 0.9127\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.1194 - acc: 0.9633 - val_loss: 0.1669 - val_acc: 0.9239\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.1217 - acc: 0.9567 - val_loss: 0.1793 - val_acc: 0.9352\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.1310 - acc: 0.9520 - val_loss: 0.1838 - val_acc: 0.9211\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.1078 - acc: 0.9643 - val_loss: 0.1750 - val_acc: 0.9239\n",
            "Score for fold 1: loss of 0.1902773231267929; acc of 91.77215099334717%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 46s 338ms/step - loss: 1.0056 - acc: 0.4901 - val_loss: 0.6858 - val_acc: 0.4704\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.6627 - acc: 0.6040 - val_loss: 0.6542 - val_acc: 0.5127\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.4868 - acc: 0.8034 - val_loss: 0.2955 - val_acc: 0.8676\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.4030 - acc: 0.8288 - val_loss: 0.4075 - val_acc: 0.8451\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.4290 - acc: 0.8260 - val_loss: 0.4108 - val_acc: 0.8254\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3991 - acc: 0.8363 - val_loss: 0.3232 - val_acc: 0.8789\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 45s 336ms/step - loss: 0.3872 - acc: 0.8636 - val_loss: 0.3990 - val_acc: 0.8592\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3718 - acc: 0.8457 - val_loss: 0.2806 - val_acc: 0.8986\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3453 - acc: 0.8674 - val_loss: 0.3601 - val_acc: 0.8648\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3362 - acc: 0.8758 - val_loss: 0.5114 - val_acc: 0.7239\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3054 - acc: 0.8815 - val_loss: 0.2654 - val_acc: 0.8845\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3045 - acc: 0.8758 - val_loss: 0.3492 - val_acc: 0.8592\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3312 - acc: 0.8777 - val_loss: 0.4260 - val_acc: 0.8423\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 45s 336ms/step - loss: 0.3589 - acc: 0.8692 - val_loss: 0.3093 - val_acc: 0.8873\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3161 - acc: 0.8786 - val_loss: 0.2583 - val_acc: 0.9042\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.2986 - acc: 0.8871 - val_loss: 0.2959 - val_acc: 0.8704\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.2594 - acc: 0.9012 - val_loss: 0.2399 - val_acc: 0.8845\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 45s 336ms/step - loss: 0.2778 - acc: 0.8956 - val_loss: 0.2413 - val_acc: 0.9099\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.3067 - acc: 0.8956 - val_loss: 0.3011 - val_acc: 0.8901\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.2538 - acc: 0.9078 - val_loss: 0.2386 - val_acc: 0.9211\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.2338 - acc: 0.9135 - val_loss: 0.2326 - val_acc: 0.9183\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.2493 - acc: 0.9069 - val_loss: 0.2566 - val_acc: 0.8845\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.2577 - acc: 0.9125 - val_loss: 0.1973 - val_acc: 0.9239\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.2432 - acc: 0.9078 - val_loss: 0.2690 - val_acc: 0.9268\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2409 - acc: 0.9229 - val_loss: 0.2629 - val_acc: 0.9014\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1838 - acc: 0.9398 - val_loss: 0.2592 - val_acc: 0.8704\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1967 - acc: 0.9332 - val_loss: 0.1834 - val_acc: 0.9380\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1964 - acc: 0.9417 - val_loss: 0.2073 - val_acc: 0.9211\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2062 - acc: 0.9276 - val_loss: 0.2195 - val_acc: 0.9380\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1618 - acc: 0.9473 - val_loss: 0.1782 - val_acc: 0.9380\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1816 - acc: 0.9351 - val_loss: 0.1836 - val_acc: 0.9380\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.1752 - acc: 0.9407 - val_loss: 0.2479 - val_acc: 0.9296\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1370 - acc: 0.9595 - val_loss: 0.1494 - val_acc: 0.9465\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.1393 - acc: 0.9558 - val_loss: 0.2382 - val_acc: 0.9014\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.1200 - acc: 0.9577 - val_loss: 0.1774 - val_acc: 0.9380\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.1133 - acc: 0.9633 - val_loss: 0.1883 - val_acc: 0.9352\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.0983 - acc: 0.9708 - val_loss: 0.1574 - val_acc: 0.9352\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.0908 - acc: 0.9652 - val_loss: 0.2353 - val_acc: 0.9521\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1371 - acc: 0.9520 - val_loss: 0.1619 - val_acc: 0.9324\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1761 - acc: 0.9417 - val_loss: 0.2845 - val_acc: 0.9296\n",
            "Score for fold 2: loss of 0.2579078674316406; acc of 94.9367105960846%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 46s 339ms/step - loss: 1.0446 - acc: 0.5132 - val_loss: 0.6475 - val_acc: 0.5085\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.6711 - acc: 0.5836 - val_loss: 0.5661 - val_acc: 0.5565\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.5750 - acc: 0.7096 - val_loss: 0.3591 - val_acc: 0.8955\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.5436 - acc: 0.7500 - val_loss: 0.2962 - val_acc: 0.8785\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.4079 - acc: 0.8224 - val_loss: 0.2840 - val_acc: 0.8785\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.4122 - acc: 0.8374 - val_loss: 0.3175 - val_acc: 0.8955\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.3458 - acc: 0.8684 - val_loss: 0.2470 - val_acc: 0.8983\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3801 - acc: 0.8468 - val_loss: 0.3369 - val_acc: 0.8672\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3854 - acc: 0.8468 - val_loss: 0.2916 - val_acc: 0.8983\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.3572 - acc: 0.8581 - val_loss: 0.2837 - val_acc: 0.9096\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.4871 - acc: 0.8045 - val_loss: 0.3062 - val_acc: 0.9011\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3333 - acc: 0.8712 - val_loss: 0.3029 - val_acc: 0.8672\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.3327 - acc: 0.8675 - val_loss: 0.2590 - val_acc: 0.9181\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.2985 - acc: 0.8882 - val_loss: 0.2636 - val_acc: 0.9209\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3255 - acc: 0.8853 - val_loss: 0.2755 - val_acc: 0.9294\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3270 - acc: 0.8797 - val_loss: 0.2188 - val_acc: 0.9294\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2997 - acc: 0.8816 - val_loss: 0.2482 - val_acc: 0.9322\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.2727 - acc: 0.9032 - val_loss: 0.2245 - val_acc: 0.9237\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.2631 - acc: 0.8938 - val_loss: 0.2273 - val_acc: 0.9209\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2575 - acc: 0.9098 - val_loss: 0.3571 - val_acc: 0.9011\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.2776 - acc: 0.8919 - val_loss: 0.2546 - val_acc: 0.9350\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2583 - acc: 0.8966 - val_loss: 0.2707 - val_acc: 0.9266\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2295 - acc: 0.9117 - val_loss: 0.2639 - val_acc: 0.9181\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.2381 - acc: 0.9164 - val_loss: 0.1986 - val_acc: 0.9407\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2634 - acc: 0.8957 - val_loss: 0.3037 - val_acc: 0.9322\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2884 - acc: 0.8966 - val_loss: 0.2550 - val_acc: 0.9096\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2911 - acc: 0.9032 - val_loss: 0.2707 - val_acc: 0.8955\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2378 - acc: 0.9117 - val_loss: 0.2696 - val_acc: 0.9181\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2066 - acc: 0.9154 - val_loss: 0.2297 - val_acc: 0.9181\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2308 - acc: 0.9117 - val_loss: 0.2346 - val_acc: 0.9181\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2161 - acc: 0.9201 - val_loss: 0.2719 - val_acc: 0.9463\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2037 - acc: 0.9295 - val_loss: 0.2548 - val_acc: 0.9379\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1874 - acc: 0.9229 - val_loss: 0.2335 - val_acc: 0.9463\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1785 - acc: 0.9361 - val_loss: 0.1727 - val_acc: 0.9463\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2010 - acc: 0.9342 - val_loss: 0.2111 - val_acc: 0.9492\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.1872 - acc: 0.9286 - val_loss: 0.1736 - val_acc: 0.9576\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 45s 335ms/step - loss: 0.1431 - acc: 0.9502 - val_loss: 0.1736 - val_acc: 0.9548\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.1383 - acc: 0.9464 - val_loss: 0.1749 - val_acc: 0.9492\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1738 - acc: 0.9511 - val_loss: 0.2310 - val_acc: 0.9520\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.1780 - acc: 0.9295 - val_loss: 0.2146 - val_acc: 0.9322\n",
            "Score for fold 3: loss of 0.14195770025253296; acc of 93.67088675498962%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 46s 342ms/step - loss: 1.0149 - acc: 0.4868 - val_loss: 0.6509 - val_acc: 0.4972\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.6587 - acc: 0.6053 - val_loss: 1.3050 - val_acc: 0.4972\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.5051 - acc: 0.7754 - val_loss: 0.3803 - val_acc: 0.8701\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.4758 - acc: 0.8242 - val_loss: 0.3186 - val_acc: 0.9181\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3911 - acc: 0.8449 - val_loss: 0.2454 - val_acc: 0.9237\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3621 - acc: 0.8618 - val_loss: 0.2683 - val_acc: 0.9011\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3503 - acc: 0.8590 - val_loss: 0.3824 - val_acc: 0.8333\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3883 - acc: 0.8543 - val_loss: 0.2789 - val_acc: 0.9294\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3516 - acc: 0.8581 - val_loss: 0.2795 - val_acc: 0.8729\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3176 - acc: 0.8712 - val_loss: 0.2395 - val_acc: 0.9153\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3244 - acc: 0.8806 - val_loss: 0.2596 - val_acc: 0.9237\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3276 - acc: 0.8694 - val_loss: 0.2519 - val_acc: 0.9294\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3298 - acc: 0.8759 - val_loss: 0.2453 - val_acc: 0.9322\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.3065 - acc: 0.8788 - val_loss: 0.2674 - val_acc: 0.8644\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2865 - acc: 0.8797 - val_loss: 0.2075 - val_acc: 0.9294\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2928 - acc: 0.9070 - val_loss: 0.2270 - val_acc: 0.9209\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2940 - acc: 0.8844 - val_loss: 0.3454 - val_acc: 0.9209\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2880 - acc: 0.8882 - val_loss: 0.2290 - val_acc: 0.9294\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 44s 335ms/step - loss: 0.3070 - acc: 0.9135 - val_loss: 0.1966 - val_acc: 0.9322\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2531 - acc: 0.9135 - val_loss: 0.2725 - val_acc: 0.9096\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2327 - acc: 0.9201 - val_loss: 0.2099 - val_acc: 0.9379\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2260 - acc: 0.9211 - val_loss: 0.2664 - val_acc: 0.9237\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2221 - acc: 0.9182 - val_loss: 0.2308 - val_acc: 0.9237\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2182 - acc: 0.9182 - val_loss: 0.2505 - val_acc: 0.9266\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2286 - acc: 0.9126 - val_loss: 0.1948 - val_acc: 0.9322\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2004 - acc: 0.9135 - val_loss: 0.1895 - val_acc: 0.9322\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2226 - acc: 0.9239 - val_loss: 0.1791 - val_acc: 0.9407\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1874 - acc: 0.9276 - val_loss: 0.1836 - val_acc: 0.9407\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1673 - acc: 0.9398 - val_loss: 0.1865 - val_acc: 0.9350\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1993 - acc: 0.9239 - val_loss: 0.1720 - val_acc: 0.9435\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1737 - acc: 0.9427 - val_loss: 0.1704 - val_acc: 0.9407\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1916 - acc: 0.9333 - val_loss: 0.1776 - val_acc: 0.9492\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1705 - acc: 0.9295 - val_loss: 0.1510 - val_acc: 0.9576\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1807 - acc: 0.9258 - val_loss: 0.2253 - val_acc: 0.9068\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2076 - acc: 0.9220 - val_loss: 0.1801 - val_acc: 0.9463\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1271 - acc: 0.9530 - val_loss: 0.1424 - val_acc: 0.9576\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1253 - acc: 0.9521 - val_loss: 0.1450 - val_acc: 0.9548\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.1354 - acc: 0.9549 - val_loss: 0.1682 - val_acc: 0.9463\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.2105 - acc: 0.9267 - val_loss: 0.1584 - val_acc: 0.9520\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 44s 334ms/step - loss: 0.0878 - acc: 0.9633 - val_loss: 0.1621 - val_acc: 0.9463\n",
            "Score for fold 4: loss of 0.106547512114048; acc of 96.20253443717957%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.1902773231267929 - Accuracy: 91.77215099334717%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.2579078674316406 - Accuracy: 94.9367105960846%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.14195770025253296 - Accuracy: 93.67088675498962%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.106547512114048 - Accuracy: 96.20253443717957%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 94.14557069540024 (+- 1.6367229532534742)\n",
            "> Loss: 0.17417260073125362\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 eme preprocessing : random + milieu de scan"
      ],
      "metadata": {
        "id": "QEZP1MJp9jQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = importation(\"/content/drive/MyDrive/UV PROJET P6/dataset_normal_methode_2.2_3.1.hdf5\", \n",
        "                      \"/content/drive/MyDrive/UV PROJET P6/dataset_malade_methode_2.2_3.1.hdf5\")\n",
        "Data, Y = labelling(Dataset)\n",
        "Data_train, Data_test, y_train, y_test = train_test_split(Data, Y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "LcA9scDiFKnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KfoldValidation3D(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1h7Kyx0FKUe",
        "outputId": "bf186c2e-39a6-4f99-97ab-a54c2a76630c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 90s 569ms/step - loss: 0.9471 - acc: 0.5155 - val_loss: 0.6515 - val_acc: 0.5408\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 75s 562ms/step - loss: 0.5378 - acc: 0.7328 - val_loss: 0.3146 - val_acc: 0.8704\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 77s 576ms/step - loss: 0.4916 - acc: 0.7723 - val_loss: 0.4774 - val_acc: 0.8056\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 78s 584ms/step - loss: 0.4163 - acc: 0.8373 - val_loss: 0.2913 - val_acc: 0.8704\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 78s 585ms/step - loss: 0.3813 - acc: 0.8476 - val_loss: 0.2502 - val_acc: 0.8930\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 78s 588ms/step - loss: 0.3961 - acc: 0.8260 - val_loss: 0.2772 - val_acc: 0.8958\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 78s 590ms/step - loss: 0.3690 - acc: 0.8617 - val_loss: 0.2817 - val_acc: 0.8930\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 80s 605ms/step - loss: 0.3677 - acc: 0.8570 - val_loss: 0.3078 - val_acc: 0.9099\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 80s 606ms/step - loss: 0.3327 - acc: 0.8768 - val_loss: 0.2217 - val_acc: 0.9183\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 81s 606ms/step - loss: 0.3188 - acc: 0.8843 - val_loss: 0.2173 - val_acc: 0.9127\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 81s 606ms/step - loss: 0.2953 - acc: 0.8918 - val_loss: 0.4255 - val_acc: 0.8282\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 81s 607ms/step - loss: 0.2914 - acc: 0.8965 - val_loss: 0.2052 - val_acc: 0.9352\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 80s 606ms/step - loss: 0.2903 - acc: 0.8946 - val_loss: 0.2418 - val_acc: 0.9070\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.2915 - acc: 0.9003 - val_loss: 0.2661 - val_acc: 0.8958\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 80s 604ms/step - loss: 0.2963 - acc: 0.8833 - val_loss: 0.2410 - val_acc: 0.8986\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.2867 - acc: 0.8993 - val_loss: 0.2119 - val_acc: 0.9408\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.2400 - acc: 0.9135 - val_loss: 0.1802 - val_acc: 0.9437\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 80s 601ms/step - loss: 0.2198 - acc: 0.9238 - val_loss: 0.6002 - val_acc: 0.6873\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 80s 599ms/step - loss: 0.2793 - acc: 0.8946 - val_loss: 0.2334 - val_acc: 0.9099\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.2039 - acc: 0.9182 - val_loss: 0.1406 - val_acc: 0.9465\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.1935 - acc: 0.9304 - val_loss: 0.1492 - val_acc: 0.9324\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.2132 - acc: 0.9313 - val_loss: 0.1600 - val_acc: 0.9352\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 80s 601ms/step - loss: 0.3002 - acc: 0.8796 - val_loss: 0.1487 - val_acc: 0.9465\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.2472 - acc: 0.9106 - val_loss: 0.1643 - val_acc: 0.9408\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.1967 - acc: 0.9276 - val_loss: 0.1700 - val_acc: 0.9268\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 80s 600ms/step - loss: 0.2067 - acc: 0.9313 - val_loss: 0.1587 - val_acc: 0.9408\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 80s 604ms/step - loss: 0.2350 - acc: 0.9116 - val_loss: 0.1862 - val_acc: 0.9268\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 80s 604ms/step - loss: 0.1974 - acc: 0.9294 - val_loss: 0.1510 - val_acc: 0.9521\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 80s 603ms/step - loss: 0.1772 - acc: 0.9360 - val_loss: 0.1416 - val_acc: 0.9606\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 80s 603ms/step - loss: 0.1647 - acc: 0.9332 - val_loss: 0.1391 - val_acc: 0.9493\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 80s 599ms/step - loss: 0.1409 - acc: 0.9417 - val_loss: 0.1541 - val_acc: 0.9408\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 79s 595ms/step - loss: 0.1272 - acc: 0.9520 - val_loss: 0.2653 - val_acc: 0.9239\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 79s 592ms/step - loss: 0.1460 - acc: 0.9454 - val_loss: 0.1464 - val_acc: 0.9549\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 79s 591ms/step - loss: 0.1113 - acc: 0.9614 - val_loss: 0.1553 - val_acc: 0.9521\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 78s 590ms/step - loss: 0.0776 - acc: 0.9671 - val_loss: 0.1313 - val_acc: 0.9521\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 78s 589ms/step - loss: 0.0940 - acc: 0.9671 - val_loss: 0.1579 - val_acc: 0.9493\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 78s 588ms/step - loss: 0.1920 - acc: 0.9379 - val_loss: 0.1688 - val_acc: 0.9437\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 78s 587ms/step - loss: 0.1949 - acc: 0.9398 - val_loss: 0.1317 - val_acc: 0.9577\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 78s 587ms/step - loss: 0.0965 - acc: 0.9624 - val_loss: 0.1787 - val_acc: 0.9493\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 78s 588ms/step - loss: 0.0830 - acc: 0.9708 - val_loss: 0.1990 - val_acc: 0.9465\n",
            "Score for fold 1: loss of 0.2736803889274597; acc of 93.03797483444214%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 82s 613ms/step - loss: 1.0401 - acc: 0.5165 - val_loss: 0.6602 - val_acc: 0.5183\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 81s 610ms/step - loss: 0.7113 - acc: 0.5550 - val_loss: 0.6853 - val_acc: 0.5211\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 81s 607ms/step - loss: 0.7454 - acc: 0.4807 - val_loss: 0.7067 - val_acc: 0.5183\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 81s 607ms/step - loss: 0.7106 - acc: 0.4788 - val_loss: 0.6831 - val_acc: 0.5183\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 80s 605ms/step - loss: 0.6913 - acc: 0.5353 - val_loss: 0.7050 - val_acc: 0.4873\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.7157 - acc: 0.4779 - val_loss: 0.6967 - val_acc: 0.4873\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 80s 601ms/step - loss: 0.7032 - acc: 0.5108 - val_loss: 0.7044 - val_acc: 0.4873\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 80s 601ms/step - loss: 0.6962 - acc: 0.5108 - val_loss: 0.6906 - val_acc: 0.4873\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 80s 601ms/step - loss: 0.6961 - acc: 0.5221 - val_loss: 0.6909 - val_acc: 0.4873\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.6921 - acc: 0.5024 - val_loss: 0.6899 - val_acc: 0.4873\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.6881 - acc: 0.4995 - val_loss: 0.6902 - val_acc: 0.4873\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.6919 - acc: 0.5099 - val_loss: 0.6873 - val_acc: 0.5211\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 80s 601ms/step - loss: 0.6936 - acc: 0.4967 - val_loss: 0.6880 - val_acc: 0.4873\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 80s 603ms/step - loss: 0.6909 - acc: 0.5240 - val_loss: 0.6871 - val_acc: 0.4901\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.6895 - acc: 0.5052 - val_loss: 0.6872 - val_acc: 0.5070\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 79s 594ms/step - loss: 0.6890 - acc: 0.4892 - val_loss: 0.6866 - val_acc: 0.4901\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.6878 - acc: 0.4741Restoring model weights from the end of the best epoch: 2.\n",
            "133/133 [==============================] - 77s 582ms/step - loss: 0.6878 - acc: 0.4741 - val_loss: 0.6859 - val_acc: 0.4901\n",
            "Epoch 17: early stopping\n",
            "Score for fold 2: loss of 0.709973156452179; acc of 39.87341821193695%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 82s 614ms/step - loss: 0.9915 - acc: 0.5000 - val_loss: 0.6635 - val_acc: 0.4774\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 81s 606ms/step - loss: 0.6983 - acc: 0.5921 - val_loss: 0.9712 - val_acc: 0.4774\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 81s 606ms/step - loss: 0.7129 - acc: 0.5132 - val_loss: 0.6896 - val_acc: 0.5311\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 81s 606ms/step - loss: 0.7116 - acc: 0.4962 - val_loss: 0.6920 - val_acc: 0.4774\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 80s 604ms/step - loss: 0.7015 - acc: 0.4850 - val_loss: 0.6892 - val_acc: 0.4718\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 80s 604ms/step - loss: 0.6901 - acc: 0.5150 - val_loss: 0.6885 - val_acc: 0.5311\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 80s 604ms/step - loss: 0.6939 - acc: 0.4906 - val_loss: 0.6876 - val_acc: 0.5311\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 80s 604ms/step - loss: 0.6927 - acc: 0.5197 - val_loss: 0.6879 - val_acc: 0.5311\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 81s 606ms/step - loss: 0.6927 - acc: 0.4906 - val_loss: 0.6869 - val_acc: 0.5311\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 80s 604ms/step - loss: 0.7034 - acc: 0.4915 - val_loss: 0.6882 - val_acc: 0.5282\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 80s 605ms/step - loss: 0.7018 - acc: 0.4991 - val_loss: 0.6882 - val_acc: 0.5282\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 81s 606ms/step - loss: 0.6989 - acc: 0.5009 - val_loss: 0.6919 - val_acc: 0.4774\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.6916 - acc: 0.4821 - val_loss: 0.6906 - val_acc: 0.4774\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 80s 605ms/step - loss: 0.6960 - acc: 0.4840 - val_loss: 0.6892 - val_acc: 0.4774\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 80s 605ms/step - loss: 0.6944 - acc: 0.4878 - val_loss: 0.6897 - val_acc: 0.4746\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 80s 604ms/step - loss: 0.6972 - acc: 0.4840 - val_loss: 0.6894 - val_acc: 0.4774\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 81s 606ms/step - loss: 0.6918 - acc: 0.5056 - val_loss: 0.6903 - val_acc: 0.4774\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.6910 - acc: 0.5254Restoring model weights from the end of the best epoch: 3.\n",
            "133/133 [==============================] - 80s 605ms/step - loss: 0.6910 - acc: 0.5254 - val_loss: 0.6880 - val_acc: 0.5282\n",
            "Epoch 18: early stopping\n",
            "Score for fold 3: loss of 0.6875008940696716; acc of 60.12658476829529%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 82s 610ms/step - loss: 1.0429 - acc: 0.5320 - val_loss: 0.6498 - val_acc: 0.4859\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 81s 608ms/step - loss: 0.6922 - acc: 0.5893 - val_loss: 0.8672 - val_acc: 0.4887\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 80s 604ms/step - loss: 0.5953 - acc: 0.6880 - val_loss: 0.4358 - val_acc: 0.8305\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 81s 607ms/step - loss: 0.5193 - acc: 0.7603 - val_loss: 0.3852 - val_acc: 0.8531\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 80s 603ms/step - loss: 0.4226 - acc: 0.8252 - val_loss: 0.3465 - val_acc: 0.8729\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 80s 605ms/step - loss: 0.3993 - acc: 0.8355 - val_loss: 0.4541 - val_acc: 0.8277\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 80s 605ms/step - loss: 0.3456 - acc: 0.8665 - val_loss: 0.2996 - val_acc: 0.8785\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.3526 - acc: 0.8637 - val_loss: 0.2935 - val_acc: 0.8785\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 80s 605ms/step - loss: 0.3572 - acc: 0.8675 - val_loss: 0.2869 - val_acc: 0.8870\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 80s 606ms/step - loss: 0.3532 - acc: 0.8628 - val_loss: 0.3003 - val_acc: 0.8757\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.4034 - acc: 0.8712 - val_loss: 0.6384 - val_acc: 0.5734\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 80s 604ms/step - loss: 0.3389 - acc: 0.8750 - val_loss: 0.2873 - val_acc: 0.9011\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 80s 605ms/step - loss: 0.2874 - acc: 0.8985 - val_loss: 0.2742 - val_acc: 0.9011\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 80s 603ms/step - loss: 0.2734 - acc: 0.8882 - val_loss: 0.2530 - val_acc: 0.9011\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 80s 604ms/step - loss: 0.2698 - acc: 0.8947 - val_loss: 0.2372 - val_acc: 0.9124\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.2517 - acc: 0.9051 - val_loss: 0.2378 - val_acc: 0.9096\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 80s 604ms/step - loss: 0.2326 - acc: 0.9088 - val_loss: 0.2720 - val_acc: 0.9068\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.2567 - acc: 0.9098 - val_loss: 0.2323 - val_acc: 0.9124\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 80s 604ms/step - loss: 0.2135 - acc: 0.9267 - val_loss: 0.5927 - val_acc: 0.7740\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.2205 - acc: 0.9239 - val_loss: 0.2344 - val_acc: 0.9011\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.2204 - acc: 0.9135 - val_loss: 0.2309 - val_acc: 0.9153\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 80s 600ms/step - loss: 0.2081 - acc: 0.9295 - val_loss: 0.3019 - val_acc: 0.9040\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 80s 601ms/step - loss: 0.2376 - acc: 0.9145 - val_loss: 0.3010 - val_acc: 0.9096\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 80s 601ms/step - loss: 0.2369 - acc: 0.9154 - val_loss: 0.2406 - val_acc: 0.9124\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 80s 599ms/step - loss: 0.2039 - acc: 0.9201 - val_loss: 0.2391 - val_acc: 0.9237\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 80s 600ms/step - loss: 0.2037 - acc: 0.9352 - val_loss: 0.2191 - val_acc: 0.9153\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 80s 599ms/step - loss: 0.1814 - acc: 0.9370 - val_loss: 0.2184 - val_acc: 0.9209\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 80s 601ms/step - loss: 0.2190 - acc: 0.9248 - val_loss: 0.2318 - val_acc: 0.9096\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 80s 601ms/step - loss: 0.1888 - acc: 0.9305 - val_loss: 0.2099 - val_acc: 0.9096\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 80s 599ms/step - loss: 0.1673 - acc: 0.9361 - val_loss: 0.2315 - val_acc: 0.9181\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 80s 601ms/step - loss: 0.1619 - acc: 0.9361 - val_loss: 0.2694 - val_acc: 0.9209\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.1225 - acc: 0.9549 - val_loss: 0.1743 - val_acc: 0.9435\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 80s 601ms/step - loss: 0.1617 - acc: 0.9455 - val_loss: 0.1990 - val_acc: 0.9153\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 80s 600ms/step - loss: 0.1218 - acc: 0.9662 - val_loss: 0.1961 - val_acc: 0.9350\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 80s 601ms/step - loss: 0.1172 - acc: 0.9577 - val_loss: 0.2180 - val_acc: 0.9181\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 80s 601ms/step - loss: 0.1337 - acc: 0.9549 - val_loss: 0.2313 - val_acc: 0.9068\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 80s 599ms/step - loss: 0.1246 - acc: 0.9549 - val_loss: 0.2079 - val_acc: 0.9237\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 80s 600ms/step - loss: 0.0795 - acc: 0.9709 - val_loss: 0.2001 - val_acc: 0.9209\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 80s 601ms/step - loss: 0.1096 - acc: 0.9652 - val_loss: 0.1749 - val_acc: 0.9322\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 80s 602ms/step - loss: 0.0611 - acc: 0.9821 - val_loss: 0.2166 - val_acc: 0.9379\n",
            "Score for fold 4: loss of 0.12341942638158798; acc of 94.9367105960846%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.2736803889274597 - Accuracy: 93.03797483444214%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.709973156452179 - Accuracy: 39.87341821193695%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.6875008940696716 - Accuracy: 60.12658476829529%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.12341942638158798 - Accuracy: 94.9367105960846%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 71.99367210268974 (+- 23.13970702376979)\n",
            "> Loss: 0.44864346645772457\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 eme pre processing : random + debut et fin de scan\n"
      ],
      "metadata": {
        "id": "Sln8nIva9jGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = importation(\"/content/drive/MyDrive/UV PROJET P6/dataset_normal_methode_2.3_3.1.hdf5\", \n",
        "                      \"/content/drive/MyDrive/UV PROJET P6/dataset_malade_methode_2.3_3.1.hdf5\")\n",
        "Data, Y = labelling(Dataset)\n",
        "Data_train, Data_test, y_train, y_test = train_test_split(Data, Y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "t8qYSvAYFL7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KfoldValidation3D(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA6sY6ZzFXUC",
        "outputId": "16c1db96-14ca-4eac-c429-0f34cb61f41d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 78s 581ms/step - loss: 1.0877 - acc: 0.5221 - val_loss: 0.7158 - val_acc: 0.4901\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.7604 - acc: 0.4929 - val_loss: 0.6962 - val_acc: 0.4901\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.7347 - acc: 0.5118 - val_loss: 0.6906 - val_acc: 0.5099\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.6177 - acc: 0.6576 - val_loss: 0.3137 - val_acc: 0.8817\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.6045 - acc: 0.7159 - val_loss: 0.4080 - val_acc: 0.8141\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.4602 - acc: 0.8015 - val_loss: 0.3910 - val_acc: 0.8451\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.3993 - acc: 0.8410 - val_loss: 0.3334 - val_acc: 0.9014\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.3470 - acc: 0.8664 - val_loss: 0.2748 - val_acc: 0.8789\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.3583 - acc: 0.8645 - val_loss: 0.2524 - val_acc: 0.9014\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.3432 - acc: 0.8721 - val_loss: 0.5433 - val_acc: 0.7183\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.3121 - acc: 0.8815 - val_loss: 0.2720 - val_acc: 0.8817\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.2929 - acc: 0.8796 - val_loss: 0.2613 - val_acc: 0.8845\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.3128 - acc: 0.8871 - val_loss: 0.2483 - val_acc: 0.9014\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.2956 - acc: 0.8928 - val_loss: 0.2274 - val_acc: 0.9239\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2304 - acc: 0.9135 - val_loss: 0.1868 - val_acc: 0.9296\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2148 - acc: 0.9229 - val_loss: 0.1966 - val_acc: 0.9380\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2207 - acc: 0.9219 - val_loss: 0.2051 - val_acc: 0.9296\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2489 - acc: 0.9069 - val_loss: 0.1574 - val_acc: 0.9493\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.2162 - acc: 0.9229 - val_loss: 0.1938 - val_acc: 0.9239\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2029 - acc: 0.9332 - val_loss: 0.2108 - val_acc: 0.8958\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1895 - acc: 0.9332 - val_loss: 0.1577 - val_acc: 0.9493\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2094 - acc: 0.9200 - val_loss: 0.1866 - val_acc: 0.9239\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2908 - acc: 0.8815 - val_loss: 0.2234 - val_acc: 0.9155\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2600 - acc: 0.9106 - val_loss: 0.1796 - val_acc: 0.9324\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.2196 - acc: 0.9257 - val_loss: 0.2125 - val_acc: 0.9296\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1651 - acc: 0.9454 - val_loss: 0.1834 - val_acc: 0.9352\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1456 - acc: 0.9445 - val_loss: 0.1287 - val_acc: 0.9408\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.1944 - acc: 0.9257 - val_loss: 0.1342 - val_acc: 0.9465\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1765 - acc: 0.9407 - val_loss: 0.3118 - val_acc: 0.9155\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1796 - acc: 0.9351 - val_loss: 0.1541 - val_acc: 0.9493\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1416 - acc: 0.9473 - val_loss: 0.1911 - val_acc: 0.9324\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1446 - acc: 0.9473 - val_loss: 0.1335 - val_acc: 0.9465\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - ETA: 0s - loss: 0.1541 - acc: 0.9464Restoring model weights from the end of the best epoch: 18.\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1541 - acc: 0.9464 - val_loss: 0.4775 - val_acc: 0.7127\n",
            "Epoch 33: early stopping\n",
            "Score for fold 1: loss of 0.1582290083169937; acc of 93.67088675498962%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 77s 574ms/step - loss: 1.0007 - acc: 0.4929 - val_loss: 0.6386 - val_acc: 0.5099\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.6980 - acc: 0.6134 - val_loss: 0.4342 - val_acc: 0.8423\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.5475 - acc: 0.7300 - val_loss: 0.3407 - val_acc: 0.8535\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.4400 - acc: 0.8278 - val_loss: 0.3091 - val_acc: 0.8958\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.4072 - acc: 0.8344 - val_loss: 0.4027 - val_acc: 0.8592\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.4172 - acc: 0.8297 - val_loss: 0.2982 - val_acc: 0.8704\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.3140 - acc: 0.8833 - val_loss: 0.2540 - val_acc: 0.8901\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.3235 - acc: 0.8786 - val_loss: 0.2693 - val_acc: 0.8789\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.4283 - acc: 0.8401 - val_loss: 0.2581 - val_acc: 0.8986\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3366 - acc: 0.8692 - val_loss: 0.2614 - val_acc: 0.8958\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3311 - acc: 0.8739 - val_loss: 0.2383 - val_acc: 0.9042\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.3071 - acc: 0.8890 - val_loss: 0.2803 - val_acc: 0.8901\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2577 - acc: 0.8956 - val_loss: 0.2036 - val_acc: 0.9268\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2782 - acc: 0.8890 - val_loss: 0.2320 - val_acc: 0.9324\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.3537 - acc: 0.8636 - val_loss: 0.2843 - val_acc: 0.8676\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2933 - acc: 0.8890 - val_loss: 0.2119 - val_acc: 0.9127\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2799 - acc: 0.8890 - val_loss: 0.1928 - val_acc: 0.9239\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.2789 - acc: 0.9059 - val_loss: 0.2104 - val_acc: 0.9070\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2260 - acc: 0.9135 - val_loss: 0.1564 - val_acc: 0.9296\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.2124 - acc: 0.9059 - val_loss: 0.1815 - val_acc: 0.9268\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.1910 - acc: 0.9304 - val_loss: 0.1442 - val_acc: 0.9465\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2759 - acc: 0.8899 - val_loss: 0.2415 - val_acc: 0.8930\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.2291 - acc: 0.9144 - val_loss: 0.1713 - val_acc: 0.9324\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2049 - acc: 0.9210 - val_loss: 0.1651 - val_acc: 0.9380\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2149 - acc: 0.9182 - val_loss: 0.1564 - val_acc: 0.9352\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.1423 - acc: 0.9417 - val_loss: 0.1053 - val_acc: 0.9690\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1372 - acc: 0.9417 - val_loss: 0.1216 - val_acc: 0.9549\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1903 - acc: 0.9323 - val_loss: 0.1441 - val_acc: 0.9465\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.1740 - acc: 0.9304 - val_loss: 0.2959 - val_acc: 0.8817\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1374 - acc: 0.9464 - val_loss: 0.2092 - val_acc: 0.9155\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.1474 - acc: 0.9511 - val_loss: 0.1573 - val_acc: 0.9324\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1159 - acc: 0.9492 - val_loss: 0.1385 - val_acc: 0.9521\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1284 - acc: 0.9436 - val_loss: 0.1159 - val_acc: 0.9634\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 75s 564ms/step - loss: 0.0931 - acc: 0.9690 - val_loss: 0.1126 - val_acc: 0.9577\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1146 - acc: 0.9530 - val_loss: 0.1296 - val_acc: 0.9493\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.0965 - acc: 0.9633 - val_loss: 0.1425 - val_acc: 0.9549\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 75s 564ms/step - loss: 0.0749 - acc: 0.9680 - val_loss: 0.1270 - val_acc: 0.9577\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 75s 564ms/step - loss: 0.1132 - acc: 0.9633 - val_loss: 0.1479 - val_acc: 0.9437\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1425 - acc: 0.9558 - val_loss: 0.1279 - val_acc: 0.9493\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 75s 564ms/step - loss: 0.1067 - acc: 0.9548 - val_loss: 0.1067 - val_acc: 0.9521\n",
            "Score for fold 2: loss of 0.1136244386434555; acc of 95.56962251663208%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 78s 581ms/step - loss: 1.0670 - acc: 0.5301 - val_loss: 0.6350 - val_acc: 0.4859\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 76s 575ms/step - loss: 0.6808 - acc: 0.5705 - val_loss: 0.6317 - val_acc: 0.8446\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.5240 - acc: 0.7472 - val_loss: 0.4471 - val_acc: 0.7768\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.4658 - acc: 0.8120 - val_loss: 0.3632 - val_acc: 0.8531\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.3947 - acc: 0.8374 - val_loss: 0.4760 - val_acc: 0.8333\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3814 - acc: 0.8647 - val_loss: 0.2931 - val_acc: 0.8898\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3351 - acc: 0.8788 - val_loss: 0.3106 - val_acc: 0.8701\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.4010 - acc: 0.8459 - val_loss: 0.3593 - val_acc: 0.8446\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3192 - acc: 0.8806 - val_loss: 0.2887 - val_acc: 0.8870\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2873 - acc: 0.8938 - val_loss: 0.2899 - val_acc: 0.8983\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.3229 - acc: 0.8853 - val_loss: 0.2713 - val_acc: 0.8927\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.2739 - acc: 0.8957 - val_loss: 0.2971 - val_acc: 0.8955\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.2629 - acc: 0.9079 - val_loss: 0.2819 - val_acc: 0.8983\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.2233 - acc: 0.9239 - val_loss: 0.2537 - val_acc: 0.9096\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.2220 - acc: 0.9182 - val_loss: 0.2451 - val_acc: 0.9096\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2308 - acc: 0.9070 - val_loss: 0.4260 - val_acc: 0.8729\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2410 - acc: 0.9126 - val_loss: 0.2627 - val_acc: 0.8927\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.2094 - acc: 0.9323 - val_loss: 0.2733 - val_acc: 0.8955\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.1845 - acc: 0.9380 - val_loss: 0.2386 - val_acc: 0.9068\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.1476 - acc: 0.9455 - val_loss: 0.2438 - val_acc: 0.9153\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.1669 - acc: 0.9436 - val_loss: 0.2682 - val_acc: 0.8983\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.1453 - acc: 0.9492 - val_loss: 0.3259 - val_acc: 0.9040\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.2881 - acc: 0.9117 - val_loss: 0.3573 - val_acc: 0.8559\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.3505 - acc: 0.8929 - val_loss: 0.2319 - val_acc: 0.9153\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1824 - acc: 0.9333 - val_loss: 0.2173 - val_acc: 0.9124\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1906 - acc: 0.9417 - val_loss: 0.2511 - val_acc: 0.9096\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1514 - acc: 0.9436 - val_loss: 0.3084 - val_acc: 0.9153\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1596 - acc: 0.9521 - val_loss: 0.2339 - val_acc: 0.9181\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1218 - acc: 0.9568 - val_loss: 0.1822 - val_acc: 0.9237\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1400 - acc: 0.9511 - val_loss: 0.2166 - val_acc: 0.9209\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1046 - acc: 0.9568 - val_loss: 0.2279 - val_acc: 0.9068\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.0919 - acc: 0.9680 - val_loss: 0.2781 - val_acc: 0.9266\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1091 - acc: 0.9643 - val_loss: 0.2415 - val_acc: 0.9266\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.0945 - acc: 0.9643 - val_loss: 0.2415 - val_acc: 0.9153\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 75s 564ms/step - loss: 0.0558 - acc: 0.9774 - val_loss: 0.5794 - val_acc: 0.9068\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1239 - acc: 0.9586 - val_loss: 0.2723 - val_acc: 0.8983\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.0787 - acc: 0.9727 - val_loss: 0.2458 - val_acc: 0.9096\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.0623 - acc: 0.9859 - val_loss: 0.3591 - val_acc: 0.9350\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.0775 - acc: 0.9727 - val_loss: 0.3257 - val_acc: 0.9350\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.0575 - acc: 0.9793 - val_loss: 0.3302 - val_acc: 0.9407\n",
            "Score for fold 3: loss of 0.1838568150997162; acc of 94.9367105960846%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 78s 580ms/step - loss: 1.0225 - acc: 0.5056 - val_loss: 0.7010 - val_acc: 0.4915\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 77s 576ms/step - loss: 0.6842 - acc: 0.5695 - val_loss: 0.8513 - val_acc: 0.5169\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.5727 - acc: 0.7152 - val_loss: 0.3936 - val_acc: 0.8136\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.4908 - acc: 0.7961 - val_loss: 0.4131 - val_acc: 0.8192\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.3623 - acc: 0.8628 - val_loss: 0.3067 - val_acc: 0.8757\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3460 - acc: 0.8656 - val_loss: 0.3339 - val_acc: 0.8757\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3421 - acc: 0.8637 - val_loss: 0.2804 - val_acc: 0.9011\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3313 - acc: 0.8722 - val_loss: 0.4341 - val_acc: 0.8559\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3020 - acc: 0.8844 - val_loss: 0.3015 - val_acc: 0.8672\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3490 - acc: 0.8722 - val_loss: 0.2529 - val_acc: 0.8955\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2955 - acc: 0.8985 - val_loss: 0.2756 - val_acc: 0.8729\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.3259 - acc: 0.8694 - val_loss: 0.2629 - val_acc: 0.8898\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.2698 - acc: 0.8947 - val_loss: 0.2172 - val_acc: 0.9181\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.2523 - acc: 0.9107 - val_loss: 0.1981 - val_acc: 0.9209\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2673 - acc: 0.8976 - val_loss: 0.2335 - val_acc: 0.9040\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.2617 - acc: 0.8994 - val_loss: 0.2342 - val_acc: 0.9294\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.2288 - acc: 0.9286 - val_loss: 0.1999 - val_acc: 0.9181\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.2502 - acc: 0.9135 - val_loss: 0.2064 - val_acc: 0.9209\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2335 - acc: 0.9135 - val_loss: 0.2187 - val_acc: 0.9181\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2452 - acc: 0.9182 - val_loss: 0.2049 - val_acc: 0.9237\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1958 - acc: 0.9192 - val_loss: 0.1847 - val_acc: 0.9181\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1846 - acc: 0.9267 - val_loss: 0.2108 - val_acc: 0.8898\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1934 - acc: 0.9286 - val_loss: 0.2563 - val_acc: 0.8870\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1830 - acc: 0.9295 - val_loss: 0.1943 - val_acc: 0.9237\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1662 - acc: 0.9408 - val_loss: 0.1846 - val_acc: 0.9492\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1162 - acc: 0.9539 - val_loss: 0.1594 - val_acc: 0.9605\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1276 - acc: 0.9436 - val_loss: 0.1709 - val_acc: 0.9463\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1350 - acc: 0.9502 - val_loss: 0.1532 - val_acc: 0.9492\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1591 - acc: 0.9530 - val_loss: 0.2120 - val_acc: 0.9266\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1601 - acc: 0.9436 - val_loss: 0.1775 - val_acc: 0.9322\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1574 - acc: 0.9427 - val_loss: 0.1604 - val_acc: 0.9633\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 75s 564ms/step - loss: 0.1124 - acc: 0.9596 - val_loss: 0.2209 - val_acc: 0.9068\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 75s 564ms/step - loss: 0.0808 - acc: 0.9671 - val_loss: 0.1588 - val_acc: 0.9576\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 75s 564ms/step - loss: 0.1050 - acc: 0.9586 - val_loss: 0.2246 - val_acc: 0.9209\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1546 - acc: 0.9427 - val_loss: 0.1633 - val_acc: 0.9463\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1193 - acc: 0.9530 - val_loss: 0.1779 - val_acc: 0.9463\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1042 - acc: 0.9624 - val_loss: 0.1780 - val_acc: 0.9407\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.0876 - acc: 0.9680 - val_loss: 0.2324 - val_acc: 0.9520\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.0883 - acc: 0.9718 - val_loss: 0.2385 - val_acc: 0.9435\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1298 - acc: 0.9577 - val_loss: 0.1845 - val_acc: 0.9520\n",
            "Score for fold 4: loss of 0.29151326417922974; acc of 94.30379867553711%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.1582290083169937 - Accuracy: 93.67088675498962%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.1136244386434555 - Accuracy: 95.56962251663208%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.1838568150997162 - Accuracy: 94.9367105960846%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.29151326417922974 - Accuracy: 94.30379867553711%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 94.62025463581085 (+- 0.7076170390570615)\n",
            "> Loss: 0.18680588155984879\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 eme preprocessing : SSIM + fin de scan\n"
      ],
      "metadata": {
        "id": "E4T2UC269i8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = importation(\"/content/drive/MyDrive/UV PROJET P6/dataset_normal_methode_2.1_3.2.hdf5\", \n",
        "                      \"/content/drive/MyDrive/UV PROJET P6/dataset_malade_methode_2.1_3.2.hdf5\")\n",
        "Data, Y = labelling(Dataset)\n",
        "Data_train, Data_test, y_train, y_test = train_test_split(Data, Y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "Ttt7cJZ8FOVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KfoldValidation3D(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alBQ0eCzFYi5",
        "outputId": "78158a1b-7ab1-45ae-da00-53e7a51edea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 82s 608ms/step - loss: 1.0676 - acc: 0.5136 - val_loss: 0.6102 - val_acc: 0.8056\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.7436 - acc: 0.5390 - val_loss: 0.6172 - val_acc: 0.5352\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.6514 - acc: 0.6284 - val_loss: 0.3322 - val_acc: 0.8704\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.5534 - acc: 0.7592 - val_loss: 0.3089 - val_acc: 0.8732\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.4178 - acc: 0.8363 - val_loss: 0.3345 - val_acc: 0.8732\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.3481 - acc: 0.8692 - val_loss: 0.2784 - val_acc: 0.8930\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3586 - acc: 0.8805 - val_loss: 0.2861 - val_acc: 0.8873\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.3378 - acc: 0.8702 - val_loss: 0.2657 - val_acc: 0.8958\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3168 - acc: 0.8786 - val_loss: 0.2622 - val_acc: 0.8901\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3242 - acc: 0.8739 - val_loss: 0.2914 - val_acc: 0.8930\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.3158 - acc: 0.8890 - val_loss: 0.2775 - val_acc: 0.9014\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.2738 - acc: 0.8965 - val_loss: 0.2425 - val_acc: 0.9211\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2488 - acc: 0.9012 - val_loss: 0.2522 - val_acc: 0.8958\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.2702 - acc: 0.9031 - val_loss: 0.2385 - val_acc: 0.9099\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.2547 - acc: 0.8899 - val_loss: 0.2783 - val_acc: 0.9014\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2444 - acc: 0.8975 - val_loss: 0.2875 - val_acc: 0.9155\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2508 - acc: 0.9031 - val_loss: 0.2837 - val_acc: 0.8958\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.2589 - acc: 0.8956 - val_loss: 0.2580 - val_acc: 0.8873\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2497 - acc: 0.9116 - val_loss: 0.2637 - val_acc: 0.8958\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.2143 - acc: 0.9238 - val_loss: 0.2075 - val_acc: 0.9239\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.2643 - acc: 0.9050 - val_loss: 0.2642 - val_acc: 0.9099\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2425 - acc: 0.9144 - val_loss: 0.2177 - val_acc: 0.9183\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1890 - acc: 0.9341 - val_loss: 0.2242 - val_acc: 0.9042\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1603 - acc: 0.9483 - val_loss: 0.2406 - val_acc: 0.9296\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.1856 - acc: 0.9294 - val_loss: 0.1940 - val_acc: 0.9239\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.1425 - acc: 0.9530 - val_loss: 0.2112 - val_acc: 0.9211\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.1429 - acc: 0.9530 - val_loss: 0.1824 - val_acc: 0.9408\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.1704 - acc: 0.9511 - val_loss: 0.1881 - val_acc: 0.9324\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.1796 - acc: 0.9323 - val_loss: 0.2114 - val_acc: 0.9239\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.1717 - acc: 0.9351 - val_loss: 0.1961 - val_acc: 0.9239\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.1126 - acc: 0.9614 - val_loss: 0.2135 - val_acc: 0.9211\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1051 - acc: 0.9595 - val_loss: 0.2090 - val_acc: 0.9324\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.1523 - acc: 0.9539 - val_loss: 0.1887 - val_acc: 0.9211\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.1241 - acc: 0.9530 - val_loss: 0.1880 - val_acc: 0.9183\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.0982 - acc: 0.9737 - val_loss: 0.2193 - val_acc: 0.9324\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.0904 - acc: 0.9727 - val_loss: 0.2262 - val_acc: 0.9268\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.1086 - acc: 0.9586 - val_loss: 0.1880 - val_acc: 0.9352\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.1032 - acc: 0.9680 - val_loss: 0.1974 - val_acc: 0.9268\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.0745 - acc: 0.9671 - val_loss: 0.2013 - val_acc: 0.9127\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.0537 - acc: 0.9859 - val_loss: 0.2263 - val_acc: 0.9408\n",
            "Score for fold 1: loss of 0.14357762038707733; acc of 94.9367105960846%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 78s 579ms/step - loss: 1.0894 - acc: 0.4911 - val_loss: 0.6684 - val_acc: 0.4958\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 76s 575ms/step - loss: 0.7003 - acc: 0.5673 - val_loss: 0.5412 - val_acc: 0.8366\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.5967 - acc: 0.6971 - val_loss: 0.4422 - val_acc: 0.8000\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.4550 - acc: 0.8137 - val_loss: 0.3011 - val_acc: 0.8648\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.4577 - acc: 0.8100 - val_loss: 0.3165 - val_acc: 0.8732\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3743 - acc: 0.8561 - val_loss: 0.4842 - val_acc: 0.7549\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3877 - acc: 0.8467 - val_loss: 0.2899 - val_acc: 0.8761\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2998 - acc: 0.8824 - val_loss: 0.3441 - val_acc: 0.8592\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.4124 - acc: 0.8410 - val_loss: 0.2743 - val_acc: 0.8845\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.3145 - acc: 0.8815 - val_loss: 0.2944 - val_acc: 0.8901\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.3092 - acc: 0.8852 - val_loss: 0.2735 - val_acc: 0.8901\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.3046 - acc: 0.8862 - val_loss: 0.2839 - val_acc: 0.8873\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.2708 - acc: 0.8965 - val_loss: 0.2796 - val_acc: 0.8704\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.3107 - acc: 0.8946 - val_loss: 0.6430 - val_acc: 0.7268\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2944 - acc: 0.8946 - val_loss: 0.3316 - val_acc: 0.8873\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2819 - acc: 0.8890 - val_loss: 0.2532 - val_acc: 0.9155\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2403 - acc: 0.9078 - val_loss: 0.2366 - val_acc: 0.9042\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2202 - acc: 0.9106 - val_loss: 0.2299 - val_acc: 0.9183\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.2269 - acc: 0.9106 - val_loss: 0.2403 - val_acc: 0.9183\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2211 - acc: 0.9163 - val_loss: 0.2797 - val_acc: 0.8789\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.2739 - acc: 0.8984 - val_loss: 0.2737 - val_acc: 0.9127\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 79s 591ms/step - loss: 0.1935 - acc: 0.9144 - val_loss: 0.2059 - val_acc: 0.9239\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.1764 - acc: 0.9276 - val_loss: 0.2061 - val_acc: 0.9324\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1981 - acc: 0.9116 - val_loss: 0.2279 - val_acc: 0.9099\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1666 - acc: 0.9398 - val_loss: 0.2414 - val_acc: 0.9070\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1543 - acc: 0.9351 - val_loss: 0.2028 - val_acc: 0.9296\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1795 - acc: 0.9323 - val_loss: 0.2127 - val_acc: 0.9296\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1277 - acc: 0.9530 - val_loss: 0.2347 - val_acc: 0.9296\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1509 - acc: 0.9407 - val_loss: 0.2147 - val_acc: 0.9239\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1843 - acc: 0.9360 - val_loss: 0.2062 - val_acc: 0.9268\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 75s 564ms/step - loss: 0.1793 - acc: 0.9341 - val_loss: 0.2032 - val_acc: 0.9099\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1128 - acc: 0.9520 - val_loss: 0.3389 - val_acc: 0.9211\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1564 - acc: 0.9530 - val_loss: 0.1774 - val_acc: 0.9437\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 75s 564ms/step - loss: 0.1006 - acc: 0.9661 - val_loss: 0.2042 - val_acc: 0.9380\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 75s 564ms/step - loss: 0.0914 - acc: 0.9661 - val_loss: 0.2472 - val_acc: 0.9183\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.0900 - acc: 0.9671 - val_loss: 0.2309 - val_acc: 0.9268\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 75s 564ms/step - loss: 0.0534 - acc: 0.9784 - val_loss: 0.3657 - val_acc: 0.9211\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1047 - acc: 0.9643 - val_loss: 0.3176 - val_acc: 0.9183\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 75s 564ms/step - loss: 0.1216 - acc: 0.9614 - val_loss: 0.1928 - val_acc: 0.9465\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1136 - acc: 0.9633 - val_loss: 0.2177 - val_acc: 0.9127\n",
            "Score for fold 2: loss of 0.1529330313205719; acc of 94.30379867553711%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 78s 581ms/step - loss: 0.9290 - acc: 0.5827 - val_loss: 0.5093 - val_acc: 0.7232\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.5713 - acc: 0.7293 - val_loss: 0.4152 - val_acc: 0.8023\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.5029 - acc: 0.7726 - val_loss: 0.4046 - val_acc: 0.8192\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.3845 - acc: 0.8506 - val_loss: 0.3884 - val_acc: 0.8277\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.3903 - acc: 0.8402 - val_loss: 0.2971 - val_acc: 0.8757\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.3432 - acc: 0.8741 - val_loss: 0.3621 - val_acc: 0.8362\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.3529 - acc: 0.8571 - val_loss: 0.2525 - val_acc: 0.8898\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3658 - acc: 0.8647 - val_loss: 0.4259 - val_acc: 0.7712\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.2871 - acc: 0.8966 - val_loss: 0.2559 - val_acc: 0.8898\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2738 - acc: 0.8929 - val_loss: 0.2262 - val_acc: 0.9096\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.3504 - acc: 0.8506 - val_loss: 0.5651 - val_acc: 0.6949\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2729 - acc: 0.8994 - val_loss: 0.4075 - val_acc: 0.8757\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.2612 - acc: 0.9070 - val_loss: 0.1907 - val_acc: 0.9266\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.2545 - acc: 0.9070 - val_loss: 0.2825 - val_acc: 0.8870\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.2121 - acc: 0.9239 - val_loss: 0.1995 - val_acc: 0.9096\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1898 - acc: 0.9389 - val_loss: 0.2788 - val_acc: 0.8898\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2205 - acc: 0.9229 - val_loss: 0.2142 - val_acc: 0.9040\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2178 - acc: 0.9258 - val_loss: 0.2225 - val_acc: 0.9011\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1781 - acc: 0.9286 - val_loss: 0.1761 - val_acc: 0.9266\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2420 - acc: 0.9286 - val_loss: 0.2170 - val_acc: 0.9068\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1719 - acc: 0.9408 - val_loss: 0.1877 - val_acc: 0.9237\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1614 - acc: 0.9389 - val_loss: 0.2044 - val_acc: 0.9096\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1611 - acc: 0.9436 - val_loss: 0.1758 - val_acc: 0.9322\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.1635 - acc: 0.9492 - val_loss: 0.1864 - val_acc: 0.9124\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.1247 - acc: 0.9605 - val_loss: 0.1635 - val_acc: 0.9407\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1396 - acc: 0.9492 - val_loss: 0.1820 - val_acc: 0.9407\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2224 - acc: 0.9173 - val_loss: 0.2764 - val_acc: 0.8898\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1899 - acc: 0.9352 - val_loss: 0.1845 - val_acc: 0.9124\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.1303 - acc: 0.9539 - val_loss: 0.2093 - val_acc: 0.9237\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.1677 - acc: 0.9398 - val_loss: 0.3277 - val_acc: 0.7938\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.2201 - acc: 0.9248 - val_loss: 0.2970 - val_acc: 0.8955\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.1640 - acc: 0.9436 - val_loss: 0.2190 - val_acc: 0.9040\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.1240 - acc: 0.9596 - val_loss: 0.1949 - val_acc: 0.9294\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.1203 - acc: 0.9596 - val_loss: 0.1547 - val_acc: 0.9407\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1059 - acc: 0.9605 - val_loss: 0.1419 - val_acc: 0.9492\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.0839 - acc: 0.9718 - val_loss: 0.1643 - val_acc: 0.9322\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.1065 - acc: 0.9709 - val_loss: 0.1526 - val_acc: 0.9407\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.0905 - acc: 0.9605 - val_loss: 0.1633 - val_acc: 0.9548\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.1303 - acc: 0.9511 - val_loss: 0.1607 - val_acc: 0.9407\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.0808 - acc: 0.9803 - val_loss: 0.1676 - val_acc: 0.9350\n",
            "Score for fold 3: loss of 0.24812842905521393; acc of 90.5063271522522%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 78s 583ms/step - loss: 0.9658 - acc: 0.5273 - val_loss: 0.6536 - val_acc: 0.5028\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 77s 576ms/step - loss: 0.6697 - acc: 0.5836 - val_loss: 2.2208 - val_acc: 0.5028\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.4555 - acc: 0.7932 - val_loss: 0.2983 - val_acc: 0.9040\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.4616 - acc: 0.8026 - val_loss: 0.3100 - val_acc: 0.8757\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.3504 - acc: 0.8703 - val_loss: 0.2844 - val_acc: 0.9124\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.3565 - acc: 0.8609 - val_loss: 0.2593 - val_acc: 0.9209\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.3885 - acc: 0.8524 - val_loss: 0.2321 - val_acc: 0.9181\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.3232 - acc: 0.8806 - val_loss: 0.2687 - val_acc: 0.9237\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.3061 - acc: 0.8882 - val_loss: 0.2183 - val_acc: 0.9011\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.3476 - acc: 0.8675 - val_loss: 0.2268 - val_acc: 0.9124\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.3314 - acc: 0.8665 - val_loss: 0.2578 - val_acc: 0.9181\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.2957 - acc: 0.8910 - val_loss: 0.2337 - val_acc: 0.9237\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.2777 - acc: 0.8976 - val_loss: 0.2084 - val_acc: 0.9322\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.2794 - acc: 0.8957 - val_loss: 0.2177 - val_acc: 0.9209\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.3225 - acc: 0.8910 - val_loss: 0.2116 - val_acc: 0.9153\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.2738 - acc: 0.8797 - val_loss: 0.2228 - val_acc: 0.9209\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.2463 - acc: 0.9107 - val_loss: 0.1950 - val_acc: 0.9322\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.2428 - acc: 0.8985 - val_loss: 0.1922 - val_acc: 0.9266\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.2369 - acc: 0.9192 - val_loss: 0.1864 - val_acc: 0.9379\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.2581 - acc: 0.9032 - val_loss: 0.2035 - val_acc: 0.9322\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.2222 - acc: 0.9182 - val_loss: 0.1648 - val_acc: 0.9463\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.2053 - acc: 0.9201 - val_loss: 0.2106 - val_acc: 0.9435\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.2250 - acc: 0.9220 - val_loss: 0.2291 - val_acc: 0.9435\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.1633 - acc: 0.9398 - val_loss: 0.2277 - val_acc: 0.9266\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.2873 - acc: 0.9032 - val_loss: 0.1862 - val_acc: 0.9548\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.1901 - acc: 0.9323 - val_loss: 0.1506 - val_acc: 0.9548\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.1970 - acc: 0.9267 - val_loss: 0.1889 - val_acc: 0.9379\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.1554 - acc: 0.9417 - val_loss: 0.1438 - val_acc: 0.9492\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.1948 - acc: 0.9323 - val_loss: 0.1609 - val_acc: 0.9548\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.1920 - acc: 0.9305 - val_loss: 0.1481 - val_acc: 0.9463\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.1469 - acc: 0.9398 - val_loss: 0.1491 - val_acc: 0.9492\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.1565 - acc: 0.9455 - val_loss: 0.1457 - val_acc: 0.9463\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1222 - acc: 0.9558 - val_loss: 0.1337 - val_acc: 0.9548\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.1023 - acc: 0.9680 - val_loss: 0.1353 - val_acc: 0.9520\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.1272 - acc: 0.9568 - val_loss: 0.1563 - val_acc: 0.9576\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.2217 - acc: 0.9107 - val_loss: 0.2774 - val_acc: 0.9153\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.1260 - acc: 0.9492 - val_loss: 0.1161 - val_acc: 0.9520\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.0817 - acc: 0.9737 - val_loss: 0.1243 - val_acc: 0.9463\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.0748 - acc: 0.9784 - val_loss: 0.1491 - val_acc: 0.9407\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.0930 - acc: 0.9690 - val_loss: 0.0970 - val_acc: 0.9633\n",
            "Score for fold 4: loss of 0.08827152848243713; acc of 96.20253443717957%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.14357762038707733 - Accuracy: 94.9367105960846%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.1529330313205719 - Accuracy: 94.30379867553711%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.24812842905521393 - Accuracy: 90.5063271522522%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.08827152848243713 - Accuracy: 96.20253443717957%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 93.98734271526337 (+- 2.1228511171711846)\n",
            "> Loss: 0.15822765231132507\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6 eme pre processing : SSIM + debut et fin de scan"
      ],
      "metadata": {
        "id": "tkzB0Jxh9iuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = importation(\"/content/drive/MyDrive/UV PROJET P6/dataset_normal_methode_2.3_3.2.hdf5\", \n",
        "                      \"/content/drive/MyDrive/UV PROJET P6/dataset_malade_methode_2.3_3.2.hdf5\")\n",
        "Data, Y = labelling(Dataset)\n",
        "Data_train, Data_test, y_train, y_test = train_test_split(Data, Y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "xtzjeg9BFPWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KfoldValidation3D(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKtUoH3BFa3K",
        "outputId": "1b8aad6b-90e2-4e98-d47c-7c0085fe2b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 78s 579ms/step - loss: 1.0086 - acc: 0.5381 - val_loss: 0.6627 - val_acc: 0.5437\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.6500 - acc: 0.6030 - val_loss: 1.6255 - val_acc: 0.5944\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.7630 - acc: 0.5494 - val_loss: 0.3970 - val_acc: 0.8366\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.5171 - acc: 0.7752 - val_loss: 0.3167 - val_acc: 0.8704\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.4654 - acc: 0.8184 - val_loss: 0.4186 - val_acc: 0.8507\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.3973 - acc: 0.8438 - val_loss: 0.2941 - val_acc: 0.8704\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.3466 - acc: 0.8589 - val_loss: 0.2649 - val_acc: 0.8901\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.3463 - acc: 0.8645 - val_loss: 0.2898 - val_acc: 0.8873\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.3278 - acc: 0.8843 - val_loss: 0.2828 - val_acc: 0.8817\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3604 - acc: 0.8579 - val_loss: 0.2438 - val_acc: 0.8958\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.3441 - acc: 0.8768 - val_loss: 0.3068 - val_acc: 0.8901\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 76s 568ms/step - loss: 0.3009 - acc: 0.8768 - val_loss: 0.2668 - val_acc: 0.8817\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.3382 - acc: 0.8692 - val_loss: 0.2043 - val_acc: 0.9155\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.3004 - acc: 0.8909 - val_loss: 0.1977 - val_acc: 0.9211\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.2994 - acc: 0.8881 - val_loss: 0.3396 - val_acc: 0.8817\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2817 - acc: 0.8975 - val_loss: 0.1987 - val_acc: 0.9239\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.2626 - acc: 0.9003 - val_loss: 0.2241 - val_acc: 0.9127\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.2672 - acc: 0.8984 - val_loss: 0.2265 - val_acc: 0.9268\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.2513 - acc: 0.9078 - val_loss: 0.2179 - val_acc: 0.9099\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2592 - acc: 0.9116 - val_loss: 0.2243 - val_acc: 0.9239\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.2215 - acc: 0.9210 - val_loss: 0.1734 - val_acc: 0.9239\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.1901 - acc: 0.9304 - val_loss: 0.1749 - val_acc: 0.9352\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.2049 - acc: 0.9304 - val_loss: 0.1707 - val_acc: 0.9437\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.1856 - acc: 0.9304 - val_loss: 0.1527 - val_acc: 0.9408\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 75s 568ms/step - loss: 0.1716 - acc: 0.9341 - val_loss: 0.1585 - val_acc: 0.9380\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.2043 - acc: 0.9304 - val_loss: 0.2884 - val_acc: 0.9437\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1900 - acc: 0.9370 - val_loss: 0.1806 - val_acc: 0.9465\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1870 - acc: 0.9445 - val_loss: 0.2044 - val_acc: 0.9324\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1549 - acc: 0.9454 - val_loss: 0.1464 - val_acc: 0.9352\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1596 - acc: 0.9501 - val_loss: 0.1721 - val_acc: 0.9268\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1335 - acc: 0.9558 - val_loss: 0.1227 - val_acc: 0.9577\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1513 - acc: 0.9464 - val_loss: 0.1204 - val_acc: 0.9577\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1245 - acc: 0.9605 - val_loss: 0.1255 - val_acc: 0.9521\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1325 - acc: 0.9520 - val_loss: 0.1125 - val_acc: 0.9718\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1027 - acc: 0.9690 - val_loss: 0.1292 - val_acc: 0.9634\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 75s 567ms/step - loss: 0.1587 - acc: 0.9548 - val_loss: 0.2883 - val_acc: 0.9352\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 75s 566ms/step - loss: 0.1454 - acc: 0.9426 - val_loss: 0.1992 - val_acc: 0.9296\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.1245 - acc: 0.9614 - val_loss: 0.1048 - val_acc: 0.9718\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 75s 565ms/step - loss: 0.0722 - acc: 0.9765 - val_loss: 0.1437 - val_acc: 0.9465\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 75s 564ms/step - loss: 0.1681 - acc: 0.9436 - val_loss: 0.1626 - val_acc: 0.9380\n",
            "Score for fold 1: loss of 0.1573696881532669; acc of 94.30379867553711%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 78s 580ms/step - loss: 1.0294 - acc: 0.5268 - val_loss: 0.6576 - val_acc: 0.7803\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 76s 575ms/step - loss: 0.7289 - acc: 0.5503 - val_loss: 0.6578 - val_acc: 0.7944\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.6970 - acc: 0.5720 - val_loss: 1.5622 - val_acc: 0.4732\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.4417 - acc: 0.8184 - val_loss: 0.4036 - val_acc: 0.8535\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.3990 - acc: 0.8476 - val_loss: 0.3847 - val_acc: 0.8423\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.3357 - acc: 0.8721 - val_loss: 0.3317 - val_acc: 0.8648\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.4010 - acc: 0.8344 - val_loss: 0.4317 - val_acc: 0.8507\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.3368 - acc: 0.8683 - val_loss: 0.2614 - val_acc: 0.9042\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.3240 - acc: 0.8890 - val_loss: 0.2999 - val_acc: 0.8732\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.3446 - acc: 0.8721 - val_loss: 0.3476 - val_acc: 0.8592\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2788 - acc: 0.8928 - val_loss: 0.3125 - val_acc: 0.8732\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2527 - acc: 0.9087 - val_loss: 0.2703 - val_acc: 0.8930\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.2446 - acc: 0.9059 - val_loss: 0.2572 - val_acc: 0.8986\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.2677 - acc: 0.8909 - val_loss: 0.2418 - val_acc: 0.8958\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.2643 - acc: 0.9040 - val_loss: 0.2416 - val_acc: 0.8986\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.2581 - acc: 0.9078 - val_loss: 0.2198 - val_acc: 0.9042\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.3153 - acc: 0.8824 - val_loss: 0.2612 - val_acc: 0.8986\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.2232 - acc: 0.9238 - val_loss: 0.2344 - val_acc: 0.9211\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2238 - acc: 0.9219 - val_loss: 0.2037 - val_acc: 0.9183\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.2045 - acc: 0.9266 - val_loss: 0.2044 - val_acc: 0.9127\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.2095 - acc: 0.9266 - val_loss: 0.2082 - val_acc: 0.9296\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 76s 570ms/step - loss: 0.1972 - acc: 0.9276 - val_loss: 0.2081 - val_acc: 0.9211\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.2091 - acc: 0.9200 - val_loss: 0.2212 - val_acc: 0.9070\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1908 - acc: 0.9370 - val_loss: 0.2661 - val_acc: 0.8958\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1928 - acc: 0.9257 - val_loss: 0.2024 - val_acc: 0.9155\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1708 - acc: 0.9351 - val_loss: 0.1817 - val_acc: 0.9296\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1424 - acc: 0.9567 - val_loss: 0.1936 - val_acc: 0.9296\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1703 - acc: 0.9436 - val_loss: 0.1744 - val_acc: 0.9239\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1971 - acc: 0.9257 - val_loss: 0.1769 - val_acc: 0.9296\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1354 - acc: 0.9558 - val_loss: 0.2531 - val_acc: 0.9042\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1416 - acc: 0.9530 - val_loss: 0.1632 - val_acc: 0.9352\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1306 - acc: 0.9530 - val_loss: 0.1774 - val_acc: 0.9352\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1033 - acc: 0.9652 - val_loss: 0.1857 - val_acc: 0.9239\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1613 - acc: 0.9454 - val_loss: 0.1654 - val_acc: 0.9268\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1027 - acc: 0.9614 - val_loss: 0.1544 - val_acc: 0.9352\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1185 - acc: 0.9652 - val_loss: 0.1789 - val_acc: 0.9268\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1149 - acc: 0.9605 - val_loss: 0.1614 - val_acc: 0.9380\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.0895 - acc: 0.9746 - val_loss: 0.1932 - val_acc: 0.9183\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1287 - acc: 0.9520 - val_loss: 0.1706 - val_acc: 0.9155\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 76s 569ms/step - loss: 0.1112 - acc: 0.9595 - val_loss: 0.2391 - val_acc: 0.9014\n",
            "Score for fold 2: loss of 0.16846805810928345; acc of 93.67088675498962%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 78s 582ms/step - loss: 1.0503 - acc: 0.5226 - val_loss: 0.6878 - val_acc: 0.5169\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 77s 577ms/step - loss: 0.7104 - acc: 0.5808 - val_loss: 0.9726 - val_acc: 0.5113\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 76s 575ms/step - loss: 0.7118 - acc: 0.5836 - val_loss: 0.6477 - val_acc: 0.5169\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 76s 575ms/step - loss: 0.5380 - acc: 0.7237 - val_loss: 0.3491 - val_acc: 0.8616\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 77s 576ms/step - loss: 0.4553 - acc: 0.8139 - val_loss: 0.4514 - val_acc: 0.7316\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 76s 576ms/step - loss: 0.4119 - acc: 0.8346 - val_loss: 0.4206 - val_acc: 0.7881\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 76s 575ms/step - loss: 0.3687 - acc: 0.8487 - val_loss: 0.5198 - val_acc: 0.7542\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 76s 575ms/step - loss: 0.3660 - acc: 0.8571 - val_loss: 0.3718 - val_acc: 0.9040\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 76s 575ms/step - loss: 0.3733 - acc: 0.8534 - val_loss: 0.2841 - val_acc: 0.8927\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 76s 575ms/step - loss: 0.3181 - acc: 0.8759 - val_loss: 0.2560 - val_acc: 0.9124\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.2987 - acc: 0.8872 - val_loss: 0.2326 - val_acc: 0.9124\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 76s 575ms/step - loss: 0.2969 - acc: 0.8835 - val_loss: 0.2808 - val_acc: 0.8559\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 76s 575ms/step - loss: 0.2835 - acc: 0.8825 - val_loss: 0.2201 - val_acc: 0.9237\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.2820 - acc: 0.8919 - val_loss: 0.2390 - val_acc: 0.8983\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.2417 - acc: 0.9126 - val_loss: 0.2721 - val_acc: 0.9068\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.2494 - acc: 0.9004 - val_loss: 0.2094 - val_acc: 0.9181\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.2639 - acc: 0.8976 - val_loss: 0.2328 - val_acc: 0.9068\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.2086 - acc: 0.9267 - val_loss: 0.2203 - val_acc: 0.9209\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.2156 - acc: 0.9164 - val_loss: 0.2847 - val_acc: 0.9040\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.1901 - acc: 0.9192 - val_loss: 0.2146 - val_acc: 0.9209\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.2128 - acc: 0.9267 - val_loss: 0.2978 - val_acc: 0.8814\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.2524 - acc: 0.9051 - val_loss: 0.2175 - val_acc: 0.9322\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.2069 - acc: 0.9342 - val_loss: 0.3670 - val_acc: 0.8588\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.1786 - acc: 0.9276 - val_loss: 0.2300 - val_acc: 0.9266\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.1720 - acc: 0.9276 - val_loss: 0.2490 - val_acc: 0.8983\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.1864 - acc: 0.9370 - val_loss: 0.2287 - val_acc: 0.9237\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.1689 - acc: 0.9342 - val_loss: 0.2803 - val_acc: 0.9266\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.2170 - acc: 0.9201 - val_loss: 0.2169 - val_acc: 0.9153\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.1649 - acc: 0.9276 - val_loss: 0.2128 - val_acc: 0.9237\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.1615 - acc: 0.9483 - val_loss: 0.1980 - val_acc: 0.9407\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.1550 - acc: 0.9380 - val_loss: 0.2304 - val_acc: 0.9350\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.1528 - acc: 0.9408 - val_loss: 0.2107 - val_acc: 0.9237\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.1150 - acc: 0.9596 - val_loss: 0.2016 - val_acc: 0.9407\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.0924 - acc: 0.9643 - val_loss: 0.2454 - val_acc: 0.9322\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.0919 - acc: 0.9605 - val_loss: 0.2407 - val_acc: 0.9407\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.1481 - acc: 0.9380 - val_loss: 0.2068 - val_acc: 0.9266\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.0816 - acc: 0.9709 - val_loss: 0.2234 - val_acc: 0.9548\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.1337 - acc: 0.9596 - val_loss: 0.3642 - val_acc: 0.8870\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.1197 - acc: 0.9577 - val_loss: 0.3233 - val_acc: 0.8559\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.0618 - acc: 0.9727 - val_loss: 0.2634 - val_acc: 0.9435\n",
            "Score for fold 3: loss of 0.16014635562896729; acc of 96.83544039726257%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/40\n",
            "133/133 [==============================] - 78s 584ms/step - loss: 1.0575 - acc: 0.4991 - val_loss: 0.7070 - val_acc: 0.4774\n",
            "Epoch 2/40\n",
            "133/133 [==============================] - 77s 578ms/step - loss: 0.7197 - acc: 0.5226 - val_loss: 0.7040 - val_acc: 0.4774\n",
            "Epoch 3/40\n",
            "133/133 [==============================] - 76s 575ms/step - loss: 0.6601 - acc: 0.6466 - val_loss: 0.6548 - val_acc: 0.6751\n",
            "Epoch 4/40\n",
            "133/133 [==============================] - 76s 575ms/step - loss: 0.5348 - acc: 0.7773 - val_loss: 0.3883 - val_acc: 0.8249\n",
            "Epoch 5/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.6149 - acc: 0.7011 - val_loss: 0.6122 - val_acc: 0.5650\n",
            "Epoch 6/40\n",
            "133/133 [==============================] - 76s 575ms/step - loss: 0.4952 - acc: 0.7650 - val_loss: 0.5130 - val_acc: 0.8729\n",
            "Epoch 7/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.7109 - acc: 0.5573 - val_loss: 0.5501 - val_acc: 0.8390\n",
            "Epoch 8/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.4989 - acc: 0.7942 - val_loss: 0.3880 - val_acc: 0.8390\n",
            "Epoch 9/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.3910 - acc: 0.8421 - val_loss: 0.3978 - val_acc: 0.8277\n",
            "Epoch 10/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.3744 - acc: 0.8562 - val_loss: 0.3804 - val_acc: 0.8277\n",
            "Epoch 11/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.3704 - acc: 0.8571 - val_loss: 0.3270 - val_acc: 0.8616\n",
            "Epoch 12/40\n",
            "133/133 [==============================] - 76s 575ms/step - loss: 0.3225 - acc: 0.8853 - val_loss: 0.3214 - val_acc: 0.8814\n",
            "Epoch 13/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.3322 - acc: 0.8675 - val_loss: 0.2998 - val_acc: 0.8785\n",
            "Epoch 14/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.3209 - acc: 0.8759 - val_loss: 0.2743 - val_acc: 0.8814\n",
            "Epoch 15/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.3064 - acc: 0.8938 - val_loss: 0.2941 - val_acc: 0.8729\n",
            "Epoch 16/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.3012 - acc: 0.8769 - val_loss: 0.2758 - val_acc: 0.8814\n",
            "Epoch 17/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.2838 - acc: 0.8910 - val_loss: 0.3025 - val_acc: 0.8672\n",
            "Epoch 18/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.2972 - acc: 0.8806 - val_loss: 0.2915 - val_acc: 0.8842\n",
            "Epoch 19/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.2764 - acc: 0.9032 - val_loss: 0.2503 - val_acc: 0.9011\n",
            "Epoch 20/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.3098 - acc: 0.8910 - val_loss: 0.3149 - val_acc: 0.8870\n",
            "Epoch 21/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.3128 - acc: 0.8806 - val_loss: 0.2539 - val_acc: 0.8983\n",
            "Epoch 22/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.2361 - acc: 0.9098 - val_loss: 0.3071 - val_acc: 0.8729\n",
            "Epoch 23/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.2486 - acc: 0.9098 - val_loss: 0.2592 - val_acc: 0.8898\n",
            "Epoch 24/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.2441 - acc: 0.9126 - val_loss: 0.2479 - val_acc: 0.8870\n",
            "Epoch 25/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.2226 - acc: 0.9173 - val_loss: 0.2768 - val_acc: 0.9011\n",
            "Epoch 26/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.2492 - acc: 0.9004 - val_loss: 0.2506 - val_acc: 0.9011\n",
            "Epoch 27/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.2237 - acc: 0.9154 - val_loss: 0.2446 - val_acc: 0.9068\n",
            "Epoch 28/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.2144 - acc: 0.9258 - val_loss: 0.2527 - val_acc: 0.9096\n",
            "Epoch 29/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.1998 - acc: 0.9201 - val_loss: 0.2166 - val_acc: 0.9068\n",
            "Epoch 30/40\n",
            "133/133 [==============================] - 76s 574ms/step - loss: 0.1665 - acc: 0.9314 - val_loss: 0.2370 - val_acc: 0.9068\n",
            "Epoch 31/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.1932 - acc: 0.9248 - val_loss: 0.2899 - val_acc: 0.8588\n",
            "Epoch 32/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.2024 - acc: 0.9286 - val_loss: 0.2194 - val_acc: 0.9294\n",
            "Epoch 33/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.1803 - acc: 0.9323 - val_loss: 0.2138 - val_acc: 0.9124\n",
            "Epoch 34/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.1414 - acc: 0.9464 - val_loss: 0.1805 - val_acc: 0.9379\n",
            "Epoch 35/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.1825 - acc: 0.9342 - val_loss: 0.1905 - val_acc: 0.9209\n",
            "Epoch 36/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.1390 - acc: 0.9483 - val_loss: 0.1843 - val_acc: 0.9181\n",
            "Epoch 37/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.1208 - acc: 0.9530 - val_loss: 0.2618 - val_acc: 0.9124\n",
            "Epoch 38/40\n",
            "133/133 [==============================] - 76s 571ms/step - loss: 0.1285 - acc: 0.9398 - val_loss: 0.2229 - val_acc: 0.9096\n",
            "Epoch 39/40\n",
            "133/133 [==============================] - 76s 573ms/step - loss: 0.1405 - acc: 0.9511 - val_loss: 0.2221 - val_acc: 0.9096\n",
            "Epoch 40/40\n",
            "133/133 [==============================] - 76s 572ms/step - loss: 0.1248 - acc: 0.9549 - val_loss: 0.1707 - val_acc: 0.9350\n",
            "Score for fold 4: loss of 0.14476171135902405; acc of 94.9367105960846%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.1573696881532669 - Accuracy: 94.30379867553711%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.16846805810928345 - Accuracy: 93.67088675498962%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.16014635562896729 - Accuracy: 96.83544039726257%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.14476171135902405 - Accuracy: 94.9367105960846%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 94.93670910596848 (+- 1.1840673918455842)\n",
            "> Loss: 0.15768645331263542\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4 - BILAN**"
      ],
      "metadata": {
        "id": "4oZShVdW7x_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut dresser un bilan de la cross validation appliquée aux différents pre-processing et modèles :\n"
      ],
      "metadata": {
        "id": "KEBh6bDgs_qB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![tableau récap.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "sBe0FDvKINeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On en conclut que le modèle de convolution 2D + cellule GRU nous permet d'obtenir une accuracy moyenne supérieure à 96% avec les preprocessing : \n",
        "\n",
        "\n",
        "*   Suppression d'images aléatoire pour les scans qui contenaient plus de 70 images et ajout d'images noires en début et fin de séquences pours les scans contenant moins de 70 images. \n",
        "*   Suppression d'images par comparaison SSIM pour les scans qui contenaient plus de 70 images et ajout d'images noires à la fin de la séquence pour les scans contenant moins de 70 images. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eUeJ4Vx2tSsF"
      }
    }
  ]
}
