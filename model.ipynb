{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "W3Z-_Zwxe-mU",
        "cj_oqkaDqyPo",
        "Z9CXuqkVz0mc",
        "06EDLL3az90D",
        "nKA6GL1PGk0S",
        "5lqlF5RLBIzR",
        "8zAKMv88BPBd"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quent1fvr/Infection_pulmonaire/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0 - Importation des données et librairies**\n",
        "\n"
      ],
      "metadata": {
        "id": "R0SKEyPHdLaR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F10e-oz4Zha_",
        "outputId": "f9535ea2-b684-4d4c-950e-01131a2dd904"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Pn70CEYAbrPv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import tensorflow\n",
        "import keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from keras.layers import Conv3D, BatchNormalization,MaxPooling3D, GlobalMaxPool3D\n",
        "from keras.layers import TimeDistributed, GRU, Dense, Dropout\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour l'import des datasets on crée une fonction dont les paramètres sont les chemins des fichiers hdf5 que l'on souhaite utiliser pour l'apprentissage.\n",
        "\n",
        "Les fichiers hdf5 sont générés par le notebook \"pre_processing.ipynb\". \n",
        "Pour faire notre choix de pré-processing, il suffit de choisir les fichiers hdf5 dont les méthodes du notebook sont indiquées dans le nom du fichier : \n",
        "\"dataset_..._methode_num1_num2\""
      ],
      "metadata": {
        "id": "dm-fUdAmYTI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def importation(path_normal, path_malade):\n",
        "  hf_normal = h5py.File(path_normal, \"r\")\n",
        "  hf_malade = h5py.File(path_malade, \"r\")\n",
        "\n",
        "  # conversion des données en tableaux numpy\n",
        "  Data_normal = np.array(hf_normal[\"dataset_1\"][:])\n",
        "  Data_malade = np.array(hf_malade[\"dataset_2\"][:])\n",
        "\n",
        "  return [Data_normal, Data_malade]"
      ],
      "metadata": {
        "id": "50dxvFRJZy-3"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1 - Etude des données et création des labels**\n",
        "\n"
      ],
      "metadata": {
        "id": "v3imvT8OdXzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = importation(\"/content/drive/MyDrive/UV PROJET P6/Dataset_normal.hdf5\", \n",
        "                      \"/content/drive/MyDrive/UV PROJET P6/Dataset_malade.hdf5\")"
      ],
      "metadata": {
        "id": "7f4LhS0F2eE0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iURefCa8cbi",
        "outputId": "6ca1521f-08c1-49c3-fd6d-3f9f747d256a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((55720, 48, 48), (54600, 48, 48))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "Data_normal = Dataset[0]\n",
        "Data_malade = Dataset[1]\n",
        "Data_normal.shape, Data_malade.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afin de pouvoir utiliser des séquences dans les modèles suivant, nous modifions la shape des tableaux en créant des séquences de taille 70. "
      ],
      "metadata": {
        "id": "hgGGpodXcHeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data_normal = Data_normal.reshape(Data_normal.shape[0]//70, 70, 48, 48)\n",
        "Data_malade = Data_malade.reshape(Data_malade.shape[0]//70, 70, 48, 48)\n",
        "Data = np.concatenate((Data_normal, Data_malade))\n",
        "Data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k79efgCdlAeU",
        "outputId": "a5cc04a0-5e51-44d4-ae9d-e6e7b72c1489"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1576, 70, 48, 48)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le jeu de données est composé de séquences de 70 images par scan, c'est pourquoi on attribue un label pour chaque séquence :\n",
        "- 0 : La personne est négative au COVID\n",
        "- 1 : La personne est positive au COVID"
      ],
      "metadata": {
        "id": "B9aqSe55doPl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "2wqEZaJ98vEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1397ea3c-ebe2-45e8-b554-d86251781d30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1576,)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "Y_normal = np.zeros(Data_normal.shape[0], dtype=np.int8)\n",
        "Y_malade = np.ones(Data_malade.shape[0], dtype=np.int8)\n",
        "Y = np.concatenate((Y_normal, Y_malade))\n",
        "Y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afin de faciliter le traitement pour la suite on encapsule la création des labels dans une fonction :"
      ],
      "metadata": {
        "id": "domC3skI1qC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def labelling(Dataset):\n",
        "  Data_normal = Dataset[0]\n",
        "  Data_malade = Dataset[1]\n",
        "  Data_normal = Data_normal.reshape(Data_normal.shape[0]//70, 70, 48, 48)\n",
        "  Data_malade = Data_malade.reshape(Data_malade.shape[0]//70, 70, 48, 48)\n",
        "  Data = np.concatenate((Data_normal, Data_malade))\n",
        "  Y_normal = np.zeros(Data_normal.shape[0], dtype=np.int8)\n",
        "  Y_malade = np.ones(Data_malade.shape[0], dtype=np.int8)\n",
        "  Y = np.concatenate((Y_normal, Y_malade))\n",
        "  return [Data, Y]"
      ],
      "metadata": {
        "id": "FNW7UhvO15nZ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 - Création des modèles d'apprentissage**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vcO2BEhHeztz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Définition des modèles ▶ Choix à faire"
      ],
      "metadata": {
        "id": "hs79V2SVig94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modèle 1 : 2D"
      ],
      "metadata": {
        "id": "OZ0mzZT_HxvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modèle 2 : 3DCNN\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W3Z-_Zwxe-mU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "76Mz9v0RbzAE"
      },
      "outputs": [],
      "source": [
        "# création du modèle d'apprentissage\n",
        "\n",
        "def build_convnet(shape=(70, 48, 48)):\n",
        "    momentum = .9\n",
        "    model = keras.Sequential()\n",
        "    model.add(Conv3D(64, (3, 3, 3), input_shape=shape, padding='same', activation='relu'))\n",
        "    model.add(Conv3D(64, (3, 3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(MaxPooling3D())\n",
        "    \n",
        "    model.add(Conv3D(128, (3, 3, 3), padding='same', activation='relu'))\n",
        "    model.add(Conv3D(128, (3, 3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(GlobalMaxPool3D())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "4sLlW2a2b_pO"
      },
      "outputs": [],
      "source": [
        "# création du réseau de décision\n",
        "\n",
        "def action_model(shape=(70, 48, 48, 1), nbout=1):\n",
        "    convnet = build_convnet(shape)\n",
        "    \n",
        "    model = keras.Sequential()\n",
        "    model.add(convnet)\n",
        "    # and finally, we make a decision network\n",
        "    model.add(Dense(1024, activation='relu'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(nbout, activation='sigmoid'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modèle 3 ...\n",
        "\n"
      ],
      "metadata": {
        "id": "idtO9hzpiXui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Définition des paramètres globaux"
      ],
      "metadata": {
        "id": "Zie1LkbXiDoE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AhXYRexwcyC9"
      },
      "outputs": [],
      "source": [
        "SIZE = (48, 48)\n",
        "NBFRAME = 70\n",
        "EPOCH = 10\n",
        "BS = 8\n",
        "CHANNEL = 1\n",
        "INSHAPE = (70, 48, 48, 1)\n",
        "model = action_model(INSHAPE, nbout=1)\n",
        "optimizer = tensorflow.keras.optimizers.Adam(0.001)\n",
        "model.compile(\n",
        "    optimizer,\n",
        "    'binary_crossentropy',\n",
        "    metrics=['acc']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On fixe la graine aléatoire afin que l'on puisse avoir des résultats similaires en relançant l'apprentissage : "
      ],
      "metadata": {
        "id": "lQqlZXljfFJg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gxRDqnX5Iw67"
      },
      "outputs": [],
      "source": [
        "def fix_seed(seed):\n",
        "    tensorflow.random.set_seed(seed)\n",
        "\n",
        "SEED = 42\n",
        "fix_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On utilise les callbacks \"EarlyStop\" et \"ModelCheckpoint\" pour optimiser nos résultats : "
      ],
      "metadata": {
        "id": "koGjSlMJfRy3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SEvNY_2dJ5aM"
      },
      "outputs": [],
      "source": [
        "my_callbacks = [\n",
        "    tensorflow.keras.callbacks.EarlyStopping(\n",
        "        monitor = 'val_acc',\n",
        "        min_delta = 0,\n",
        "        patience = 10,\n",
        "        verbose = 1,\n",
        "        restore_best_weights = True),\n",
        "    tensorflow.keras.callbacks.ModelCheckpoint(filepath='/tmp/myModel.h5')\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Définition des fonctions globales"
      ],
      "metadata": {
        "id": "cj_oqkaDqyPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fonction de visualisation"
      ],
      "metadata": {
        "id": "Z9CXuqkVz0mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualisation():\n",
        "  acc = model.history.history['acc']\n",
        "  val_acc = model.history.history['val_acc']\n",
        "  loss = model.history.history['loss']\n",
        "  val_loss = model.history.history['val_loss']\n",
        "\n",
        "  print(acc)\n",
        "  print(val_acc)\n",
        "\n",
        "  print(loss)\n",
        "  print(val_loss)\n",
        "\n",
        "  epochs = range(len(acc))\n",
        "\n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "-jmePbAcq3Fr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fonction d'évaluation du modèle sur un jeu de données aléatoire"
      ],
      "metadata": {
        "id": "06EDLL3az90D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(dataLength, iterations):\n",
        "  \n",
        "  for i in range(iterations):\n",
        "    Data_test = []\n",
        "    for i in range(dataLength):\n",
        "      aleatoire = np.random.randint(0, Data_normal.shape[0])\n",
        "      Data_test.append(Data_normal[aleatoire][:])\n",
        "    for i in range(dataLength):\n",
        "      aleatoire = np.random.randint(0, Data_malade.shape[0])\n",
        "      Data_test.append(Data_malade[aleatoire][:])\n",
        "\n",
        "    Data_test = np.array(Data_test)\n",
        "    #Data_test.shape\n",
        "\n",
        "    Y_test_normal = np.zeros(dataLength)\n",
        "    Y_test_malade = np.ones(dataLength)\n",
        "    Y_test = np.concatenate((Y_test_normal, Y_test_malade))\n",
        "    model.evaluate(Data_test, Y_test)\n"
      ],
      "metadata": {
        "id": "FE3j5TCgrcG2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PP6JlqxidbX6"
      },
      "outputs": [],
      "source": [
        "model.fit(Data, Y, epochs=EPOCH, validation_split=0.2, batch_size=BS, callbacks=my_callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On sauvegarde le modèle avec les meilleurs poids : "
      ],
      "metadata": {
        "id": "kmBwvGneQXdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/tmp/myModel.h5\")"
      ],
      "metadata": {
        "id": "s175UYeZQUwL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mpw2qmqFMeo"
      },
      "outputs": [],
      "source": [
        "visualisation()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3 - Evaluation du modèle choisi**\n"
      ],
      "metadata": {
        "id": "EJ0Ub0JagE1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modèle 1 : Conv2D"
      ],
      "metadata": {
        "id": "nKA6GL1PGk0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶ run modèle conv2D"
      ],
      "metadata": {
        "id": "jtOc9ZlCGqqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_callbacks = [\n",
        "    tensorflow.keras.callbacks.EarlyStopping(\n",
        "        monitor = 'val_acc',\n",
        "        min_delta = 0,\n",
        "        patience = 10,\n",
        "        verbose = 1,\n",
        "        restore_best_weights = True),\n",
        "    tensorflow.keras.callbacks.ModelCheckpoint(filepath='/tmp/myModel1.h5')\n",
        "]"
      ],
      "metadata": {
        "id": "IGbvZBfdIIcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(Data, Y, epochs=EPOCH, validation_split=0.2, batch_size=BS, callbacks=my_callbacks)"
      ],
      "metadata": {
        "id": "z2MveOudG0fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On sauvegarde le modèle avec les meilleurs poids : "
      ],
      "metadata": {
        "id": "lt2d2Lw3GzVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/tmp/myModel1.h5\")"
      ],
      "metadata": {
        "id": "MPLyiuJ1G460"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualisation()"
      ],
      "metadata": {
        "id": "_6iiAW9gHBDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation(50, 10)"
      ],
      "metadata": {
        "id": "6WM7z3LoHChL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modèle 2 : 3DCNN"
      ],
      "metadata": {
        "id": "5lqlF5RLBIzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶ run du modèle 3DCNN"
      ],
      "metadata": {
        "id": "DAY1wdhjGOa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_callbacks = [\n",
        "    tensorflow.keras.callbacks.EarlyStopping(\n",
        "        monitor = 'val_acc',\n",
        "        min_delta = 0,\n",
        "        patience = 10,\n",
        "        verbose = 1,\n",
        "        restore_best_weights = True),\n",
        "    tensorflow.keras.callbacks.ModelCheckpoint(filepath='/tmp/myMode2.h5')\n",
        "]"
      ],
      "metadata": {
        "id": "1mljhoROILfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpLr9k_UFMte"
      },
      "outputs": [],
      "source": [
        "model.fit(Data, Y, epochs=EPOCH, validation_split=0.2, batch_size=BS, callbacks=my_callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On sauvegarde le modèle avec les meilleurs poids : "
      ],
      "metadata": {
        "id": "cPSWFvU6E-ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/tmp/myModel.h5\")"
      ],
      "metadata": {
        "id": "VfcZ94IdFDat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualisation()"
      ],
      "metadata": {
        "id": "2E4YsGTrFFHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation(50, 10)"
      ],
      "metadata": {
        "id": "7rUQO7rGzg3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jspas ce que c'est\n",
        "preds = np.round(model.predict(Data_test),0)\n",
        "cm = confusion_matrix(Y_test, preds)\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "E7NMS58Aml7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le modèle est plus efficace que les modèles 2D, il prédit avec une accuracy de 93% sur un jeu de donnée aléatoire en test."
      ],
      "metadata": {
        "id": "tertfpbASGAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modèle 2 : 3DCNN + autre pre-processing"
      ],
      "metadata": {
        "id": "8zAKMv88BPBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶ run du modèle 3DCNN"
      ],
      "metadata": {
        "id": "mJzuDo9QHNXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_callbacks2 = [\n",
        "    tensorflow.keras.callbacks.EarlyStopping(\n",
        "        monitor = 'val_acc',\n",
        "        min_delta = 0,\n",
        "        patience = 10,\n",
        "        verbose = 1,\n",
        "        restore_best_weights = True),\n",
        "    tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "        filepath='/tmp/myModel2.1.h5',    \n",
        "        monitor=\"val_acc\"))\n",
        "]"
      ],
      "metadata": {
        "id": "Lo144FAxIUpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On réinitialise les données :"
      ],
      "metadata": {
        "id": "MbeYSqLgF6u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = importation(\"/content/drive/MyDrive/UV PROJET P6/dataset_normal_methode_2.2_3.2.hdf5\", \n",
        "                      \"/content/drive/MyDrive/UV PROJET P6/dataset_malade_methode_2.2_3.2.hdf5\")\n",
        "Data = labelling(Dataset)[0]\n",
        "Y = labelling(Dataset)[1]"
      ],
      "metadata": {
        "id": "QeRvF7DYBD_q"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(Data, Y, epochs=EPOCH, validation_split=0.2, batch_size=BS, callbacks=my_callbacks2)"
      ],
      "metadata": {
        "id": "-pI7xW0bmtnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/tmp/myModel2.1.h5\")"
      ],
      "metadata": {
        "id": "QzlcmY7UteE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualisation()"
      ],
      "metadata": {
        "id": "fy-CGHWYHE7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation(50, 10)"
      ],
      "metadata": {
        "id": "IwFMzPlqHkWD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}